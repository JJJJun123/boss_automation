#!/usr/bin/env python3
"""
è°ƒè¯•é‡å¤å²—ä½é—®é¢˜
æ·±å…¥åˆ†æä¸ºä»€ä¹ˆä¼šæŠ“å–åˆ°é‡å¤çš„å²—ä½
"""

import asyncio
import logging
import sys
import os
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


async def debug_duplicate_issue():
    """è°ƒè¯•é‡å¤å²—ä½é—®é¢˜"""
    try:
        from crawler.real_playwright_spider import RealPlaywrightBossSpider
        
        logger.info("ğŸ” å¼€å§‹è°ƒè¯•é‡å¤å²—ä½é—®é¢˜...")
        
        spider = RealPlaywrightBossSpider(headless=False)
        
        if not await spider.start():
            logger.error("âŒ æ— æ³•å¯åŠ¨Playwright")
            return
        
        # æœç´¢æµ‹è¯•
        keyword = "å¸‚åœºé£é™©ç®¡ç†"
        city = "shanghai"
        city_code = spider.city_codes.get(city, "101020100")
        search_url = f"https://www.zhipin.com/web/geek/job?query={keyword}&city={city_code}"
        
        logger.info(f"ğŸŒ è®¿é—®: {search_url}")
        await spider.page.goto(search_url, wait_until="domcontentloaded")
        await asyncio.sleep(8)
        
        # å¤„ç†å¼¹çª—
        try:
            await asyncio.sleep(2)
        except:
            pass
        
        # æ»šåŠ¨åŠ è½½
        await spider.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
        await asyncio.sleep(3)
        
        logger.info("ğŸ” åˆ†æé¡µé¢ä¸­çš„å²—ä½å…ƒç´ å»é‡é—®é¢˜...")
        
        # ä½¿ç”¨æˆ‘ä»¬çš„ä¸»è¦é€‰æ‹©å™¨
        selector = 'li:has(a[href*="job_detail"])'
        elements = await spider.page.query_selector_all(selector)
        
        logger.info(f"ğŸ“Š æ‰¾åˆ° {len(elements)} ä¸ªå²—ä½å…ƒç´ ")
        
        # åˆ†æå‰10ä¸ªå…ƒç´ çš„è¯¦ç»†ä¿¡æ¯
        unique_jobs = {}  # ç”¨äºæ£€æµ‹é‡å¤
        element_analysis = []
        
        for i, elem in enumerate(elements[:10]):
            try:
                # è·å–å²—ä½é“¾æ¥
                link_elem = await elem.query_selector('a[href*="job_detail"]')
                href = ""
                if link_elem:
                    href = await link_elem.get_attribute('href')
                    if href:
                        # æ¸…ç†é“¾æ¥ï¼ˆç§»é™¤æŸ¥è¯¢å‚æ•°ï¼‰
                        if '?' in href:
                            href_clean = href.split('?')[0]
                        else:
                            href_clean = href
                
                # è·å–å²—ä½æ ‡é¢˜
                title_elem = await elem.query_selector('.job-name')
                title = ""
                if title_elem:
                    title = await title_elem.inner_text()
                
                # è·å–å…ƒç´ çš„ä½ç½®ä¿¡æ¯
                bbox = await elem.bounding_box()
                
                # è·å–å…ƒç´ çš„å®Œæ•´æ–‡æœ¬
                elem_text = await elem.inner_text()
                
                analysis = {
                    "index": i+1,
                    "title": title.strip() if title else "æœªæ‰¾åˆ°",
                    "href": href,
                    "href_clean": href_clean if href else "æœªæ‰¾åˆ°",
                    "position": bbox,
                    "text_preview": elem_text[:100] if elem_text else "æ— æ–‡æœ¬",
                    "text_length": len(elem_text) if elem_text else 0
                }
                
                element_analysis.append(analysis)
                
                # æ£€æŸ¥é‡å¤
                if href_clean:
                    if href_clean in unique_jobs:
                        unique_jobs[href_clean]['duplicates'].append(i+1)
                        logger.warning(f"ğŸ”„ å‘ç°é‡å¤å²—ä½ {i+1}: {title} -> {href_clean}")
                    else:
                        unique_jobs[href_clean] = {
                            'title': title,
                            'first_index': i+1,
                            'duplicates': []
                        }
                        logger.info(f"âœ… æ–°å²—ä½ {i+1}: {title} -> {href_clean}")
                
            except Exception as e:
                logger.error(f"âŒ åˆ†æå…ƒç´  {i+1} å¤±è´¥: {e}")
        
        # ä¿å­˜åˆ†æç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        analysis_path = f"duplicate_analysis_{timestamp}.json"
        
        result = {
            "total_elements": len(elements),
            "analyzed_elements": len(element_analysis),
            "unique_jobs": len(unique_jobs),
            "duplicate_summary": {},
            "element_details": element_analysis
        }
        
        # åˆ†æé‡å¤æƒ…å†µ
        for href, info in unique_jobs.items():
            if info['duplicates']:
                result["duplicate_summary"][href] = {
                    "title": info['title'],
                    "first_index": info['first_index'],
                    "duplicate_indexes": info['duplicates'],
                    "total_occurrences": 1 + len(info['duplicates'])
                }
        
        with open(analysis_path, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“Š åˆ†ææŠ¥å‘Š:")
        logger.info(f"   æ€»å…ƒç´ æ•°: {len(elements)}")
        logger.info(f"   å”¯ä¸€å²—ä½æ•°: {len(unique_jobs)}")
        logger.info(f"   é‡å¤å²—ä½æ•°: {len(result['duplicate_summary'])}")
        
        if result['duplicate_summary']:
            logger.info(f"\nğŸ”„ é‡å¤å²—ä½è¯¦æƒ…:")
            for href, info in result['duplicate_summary'].items():
                logger.info(f"   {info['title']}: å‡ºç°åœ¨ä½ç½® {info['first_index']} å’Œ {info['duplicate_indexes']}")
        
        # æˆªå›¾ä¿å­˜
        screenshot_path = f"duplicate_debug_{timestamp}.png"
        await spider.page.screenshot(path=screenshot_path)
        
        logger.info(f"ğŸ“ ç”Ÿæˆæ–‡ä»¶:")
        logger.info(f"   åˆ†ææŠ¥å‘Š: {analysis_path}")
        logger.info(f"   é¡µé¢æˆªå›¾: {screenshot_path}")
        
        await spider.close()
        
        # ç»™å‡ºä¿®å¤å»ºè®®
        logger.info(f"\nğŸ’¡ ä¿®å¤å»ºè®®:")
        if len(unique_jobs) < len(elements) // 2:
            logger.warning("âš ï¸ å¤§é‡é‡å¤å…ƒç´ ï¼Œå»ºè®®:")
            logger.warning("   1. æ£€æŸ¥é€‰æ‹©å™¨æ˜¯å¦é€‰ä¸­äº†åµŒå¥—å…ƒç´ ")
            logger.warning("   2. æ·»åŠ æ›´ä¸¥æ ¼çš„å»é‡é€»è¾‘")
            logger.warning("   3. å¯èƒ½éœ€è¦åˆ†æDOMç»“æ„è°ƒæ•´é€‰æ‹©å™¨")
        else:
            logger.info("âœ… é‡å¤æƒ…å†µè¾ƒå°‘ï¼Œä¸»è¦ä¼˜åŒ–å»é‡é€»è¾‘å³å¯")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ è°ƒè¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” Bossç›´è˜é‡å¤å²—ä½é—®é¢˜è°ƒè¯•å·¥å…·")
    print("=" * 50)
    
    success = asyncio.run(debug_duplicate_issue())
    
    if success:
        print("\nâœ… è°ƒè¯•å®Œæˆï¼Œè¯·æŸ¥çœ‹ç”Ÿæˆçš„åˆ†ææŠ¥å‘Š")
    else:
        print("\nâŒ è°ƒè¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")


if __name__ == "__main__":
    main()