#!/usr/bin/env python3
"""
æµ‹è¯•è„šæœ¬ï¼šéªŒè¯æ‰€æœ‰ä¿®å¤æ˜¯å¦æ­£å¸¸å·¥ä½œ
æµ‹è¯•å†…å®¹ï¼š
1. åˆ†ææ•°é‡æ˜¾ç¤ºæ˜¯å¦æ­£ç¡®
2. åŸå¸‚é€‰æ‹©æ˜¯å¦æ­£ç¡®
3. URLé“¾æ¥æ˜¯å¦çœŸå®
4. æµè§ˆå™¨æ˜¯å¦å¯è§
"""

import asyncio
import sys
import os
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

from crawler.real_playwright_spider import RealPlaywrightBossSpider
from config.config_manager import ConfigManager
from analyzer.job_analyzer import JobAnalyzer


async def test_playwright_spider():
    """æµ‹è¯•Playwrightçˆ¬è™«çš„æ‰€æœ‰åŠŸèƒ½"""
    logger.info("ğŸ§ª å¼€å§‹æµ‹è¯•Playwrightçˆ¬è™«...")
    
    # åˆå§‹åŒ–çˆ¬è™«ï¼ˆéæ— å¤´æ¨¡å¼ï¼Œç¡®ä¿å¯è§ï¼‰
    spider = RealPlaywrightBossSpider(headless=False)
    
    try:
        # 1. æµ‹è¯•æµè§ˆå™¨å¯åŠ¨å’Œå¯è§æ€§
        logger.info("\nğŸ“‹ æµ‹è¯•1: æµè§ˆå™¨å¯åŠ¨å’Œå¯è§æ€§")
        success = await spider.start()
        if success:
            logger.info("âœ… æµè§ˆå™¨å¯åŠ¨æˆåŠŸï¼Œçª—å£åº”è¯¥å¯è§")
            await asyncio.sleep(2)  # ç»™ç”¨æˆ·æ—¶é—´è§‚å¯Ÿ
        else:
            logger.error("âŒ æµè§ˆå™¨å¯åŠ¨å¤±è´¥")
            return
        
        # 2. æµ‹è¯•åŸå¸‚é€‰æ‹©
        logger.info("\nğŸ“‹ æµ‹è¯•2: åŸå¸‚é€‰æ‹©åŠŸèƒ½")
        test_cases = [
            ("shanghai", "ä¸Šæµ·", "101020100"),
            ("beijing", "åŒ—äº¬", "101010100"),
        ]
        
        for city_key, city_name, expected_code in test_cases:
            logger.info(f"ğŸ” æµ‹è¯•åŸå¸‚: {city_name} ({city_key})")
            city_code = spider.city_codes.get(city_key)
            if city_code == expected_code:
                logger.info(f"âœ… åŸå¸‚ä»£ç æ­£ç¡®: {city_code}")
            else:
                logger.error(f"âŒ åŸå¸‚ä»£ç é”™è¯¯: æœŸæœ› {expected_code}, å®é™… {city_code}")
        
        # 3. æµ‹è¯•æœç´¢å’ŒURLæå–
        logger.info("\nğŸ“‹ æµ‹è¯•3: æœç´¢åŠŸèƒ½å’ŒURLæå–")
        keyword = "æ•°æ®åˆ†æ"
        city = "shanghai"
        max_jobs = 5
        
        logger.info(f"ğŸ” æœç´¢å‚æ•°: {keyword} | {city} | {max_jobs}ä¸ªå²—ä½")
        jobs = await spider.search_jobs(keyword, city, max_jobs)
        
        logger.info(f"\nğŸ“Š æœç´¢ç»“æœ: æ‰¾åˆ° {len(jobs)} ä¸ªå²—ä½")
        
        # éªŒè¯ç»“æœ
        if jobs:
            for i, job in enumerate(jobs[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ª
                logger.info(f"\nå²—ä½ #{i}:")
                logger.info(f"  èŒä½: {job.get('title', 'æœªçŸ¥')}")
                logger.info(f"  å…¬å¸: {job.get('company', 'æœªçŸ¥')}")
                logger.info(f"  åœ°ç‚¹: {job.get('work_location', 'æœªçŸ¥')}")
                logger.info(f"  è–ªèµ„: {job.get('salary', 'æœªçŸ¥')}")
                logger.info(f"  URL: {job.get('url', 'æ— ')}")
                
                # éªŒè¯URLæ ¼å¼
                url = job.get('url', '')
                if url and url.startswith('https://www.zhipin.com/job_detail/'):
                    logger.info("  âœ… URLæ ¼å¼æ­£ç¡®")
                elif not url:
                    logger.warning("  âš ï¸ æœªæå–åˆ°URL")
                else:
                    logger.info(f"  â„¹ï¸ URLæ ¼å¼: {url}")
                
                # éªŒè¯åœ°ç‚¹æ˜¯å¦åŒ¹é…
                location = job.get('work_location', '')
                if 'ä¸Šæµ·' in location:
                    logger.info("  âœ… åœ°ç‚¹åŒ¹é…")
                else:
                    logger.warning(f"  âš ï¸ åœ°ç‚¹å¯èƒ½ä¸åŒ¹é…: {location}")
        else:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•å²—ä½")
        
        # 4. æµ‹è¯•åˆ†ææ•°é‡
        logger.info("\nğŸ“‹ æµ‹è¯•4: åˆ†ææ•°é‡æ˜¾ç¤º")
        if jobs:
            # åˆå§‹åŒ–é…ç½®å’Œåˆ†æå™¨
            config_manager = ConfigManager()
            ai_config = config_manager.get_ai_config()
            analyzer = JobAnalyzer(ai_config['provider'])
            
            # æ¨¡æ‹Ÿåˆ†æ
            analyze_count = min(3, len(jobs))
            logger.info(f"å‡†å¤‡åˆ†æ {analyze_count} ä¸ªå²—ä½...")
            
            analyzed_jobs = []
            for i, job in enumerate(jobs[:analyze_count]):
                # æ·»åŠ æ¨¡æ‹Ÿåˆ†æç»“æœ
                job['analysis'] = {
                    'score': 5 + i,  # 5, 6, 7
                    'recommendation': 'æ¨è' if (5 + i) >= ai_config['min_score'] else 'ä¸æ¨è',
                    'summary': f'æµ‹è¯•åˆ†æç»“æœ {i+1}'
                }
                analyzed_jobs.append(job)
            
            # éªŒè¯è¿‡æ»¤é€»è¾‘
            logger.info(f"\nåˆ†æå®Œæˆ: {len(analyzed_jobs)} ä¸ªå²—ä½")
            recommended = sum(1 for job in analyzed_jobs if job['analysis']['score'] >= ai_config['min_score'])
            logger.info(f"æ¨èå²—ä½: {recommended} ä¸ª")
            logger.info(f"æ˜¾ç¤ºå²—ä½: {len(analyzed_jobs)} ä¸ª (åº”è¯¥æ˜¾ç¤ºæ‰€æœ‰åˆ†æçš„å²—ä½)")
            
            if len(analyzed_jobs) == analyze_count:
                logger.info("âœ… åˆ†ææ•°é‡æ˜¾ç¤ºæ­£ç¡®")
            else:
                logger.error("âŒ åˆ†ææ•°é‡æ˜¾ç¤ºä¸æ­£ç¡®")
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # æ¸…ç†èµ„æº
        await spider.close()
        logger.info("\nğŸ æµ‹è¯•å®Œæˆ")


def main():
    """ä¸»å‡½æ•°"""
    logger.info("=" * 50)
    logger.info("Bossç›´è˜è‡ªåŠ¨åŒ– - ä¿®å¤éªŒè¯æµ‹è¯•")
    logger.info("=" * 50)
    
    # è¿è¡Œå¼‚æ­¥æµ‹è¯•
    asyncio.run(test_playwright_spider())


if __name__ == "__main__":
    main()