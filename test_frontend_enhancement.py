#!/usr/bin/env python3
"""
æµ‹è¯•å‰ç«¯ä¼˜åŒ–å’Œå²—ä½è¯¦æƒ…æŠ“å–å¢å¼º
"""

import asyncio
import logging
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


async def test_enhanced_extraction():
    """æµ‹è¯•å¢å¼ºçš„å²—ä½è¯¦æƒ…æŠ“å–"""
    try:
        logger.info("ğŸš€ æµ‹è¯•å¢å¼ºçš„å²—ä½è¯¦æƒ…æŠ“å–...")
        
        from crawler.real_playwright_spider import RealPlaywrightBossSpider
        
        spider = RealPlaywrightBossSpider(headless=False)
        
        if not await spider.start():
            logger.error("âŒ æ— æ³•å¯åŠ¨Playwright")
            return False
        
        # æµ‹è¯•æœç´¢
        keyword = "AIäº§å“ç»ç†"
        city = "shanghai"
        
        logger.info(f"ğŸ” æœç´¢: {keyword} åœ¨ {city}")
        jobs = await spider.search_jobs(keyword, city, 2)  # åªæµ‹è¯•2ä¸ªå²—ä½
        
        if jobs:
            logger.info(f"âœ… æˆåŠŸæå– {len(jobs)} ä¸ªå²—ä½")
            
            for i, job in enumerate(jobs, 1):
                logger.info(f"\n{'='*60}")
                logger.info(f"ğŸ“‹ å²—ä½ {i} è¯¦ç»†ä¿¡æ¯:")
                logger.info(f"   æ ‡é¢˜: {job.get('title', 'N/A')}")
                logger.info(f"   å…¬å¸: {job.get('company', 'N/A')}")
                logger.info(f"   åœ°ç‚¹: {job.get('work_location', 'N/A')}")
                logger.info(f"   è–ªèµ„: {job.get('salary', 'N/A')}")
                logger.info(f"   URL: {job.get('url', 'N/A')[:80]}...")
                
                # æ£€æŸ¥å²—ä½æè¿°è´¨é‡
                desc = job.get('job_description', '')
                req = job.get('job_requirements', '')
                company_details = job.get('company_details', '')
                
                logger.info(f"\n   ğŸ“„ å²—ä½æè¿°è´¨é‡æ£€æŸ¥:")
                if desc and 'å…·ä½“èŒè´£è¯·æŸ¥çœ‹å²—ä½è¯¦æƒ…' not in desc and len(desc) > 100:
                    logger.info(f"      âœ… çœŸå®å²—ä½æè¿° ({len(desc)}å­—ç¬¦): {desc[:100]}...")
                else:
                    logger.warning(f"      âš ï¸ å²—ä½æè¿°ä»æ˜¯æ¨¡æ¿æ–‡æœ¬æˆ–å¤ªçŸ­")
                
                if req and 'è¦æ±‚1-3å¹´å·¥ä½œç»éªŒ' not in req and len(req) > 50:
                    logger.info(f"      âœ… çœŸå®ä»»èŒè¦æ±‚ ({len(req)}å­—ç¬¦): {req[:80]}...")
                else:
                    logger.warning(f"      âš ï¸ ä»»èŒè¦æ±‚ä»æ˜¯æ¨¡æ¿æ–‡æœ¬æˆ–å¤ªçŸ­")
                
                if company_details and 'æŸ¥çœ‹è¯¦æƒ…äº†è§£æ›´å¤š' not in company_details:
                    logger.info(f"      âœ… å…¬å¸è¯¦æƒ…: {company_details}")
                else:
                    logger.warning(f"      âš ï¸ å…¬å¸è¯¦æƒ…æœªæ›´æ–°")
                
                # æ£€æŸ¥é¢å¤–ä¿¡æ¯
                if job.get('benefits'):
                    logger.info(f"      âœ… ç¦åˆ©ä¿¡æ¯: {job.get('benefits')[:50]}...")
                if job.get('detailed_address'):
                    logger.info(f"      âœ… è¯¦ç»†åœ°å€: {job.get('detailed_address')}")
                if job.get('publish_time'):
                    logger.info(f"      âœ… å‘å¸ƒæ—¶é—´: {job.get('publish_time')}")
            
            return True
            
        else:
            logger.warning("âš ï¸ æœªæå–åˆ°å²—ä½")
            return False
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        try:
            await spider.close()
        except:
            pass


def test_frontend_updates():
    """æµ‹è¯•å‰ç«¯ç•Œé¢æ›´æ–°"""
    logger.info("\nğŸ–¥ï¸ å‰ç«¯ç•Œé¢ä¼˜åŒ–è¯´æ˜:")
    logger.info("1. âœ… åˆ é™¤äº†'å·²åˆ†æ'ç»Ÿè®¡é¡¹ï¼Œç°åœ¨åªæ˜¾ç¤º'æ€»æœç´¢æ•°'å’Œ'åˆæ ¼å²—ä½'")
    logger.info("2. âœ… 'æ€»æœç´¢æ•°'ç°åœ¨å¯ç‚¹å‡»ï¼Œç‚¹å‡»åæ˜¾ç¤ºæ‰€æœ‰æœç´¢åˆ°çš„å²—ä½")
    logger.info("3. âœ… 'åˆæ ¼å²—ä½'ç°åœ¨å¯ç‚¹å‡»ï¼Œç‚¹å‡»åæ˜¾ç¤ºæ¨èçš„å²—ä½")
    logger.info("4. âœ… æ·»åŠ äº†è§†å›¾åˆ‡æ¢åŠŸèƒ½ï¼Œå¯ä»¥åœ¨æ‰€æœ‰å²—ä½å’Œæ¨èå²—ä½ä¹‹é—´åˆ‡æ¢")
    logger.info("5. âœ… åç«¯APIæ–°å¢ /api/jobs/all ç«¯ç‚¹ï¼Œç”¨äºè·å–æ‰€æœ‰å²—ä½æ•°æ®")
    logger.info("\nğŸ’¡ ä½¿ç”¨æ–¹æ³•: python run_web.py å¯åŠ¨åï¼Œæœç´¢å®Œæˆåç‚¹å‡»ç»Ÿè®¡æ•°å­—å³å¯åˆ‡æ¢è§†å›¾")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ Bossç›´è˜å¢å¼ºåŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    print("æœ¬æµ‹è¯•å°†éªŒè¯:")
    print("1. å¢å¼ºçš„å²—ä½è¯¦æƒ…æŠ“å–")
    print("2. å‰ç«¯ç•Œé¢ä¼˜åŒ–")
    print()
    
    # æµ‹è¯•å²—ä½è¯¦æƒ…æŠ“å–
    result = asyncio.run(test_enhanced_extraction())
    
    if result:
        print("\nâœ… å²—ä½è¯¦æƒ…æŠ“å–æµ‹è¯•é€šè¿‡")
    else:
        print("\nâŒ å²—ä½è¯¦æƒ…æŠ“å–æµ‹è¯•å¤±è´¥")
    
    # è¯´æ˜å‰ç«¯æ›´æ–°
    test_frontend_updates()
    
    print("\nğŸš€ ä¸‹ä¸€æ­¥: è¿è¡Œ python run_web.py ä½“éªŒå®Œæ•´åŠŸèƒ½")


if __name__ == "__main__":
    main()