#!/usr/bin/env python3
"""
æ™ºèƒ½åˆ†å±‚å²—ä½åˆ†æå™¨
å®ç°æ‰¹é‡å¤„ç†å’Œæ™ºèƒ½åˆ†å±‚ç­–ç•¥ï¼š
1. GLMæ‰¹é‡æå–ï¼ˆä¾¿å®œï¼‰
2. DeepSeekæ‰¹é‡æ‰“åˆ†ï¼ˆä¸­ç­‰ï¼‰
3. Claudeæ·±åº¦åˆ†æTOPå²—ä½ï¼ˆè´µä½†ç²¾å‡†ï¼‰
"""

import os
import json
import hashlib
import logging
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
from pathlib import Path

from .ai_client_factory import AIClientFactory

logger = logging.getLogger(__name__)


class SmartJobAnalyzer:
    """æ™ºèƒ½åˆ†å±‚å²—ä½åˆ†æå™¨ - æˆæœ¬ä¼˜åŒ–ç‰ˆ"""
    
    def __init__(self):
        """åˆå§‹åŒ–æ™ºèƒ½åˆ†æå™¨"""
        # åˆå§‹åŒ–ä¸åŒå±‚çº§çš„AIå®¢æˆ·ç«¯
        try:
            self.glm_client = AIClientFactory.create_client("glm", "glm-4.5")
        except Exception as e:
            print(f"âš ï¸ GLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨DeepSeekä½œä¸ºåå¤‡: {e}")
            self.glm_client = AIClientFactory.create_client("deepseek", "deepseek-chat")
        
        self.deepseek_client = AIClientFactory.create_client("deepseek", "deepseek-chat")
        
        # Claudeå®¢æˆ·ç«¯ï¼ˆä»…åœ¨éœ€è¦æ—¶åˆå§‹åŒ–ï¼‰
        self._claude_client = None
        
        # æ‰¹å¤„ç†é…ç½®
        self.batch_size = 10  # æ¯æ‰¹å¤„ç†10ä¸ªå²—ä½
        
        # ç¼“å­˜ç®¡ç†
        self.cache_dir = Path("data/cache")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_jobs": 0,
            "extracted": 0,
            "scored": 0,
            "deep_analyzed": 0,
            "cache_hits": 0,
            "api_calls": 0,
            "estimated_cost": 0.0
        }
        
        print("ğŸš€ æ™ºèƒ½åˆ†å±‚åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
        print("ğŸ“Š æ‰¹å¤„ç†å¤§å°: 10ä¸ªå²—ä½/æ‰¹")
        print("ğŸ’° é¢„æœŸæˆæœ¬: $0.65/100ä¸ªå²—ä½")
    
    @property
    def claude_client(self):
        """å»¶è¿Ÿåˆå§‹åŒ–Claudeå®¢æˆ·ç«¯"""
        if self._claude_client is None:
            try:
                self._claude_client = AIClientFactory.create_client("claude", "claude-3-5-sonnet-20241022")
            except Exception as e:
                print(f"âš ï¸ Claudeå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨DeepSeekä½œä¸ºåå¤‡: {e}")
                self._claude_client = self.deepseek_client  # ä½¿ç”¨DeepSeekä½œä¸ºåå¤‡
        return self._claude_client
    
    def analyze_jobs_smart(self, jobs: List[Dict[str, Any]], resume: Optional[Dict] = None) -> Dict[str, Any]:
        """
        æ™ºèƒ½åˆ†æå²—ä½åˆ—è¡¨
        
        Args:
            jobs: å²—ä½åˆ—è¡¨
            resume: ç”¨æˆ·ç®€å†ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            åˆ†æç»“æœï¼ŒåŒ…å«æ‰€æœ‰å²—ä½è¯„åˆ†å’ŒTOPå²—ä½æ·±åº¦åˆ†æ
        """
        print(f"\nğŸ¯ å¼€å§‹æ™ºèƒ½åˆ†æ {len(jobs)} ä¸ªå²—ä½...")
        self.stats["total_jobs"] = len(jobs)
        
        # Step 1: æ‰¹é‡æå–ç»“æ„åŒ–ä¿¡æ¯
        print("\nğŸ“Š ç¬¬ä¸€æ­¥ï¼šæ‰¹é‡æå–å²—ä½ä¿¡æ¯ï¼ˆGLMï¼‰...")
        extracted_jobs = self._batch_extract(jobs)
        
        # Step 2: æ‰¹é‡æ‰“åˆ†ç­›é€‰
        print("\nğŸ” ç¬¬äºŒæ­¥ï¼šæ‰¹é‡è¯„åˆ†ç­›é€‰ï¼ˆDeepSeekï¼‰...")
        scored_jobs = self._batch_score(extracted_jobs, resume)
        
        # Step 3: é€‰æ‹©TOPå²—ä½
        print("\nğŸ¯ ç¬¬ä¸‰æ­¥ï¼šé€‰æ‹©TOPå²—ä½...")
        top_jobs = self._select_top_jobs(scored_jobs, n=10)
        
        # Step 4: æ·±åº¦åˆ†æTOPå²—ä½
        print(f"\nğŸ’ ç¬¬å››æ­¥ï¼šæ·±åº¦åˆ†æTOP {len(top_jobs)} ä¸ªå²—ä½ï¼ˆClaudeï¼‰...")
        detailed_analysis = self._deep_analyze(top_jobs, resume)
        
        # ç”Ÿæˆåˆ†ææŠ¥å‘Š
        report = self._generate_report(scored_jobs, detailed_analysis)
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        self._print_stats()
        
        return report
    
    def _batch_extract(self, jobs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æ‰¹é‡æå–å²—ä½ä¿¡æ¯"""
        extracted_jobs = []
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(jobs), self.batch_size):
            batch = jobs[i:i + self.batch_size]
            batch_num = i // self.batch_size + 1
            total_batches = (len(jobs) + self.batch_size - 1) // self.batch_size
            
            print(f"  å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches}...")
            
            # æ£€æŸ¥ç¼“å­˜
            batch_results = []
            jobs_to_process = []
            cache_indices = []
            
            for idx, job in enumerate(batch):
                job_id = self._generate_job_id(job)
                cached = self._get_cached_extraction(job_id)
                
                if cached:
                    batch_results.append(cached)
                    self.stats["cache_hits"] += 1
                else:
                    jobs_to_process.append(job)
                    cache_indices.append(idx)
            
            # å¤„ç†æœªç¼“å­˜çš„å²—ä½
            if jobs_to_process:
                try:
                    # æ„å»ºæ‰¹é‡æå–æç¤ºè¯
                    prompt = self._build_batch_extract_prompt(jobs_to_process)
                    
                    # è°ƒç”¨GLMæ‰¹é‡æå–
                    response = self.glm_client.call_api_simple(prompt, max_tokens=2000)
                    self.stats["api_calls"] += 1
                    self.stats["estimated_cost"] += 0.0005 * len(jobs_to_process)
                    
                    # è§£æç»“æœ
                    extracted_batch = self._parse_batch_extraction(response, jobs_to_process)
                    
                    # åˆå¹¶ç»“æœå¹¶ç¼“å­˜
                    result_idx = 0
                    for idx in range(len(batch)):
                        if idx in cache_indices:
                            job = batch[idx]
                            extracted = extracted_batch[result_idx] if result_idx < len(extracted_batch) else {}
                            job_id = self._generate_job_id(job)
                            
                            # ç¼“å­˜æå–ç»“æœ
                            self._cache_extraction(job_id, extracted)
                            batch_results.insert(idx, extracted)
                            result_idx += 1
                            self.stats["extracted"] += 1
                    
                except Exception as e:
                    logger.error(f"æ‰¹é‡æå–å¤±è´¥: {e}")
                    # é™çº§åˆ°å•ä¸ªå¤„ç†
                    for job in jobs_to_process:
                        extracted = self._extract_single(job)
                        batch_results.append(extracted)
            
            # åˆå¹¶åŸå§‹å²—ä½ä¿¡æ¯å’Œæå–ç»“æœ
            for job, extracted in zip(batch, batch_results):
                job_with_extraction = {**job, "extracted_info": extracted}
                extracted_jobs.append(job_with_extraction)
        
        return extracted_jobs
    
    def _batch_score(self, jobs: List[Dict[str, Any]], resume: Optional[Dict]) -> List[Dict[str, Any]]:
        """æ‰¹é‡è¯„åˆ†å²—ä½"""
        scored_jobs = []
        
        # å‡†å¤‡ç®€å†æ‘˜è¦
        resume_summary = self._prepare_resume_summary(resume) if resume else None
        
        # åˆ†æ‰¹è¯„åˆ†
        for i in range(0, len(jobs), self.batch_size):
            batch = jobs[i:i + self.batch_size]
            batch_num = i // self.batch_size + 1
            total_batches = (len(jobs) + self.batch_size - 1) // self.batch_size
            
            print(f"  è¯„åˆ†æ‰¹æ¬¡ {batch_num}/{total_batches}...")
            
            try:
                # æ„å»ºæ‰¹é‡è¯„åˆ†æç¤ºè¯
                prompt = self._build_batch_score_prompt(batch, resume_summary)
                
                # è°ƒç”¨DeepSeekæ‰¹é‡è¯„åˆ†
                response = self.deepseek_client.call_api_simple(prompt, max_tokens=1000)
                self.stats["api_calls"] += 1
                self.stats["estimated_cost"] += 0.001 * len(batch)
                
                # è§£æè¯„åˆ†ç»“æœ
                scores = self._parse_batch_scores(response, batch)
                
                # åˆå¹¶è¯„åˆ†åˆ°å²—ä½ä¿¡æ¯
                for job, score_info in zip(batch, scores):
                    job_with_score = {
                        **job,
                        "score": score_info.get("score", 0),
                        "match_reason": score_info.get("reason", ""),
                        "match_details": score_info
                    }
                    scored_jobs.append(job_with_score)
                    self.stats["scored"] += 1
                    
            except Exception as e:
                logger.error(f"æ‰¹é‡è¯„åˆ†å¤±è´¥: {e}")
                # é™çº§å¤„ç†ï¼šç»™é»˜è®¤åˆ†æ•°
                for job in batch:
                    job_with_score = {
                        **job,
                        "score": 5.0,
                        "match_reason": "è¯„åˆ†æœåŠ¡æš‚æ—¶ä¸å¯ç”¨",
                        "match_details": {}
                    }
                    scored_jobs.append(job_with_score)
        
        return scored_jobs
    
    def _select_top_jobs(self, jobs: List[Dict[str, Any]], n: int = 10) -> List[Dict[str, Any]]:
        """é€‰æ‹©TOP Nä¸ªå²—ä½"""
        # æŒ‰è¯„åˆ†æ’åº
        sorted_jobs = sorted(jobs, key=lambda x: x.get("score", 0), reverse=True)
        
        # é€‰æ‹©TOP N
        top_jobs = sorted_jobs[:n]
        
        print(f"âœ… é€‰å‡ºTOP {len(top_jobs)}ä¸ªå²—ä½")
        for i, job in enumerate(top_jobs[:5], 1):
            print(f"  {i}. {job['title'][:30]}... - {job.get('score', 0):.1f}åˆ†")
        
        if len(top_jobs) > 5:
            print(f"  ... è¿˜æœ‰{len(top_jobs) - 5}ä¸ªå²—ä½")
        
        return top_jobs
    
    def _deep_analyze(self, jobs: List[Dict[str, Any]], resume: Optional[Dict]) -> List[Dict[str, Any]]:
        """æ·±åº¦åˆ†æTOPå²—ä½"""
        detailed_results = []
        
        for i, job in enumerate(jobs, 1):
            print(f"  æ·±åº¦åˆ†æ {i}/{len(jobs)}: {job['title'][:30]}...")
            
            try:
                # æ„å»ºæ·±åº¦åˆ†ææç¤ºè¯
                prompt = self._build_deep_analysis_prompt(job, resume)
                
                # è°ƒç”¨Claudeæ·±åº¦åˆ†æ
                response = self.claude_client.call_api_simple(prompt, max_tokens=2000)
                self.stats["api_calls"] += 1
                self.stats["estimated_cost"] += 0.05
                self.stats["deep_analyzed"] += 1
                
                # è§£ææ·±åº¦åˆ†æç»“æœ
                analysis = self._parse_deep_analysis(response)
                
                # åˆå¹¶åˆ°å²—ä½ä¿¡æ¯
                job_with_analysis = {
                    **job,
                    "deep_analysis": analysis
                }
                detailed_results.append(job_with_analysis)
                
            except Exception as e:
                logger.error(f"æ·±åº¦åˆ†æå¤±è´¥: {e}")
                # ä½¿ç”¨åŸºç¡€åˆ†æä½œä¸ºé™çº§æ–¹æ¡ˆ
                job_with_analysis = {
                    **job,
                    "deep_analysis": {
                        "summary": "æ·±åº¦åˆ†ææš‚æ—¶ä¸å¯ç”¨",
                        "match_score": job.get("score", 0),
                        "recommendations": []
                    }
                }
                detailed_results.append(job_with_analysis)
        
        return detailed_results
    
    # ========== è¾…åŠ©æ–¹æ³• ==========
    
    def _generate_job_id(self, job: Dict[str, Any]) -> str:
        """ç”Ÿæˆå²—ä½å”¯ä¸€ID"""
        if job.get('link'):
            return hashlib.md5(job['link'].encode()).hexdigest()[:16]
        
        key_content = f"{job.get('company','')}|{job.get('title','')}|{job.get('salary','')}"
        return hashlib.md5(key_content.encode()).hexdigest()[:16]
    
    def _get_cached_extraction(self, job_id: str) -> Optional[Dict]:
        """è·å–ç¼“å­˜çš„æå–ç»“æœ"""
        cache_file = self.cache_dir / "extracted" / f"{job_id}.json"
        
        if cache_file.exists():
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    # æ£€æŸ¥ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆ30å¤©ï¼‰
                    if self._is_cache_valid(data, days=30):
                        return data.get("extracted_info", {})
            except Exception:
                pass
        
        return None
    
    def _cache_extraction(self, job_id: str, extracted: Dict):
        """ç¼“å­˜æå–ç»“æœ"""
        cache_file = self.cache_dir / "extracted" / f"{job_id}.json"
        cache_file.parent.mkdir(parents=True, exist_ok=True)
        
        try:
            data = {
                "extracted_info": extracted,
                "timestamp": datetime.now().isoformat()
            }
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ç¼“å­˜å¤±è´¥: {e}")
    
    def _is_cache_valid(self, data: Dict, days: int) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if not data or 'timestamp' not in data:
            return False
        
        try:
            cache_time = datetime.fromisoformat(data['timestamp'])
            age = (datetime.now() - cache_time).days
            return age < days
        except Exception:
            return False
    
    def _build_batch_extract_prompt(self, jobs: List[Dict]) -> str:
        """æ„å»ºæ‰¹é‡æå–æç¤ºè¯"""
        jobs_text = ""
        for i, job in enumerate(jobs, 1):
            jobs_text += f"\nå²—ä½{i}:\n"
            jobs_text += f"æ ‡é¢˜: {job.get('title', '')}\n"
            jobs_text += f"å…¬å¸: {job.get('company', '')}\n"
            jobs_text += f"è–ªèµ„: {job.get('salary', '')}\n"
            jobs_text += f"æè¿°: {job.get('job_description', '')[:500]}...\n"
            jobs_text += "-" * 40
        
        prompt = f"""
æ‰¹é‡æå–ä»¥ä¸‹{len(jobs)}ä¸ªå²—ä½çš„å…³é”®ä¿¡æ¯ã€‚

å¯¹æ¯ä¸ªå²—ä½ï¼Œæå–ï¼š
1. skills_required: å¿…éœ€æŠ€èƒ½åˆ—è¡¨
2. experience_years: ç»éªŒè¦æ±‚ï¼ˆæ•°å­—ï¼‰
3. education: å­¦å†è¦æ±‚
4. job_type: å²—ä½ç±»å‹ï¼ˆæŠ€æœ¯/ç®¡ç†/è¿è¥/é”€å”®ç­‰ï¼‰

å²—ä½åˆ—è¡¨ï¼š
{jobs_text}

è¾“å‡ºJSONæ•°ç»„æ ¼å¼ï¼š
[
  {{
    "job_index": 1,
    "skills_required": ["Python", "SQL"],
    "experience_years": 3,
    "education": "æœ¬ç§‘",
    "job_type": "æŠ€æœ¯"
  }},
  ...
]
"""
        return prompt
    
    def _build_batch_score_prompt(self, jobs: List[Dict], resume_summary: Optional[str]) -> str:
        """æ„å»ºæ‰¹é‡è¯„åˆ†æç¤ºè¯"""
        jobs_text = ""
        for i, job in enumerate(jobs, 1):
            extracted = job.get("extracted_info", {})
            jobs_text += f"\nå²—ä½{i}:\n"
            jobs_text += f"æ ‡é¢˜: {job.get('title', '')}\n"
            jobs_text += f"å…¬å¸: {job.get('company', '')}\n"
            jobs_text += f"æŠ€èƒ½è¦æ±‚: {extracted.get('skills_required', [])}\n"
            jobs_text += f"ç»éªŒè¦æ±‚: {extracted.get('experience_years', '')}å¹´\n"
            jobs_text += "-" * 40
        
        if resume_summary:
            resume_text = f"\nç”¨æˆ·ç®€å†æ‘˜è¦ï¼š\n{resume_summary}\n"
        else:
            resume_text = "\nç”¨æˆ·èƒŒæ™¯ï¼šé€šç”¨æ±‚èŒè€…ï¼ˆæ— ç‰¹å®šç®€å†ï¼‰\n"
        
        prompt = f"""
è¯„ä¼°ä»¥ä¸‹å²—ä½çš„åŒ¹é…åº¦ï¼ˆ0-10åˆ†ï¼‰ã€‚
{resume_text}

è¯„åˆ†æ ‡å‡†ï¼š
- 0-3åˆ†ï¼šåŸºæœ¬ä¸åŒ¹é…
- 4-6åˆ†ï¼šéƒ¨åˆ†åŒ¹é…
- 7-10åˆ†ï¼šé«˜åº¦åŒ¹é…

å²—ä½åˆ—è¡¨ï¼š
{jobs_text}

è¾“å‡ºJSONæ•°ç»„ï¼š
[
  {{"job_index": 1, "score": 8.5, "reason": "æŠ€èƒ½é«˜åº¦åŒ¹é…ï¼Œç»éªŒç¬¦åˆè¦æ±‚"}},
  {{"job_index": 2, "score": 5.0, "reason": "æŠ€èƒ½éƒ¨åˆ†åŒ¹é…ï¼Œç»éªŒç•¥æœ‰ä¸è¶³"}},
  ...
]
"""
        return prompt
    
    def _build_deep_analysis_prompt(self, job: Dict, resume: Optional[Dict]) -> str:
        """æ„å»ºæ·±åº¦åˆ†ææç¤ºè¯"""
        job_info = f"""
å²—ä½ä¿¡æ¯ï¼š
- æ ‡é¢˜ï¼š{job.get('title', '')}
- å…¬å¸ï¼š{job.get('company', '')}
- è–ªèµ„ï¼š{job.get('salary', '')}
- æè¿°ï¼š{job.get('job_description', '')}
- åˆæ­¥è¯„åˆ†ï¼š{job.get('score', 0):.1f}åˆ†
- è¯„åˆ†ç†ç”±ï¼š{job.get('match_reason', '')}
"""
        
        if resume:
            resume_info = f"\nç”¨æˆ·ç®€å†ï¼š\n{json.dumps(resume, ensure_ascii=False, indent=2)}\n"
        else:
            resume_info = ""
        
        prompt = f"""
è¯·å¯¹ä»¥ä¸‹å²—ä½è¿›è¡Œæ·±åº¦åˆ†æï¼š

{job_info}
{resume_info}

è¯·æä¾›ï¼š
1. æ•´ä½“åŒ¹é…åº¦è¯„ä¼°ï¼ˆè¯¦ç»†è¯´æ˜ï¼‰
2. ä¼˜åŠ¿äº®ç‚¹ï¼ˆ3-5ä¸ªï¼‰
3. æ½œåœ¨æŒ‘æˆ˜ï¼ˆ2-3ä¸ªï¼‰
4. é¢è¯•å‡†å¤‡å»ºè®®ï¼ˆ3-5æ¡ï¼‰
5. è–ªèµ„è°ˆåˆ¤ç­–ç•¥
6. æ˜¯å¦æ¨èæŠ•é€’ï¼ˆå¼ºçƒˆæ¨è/æ¨è/è°¨æ…è€ƒè™‘/ä¸æ¨èï¼‰

è¾“å‡ºJSONæ ¼å¼ã€‚
"""
        return prompt
    
    def _parse_batch_extraction(self, response: str, jobs: List[Dict]) -> List[Dict]:
        """è§£ææ‰¹é‡æå–ç»“æœ"""
        try:
            # å°è¯•è§£æJSON
            if "```json" in response:
                json_str = response.split("```json")[1].split("```")[0].strip()
            else:
                json_str = response
            
            results = json.loads(json_str)
            
            # ç¡®ä¿è¿”å›æ­£ç¡®æ•°é‡çš„ç»“æœ
            extracted_list = []
            for i in range(len(jobs)):
                # æŸ¥æ‰¾å¯¹åº”çš„ç»“æœ
                found = False
                for result in results:
                    if result.get("job_index") == i + 1:
                        extracted_list.append(result)
                        found = True
                        break
                
                if not found:
                    # ç»“æœç¼ºå¤±ï¼ŒæŠ›å‡ºå¼‚å¸¸
                    raise ValueError(f"å²—ä½{i+1}çš„æå–ç»“æœç¼ºå¤±")
            
            return extracted_list
            
        except Exception as e:
            logger.error(f"è§£ææ‰¹é‡æå–ç»“æœå¤±è´¥: {e}")
            # æå–å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
            raise e
    
    def _parse_batch_scores(self, response: str, jobs: List[Dict]) -> List[Dict]:
        """è§£ææ‰¹é‡è¯„åˆ†ç»“æœ"""
        try:
            # å°è¯•è§£æJSON
            if "```json" in response:
                json_str = response.split("```json")[1].split("```")[0].strip()
            else:
                json_str = response
            
            results = json.loads(json_str)
            
            # ç¡®ä¿è¿”å›æ­£ç¡®æ•°é‡çš„ç»“æœ
            scores_list = []
            for i in range(len(jobs)):
                # æŸ¥æ‰¾å¯¹åº”çš„ç»“æœ
                found = False
                for result in results:
                    if result.get("job_index") == i + 1:
                        scores_list.append(result)
                        found = True
                        break
                
                if not found:
                    # ç»“æœç¼ºå¤±ï¼ŒæŠ›å‡ºå¼‚å¸¸
                    raise ValueError(f"å²—ä½{i+1}çš„è¯„åˆ†ç»“æœç¼ºå¤±")
            
            return scores_list
            
        except Exception as e:
            logger.error(f"è§£ææ‰¹é‡è¯„åˆ†ç»“æœå¤±è´¥: {e}")
            # è¯„åˆ†å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
            raise e
    
    def _parse_deep_analysis(self, response: str) -> Dict:
        """è§£ææ·±åº¦åˆ†æç»“æœ"""
        try:
            # å°è¯•è§£æJSON
            if "```json" in response:
                json_str = response.split("```json")[1].split("```")[0].strip()
            else:
                json_str = response
            
            return json.loads(json_str)
            
        except Exception as e:
            # è§£æå¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
            logger.error(f"æ·±åº¦åˆ†æç»“æœè§£æå¤±è´¥: {e}")
            raise e
    
    def _get_default_extraction(self) -> Dict:
        """è¿™ä¸ªå‡½æ•°ä¸åº”è¯¥è¢«ä½¿ç”¨"""
        raise NotImplementedError("é»˜è®¤æå–æœºåˆ¶å·²è¢«ç§»é™¤")
    
    def _extract_single(self, job: Dict) -> Dict:
        """å•ä¸ªå²—ä½æå–ï¼ˆé™çº§æ–¹æ¡ˆï¼‰"""
        try:
            prompt = f"""
æå–å²—ä½å…³é”®ä¿¡æ¯ï¼š
æ ‡é¢˜ï¼š{job.get('title', '')}
æè¿°ï¼š{job.get('job_description', '')[:500]}

è¾“å‡ºJSONï¼š
{{"skills_required": [], "experience_years": 0, "education": "", "job_type": ""}}
"""
            response = self.glm_client.call_api_simple(prompt, max_tokens=500)
            return json.loads(response)
        except Exception:
            return {"skills_required": [], "experience_years": 0}
    
    def _prepare_resume_summary(self, resume: Optional[Dict]) -> str:
        """å‡†å¤‡ç®€å†æ‘˜è¦"""
        if not resume:
            return "é€šç”¨æ±‚èŒè€…"
        
        summary = f"""
æŠ€èƒ½ï¼š{resume.get('skills', [])}
ç»éªŒï¼š{resume.get('experience_years', 0)}å¹´
æ•™è‚²ï¼š{resume.get('education', 'æœ¬ç§‘')}
æœŸæœ›è–ªèµ„ï¼š{resume.get('expected_salary', 'é¢è®®')}
"""
        return summary
    
    def _generate_report(self, scored_jobs: List[Dict], detailed_jobs: List[Dict]) -> Dict:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        report = {
            "total_jobs": len(scored_jobs),
            "analysis_time": datetime.now().isoformat(),
            "all_jobs_with_scores": scored_jobs,
            "top_jobs_detailed": detailed_jobs,
            "statistics": {
                "average_score": sum(j.get("score", 0) for j in scored_jobs) / len(scored_jobs) if scored_jobs else 0,
                "high_match_count": sum(1 for j in scored_jobs if j.get("score", 0) >= 7),
                "medium_match_count": sum(1 for j in scored_jobs if 4 <= j.get("score", 0) < 7),
                "low_match_count": sum(1 for j in scored_jobs if j.get("score", 0) < 4),
            },
            "cost_analysis": {
                "total_api_calls": self.stats["api_calls"],
                "cache_hits": self.stats["cache_hits"],
                "estimated_cost": f"${self.stats['estimated_cost']:.2f}"
            }
        }
        
        return report
    
    def _print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "=" * 50)
        print("ğŸ“Š åˆ†æç»Ÿè®¡")
        print("=" * 50)
        print(f"æ€»å²—ä½æ•°: {self.stats['total_jobs']}")
        print(f"ä¿¡æ¯æå–: {self.stats['extracted']}ä¸ª")
        print(f"è¯„åˆ†å®Œæˆ: {self.stats['scored']}ä¸ª")
        print(f"æ·±åº¦åˆ†æ: {self.stats['deep_analyzed']}ä¸ª")
        print(f"ç¼“å­˜å‘½ä¸­: {self.stats['cache_hits']}æ¬¡")
        print(f"APIè°ƒç”¨: {self.stats['api_calls']}æ¬¡")
        print(f"é¢„ä¼°æˆæœ¬: ${self.stats['estimated_cost']:.2f}")
        print("=" * 50)