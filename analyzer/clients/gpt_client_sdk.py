#!/usr/bin/env python3
"""
GPT APIå®¢æˆ·ç«¯ - ä½¿ç”¨å®˜æ–¹OpenAI SDK
çº¯APIè°ƒç”¨å™¨ï¼Œä¸åŒ…å«ä»»ä½•ä¸šåŠ¡é€»è¾‘å’Œæç¤ºè¯
"""

import os
from typing import Optional, Dict, Any
from dotenv import load_dotenv
from ..base_client import BaseAIClient

# åŠ è½½é…ç½®æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
config_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), 'config')
secrets_file = os.path.join(config_dir, 'secrets.env')
load_dotenv(secrets_file)

# å¯¼å…¥OpenAI SDK
try:
    from openai import OpenAI
    OPENAI_SDK_AVAILABLE = True
except ImportError:
    OPENAI_SDK_AVAILABLE = False
    print("âš ï¸ OpenAI SDKæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install openai")


class GPTClientSDK(BaseAIClient):
    """GPT APIå®¢æˆ·ç«¯ - ä½¿ç”¨å®˜æ–¹SDK"""
    
    def __init__(self, model_name: Optional[str] = None):
        """
        åˆå§‹åŒ–GPTå®¢æˆ·ç«¯
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨gpt-4o-mini
        """
        super().__init__(model_name)
        
        if not OPENAI_SDK_AVAILABLE:
            raise ImportError("OpenAI SDKæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install openai")
        
        # è®¾ç½®é»˜è®¤æ¨¡å‹
        if not self.model_name:
            try:
                from config.config_manager import ConfigManager
                config_manager = ConfigManager()
                self.model_name = config_manager.get_app_config('ai.models.gpt.model_name', 'gpt-4o-mini')
            except Exception:
                self.model_name = 'gpt-4o-mini'
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            print("âš ï¸ è­¦å‘Š: æœªè®¾ç½®OPENAI_API_KEYï¼Œè¯·åœ¨config/secrets.envæ–‡ä»¶ä¸­é…ç½®")
        
        self.client = OpenAI(api_key=api_key)
        
        # ä»é…ç½®è¯»å–å‚æ•°
        try:
            from config.config_manager import ConfigManager
            config_manager = ConfigManager()
            gpt_config = config_manager.get_app_config('ai.models.gpt', {})
            self.temperature = gpt_config.get('temperature', 0.3)
            self.max_tokens = gpt_config.get('max_tokens', 1000)
        except Exception:
            self.temperature = 0.3
            self.max_tokens = 1000
        
        print(f"ğŸ¤– GPTå®¢æˆ·ç«¯(SDKç‰ˆ)åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: {self.model_name}")
        
        # éªŒè¯é…ç½®
        self._validate_configuration()
    
    def _validate_configuration(self) -> None:
        """éªŒè¯GPTé…ç½®"""
        if not os.getenv('OPENAI_API_KEY'):
            print("âš ï¸ è­¦å‘Š: æœªè®¾ç½®OPENAI_API_KEYï¼ŒAPIè°ƒç”¨å°†å¤±è´¥")
    
    def call_api(self, system_prompt: str, user_prompt: str, **kwargs) -> str:
        """
        è°ƒç”¨GPT API - ç³»ç»Ÿæç¤ºè¯ + ç”¨æˆ·æç¤ºè¯æ¨¡å¼
        
        Args:
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            user_prompt: ç”¨æˆ·æç¤ºè¯
            **kwargs: å…¶ä»–å‚æ•°ï¼ˆtemperature, max_tokensç­‰ï¼‰
            
        Returns:
            AIå“åº”æ–‡æœ¬
        """
        # åˆå¹¶å‚æ•°
        temperature = kwargs.get('temperature', self.temperature)
        max_tokens = kwargs.get('max_tokens', self.max_tokens)
        model = kwargs.get('model', self.model_name)
        
        try:
            # ä½¿ç”¨å®˜æ–¹SDKè°ƒç”¨API
            response = self.client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            # è¿”å›å“åº”å†…å®¹
            return response.choices[0].message.content
            
        except Exception as e:
            error_msg = str(e)
            if "api_key" in error_msg.lower():
                raise Exception("OpenAI API Keyæœªé…ç½®æˆ–æ— æ•ˆ")
            elif "rate_limit" in error_msg.lower():
                raise Exception("OpenAI APIè¾¾åˆ°é€Ÿç‡é™åˆ¶ï¼Œè¯·ç¨åé‡è¯•")
            elif "model" in error_msg.lower():
                raise Exception(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model}")
            else:
                raise Exception(f"OpenAI APIè°ƒç”¨å¤±è´¥: {error_msg}")
    
    def call_api_simple(self, prompt: str, **kwargs) -> str:
        """
        ç®€å•APIè°ƒç”¨ - å•ä¸€æç¤ºè¯æ¨¡å¼
        
        Args:
            prompt: å®Œæ•´æç¤ºè¯
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            AIå“åº”æ–‡æœ¬
        """
        # åˆå¹¶å‚æ•°
        temperature = kwargs.get('temperature', self.temperature)
        max_tokens = kwargs.get('max_tokens', self.max_tokens)
        model = kwargs.get('model', self.model_name)
        
        try:
            # ä½¿ç”¨å®˜æ–¹SDKè°ƒç”¨API
            response = self.client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            # è¿”å›å“åº”å†…å®¹
            return response.choices[0].message.content
            
        except Exception as e:
            error_msg = str(e)
            if "api_key" in error_msg.lower():
                raise Exception("OpenAI API Keyæœªé…ç½®æˆ–æ— æ•ˆ")
            elif "rate_limit" in error_msg.lower():
                raise Exception("OpenAI APIè¾¾åˆ°é€Ÿç‡é™åˆ¶ï¼Œè¯·ç¨åé‡è¯•")
            else:
                raise Exception(f"OpenAI APIè°ƒç”¨å¤±è´¥: {error_msg}")
    
    def call_api_stream(self, system_prompt: str, user_prompt: str, **kwargs):
        """
        æµå¼APIè°ƒç”¨ - æ”¯æŒå®æ—¶å“åº”
        
        Args:
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            user_prompt: ç”¨æˆ·æç¤ºè¯
            **kwargs: å…¶ä»–å‚æ•°
            
        Yields:
            æµå¼å“åº”ç‰‡æ®µ
        """
        temperature = kwargs.get('temperature', self.temperature)
        max_tokens = kwargs.get('max_tokens', self.max_tokens)
        model = kwargs.get('model', self.model_name)
        
        try:
            # åˆ›å»ºæµå¼å“åº”
            stream = self.client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=temperature,
                max_tokens=max_tokens,
                stream=True
            )
            
            # é€å—è¿”å›å“åº”
            for chunk in stream:
                if chunk.choices[0].delta.content is not None:
                    yield chunk.choices[0].delta.content
                    
        except Exception as e:
            raise Exception(f"OpenAIæµå¼APIè°ƒç”¨å¤±è´¥: {e}")