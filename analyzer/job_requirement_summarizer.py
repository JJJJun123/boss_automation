#!/usr/bin/env python3
"""
AIå²—ä½è¦æ±‚æ€»ç»“å¼•æ“
åŸºäºè®¾è®¡æ–‡æ¡£ä¸­çš„AIæˆæœ¬æ§åˆ¶ç­–ç•¥ï¼Œæä¾›æ™ºèƒ½å²—ä½è¦æ±‚åˆ†æå’Œæ€»ç»“
"""

import logging
import time
import hashlib
import json
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path
from .ai_client_factory import AIClientFactory

logger = logging.getLogger(__name__)


@dataclass
class JobRequirementSummary:
    """å²—ä½è¦æ±‚æ€»ç»“æ•°æ®ç»“æ„"""
    core_responsibilities: List[str]  # æ ¸å¿ƒèŒè´£
    key_requirements: List[str]       # å…³é”®è¦æ±‚
    technical_skills: List[str]       # æŠ€æœ¯æŠ€èƒ½
    soft_skills: List[str]           # è½¯æŠ€èƒ½
    experience_level: str            # ç»éªŒè¦æ±‚
    education_requirement: str       # å­¦å†è¦æ±‚
    industry_background: str         # è¡Œä¸šèƒŒæ™¯
    compensation_range: str          # è–ªèµ„èŒƒå›´
    company_stage: str              # å…¬å¸é˜¶æ®µ
    growth_potential: str           # æˆé•¿æ½œåŠ›
    match_keywords: List[str]       # åŒ¹é…å…³é”®è¯
    summary_confidence: float       # æ€»ç»“ç½®ä¿¡åº¦ (0-1)


class JobRequirementSummarizer:
    """AIå²—ä½è¦æ±‚æ€»ç»“å¼•æ“"""
    
    def __init__(self, ai_provider: str = "deepseek"):
        self.ai_provider = ai_provider
        self.ai_client = AIClientFactory.create_client(ai_provider)
        
        # æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿï¼ˆå®ç°AIæˆæœ¬æ§åˆ¶ï¼‰
        self.cache_file = Path("data/job_requirements_cache.json")
        self.cache_data = self._load_cache()
        
        # æ‰¹é‡å¤„ç†é…ç½®
        self.batch_size = 5  # æ¯æ‰¹å¤„ç†çš„å²—ä½æ•°é‡
        self.cache_hit_threshold = 0.85  # ç¼“å­˜ç›¸ä¼¼åº¦é˜ˆå€¼
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            "total_processed": 0,
            "cache_hits": 0,
            "ai_calls": 0,
            "batch_calls": 0,
            "cost_savings": 0.0,
            "processing_time": 0.0
        }
    
    def _load_cache(self) -> Dict:
        """åŠ è½½ç¼“å­˜æ•°æ®"""
        try:
            if self.cache_file.exists():
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                    logger.info(f"ğŸ“š åŠ è½½å²—ä½è¦æ±‚ç¼“å­˜: {len(cache_data)} æ¡è®°å½•")
                    return cache_data
        except Exception as e:
            logger.warning(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
        
        return {}
    
    def _save_cache(self) -> None:
        """ä¿å­˜ç¼“å­˜æ•°æ®"""
        try:
            self.cache_file.parent.mkdir(parents=True, exist_ok=True)
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache_data, f, ensure_ascii=False, indent=2)
            logger.debug(f"ğŸ’¾ ç¼“å­˜å·²ä¿å­˜: {len(self.cache_data)} æ¡è®°å½•")
        except Exception as e:
            logger.error(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
    
    def _generate_job_hash(self, job_description: str, job_requirements: str) -> str:
        """ç”Ÿæˆå²—ä½å†…å®¹å“ˆå¸Œå€¼"""
        content = f"{job_description}\n{job_requirements}".strip()
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def _check_cache(self, job_hash: str) -> Optional[JobRequirementSummary]:
        """æ£€æŸ¥ç¼“å­˜ä¸­æ˜¯å¦æœ‰ç›¸ä¼¼çš„å²—ä½è¦æ±‚æ€»ç»“"""
        if job_hash in self.cache_data:
            cached = self.cache_data[job_hash]
            self.stats["cache_hits"] += 1
            logger.debug(f"ğŸ¯ ç¼“å­˜å‘½ä¸­: {job_hash[:8]}")
            
            # æ›´æ–°ç¼“å­˜ç»Ÿè®¡
            cached["cache_hits"] = cached.get("cache_hits", 0) + 1
            cached["last_accessed"] = time.time()
            
            return self._dict_to_summary(cached["summary"])
        
        return None
    
    def _dict_to_summary(self, data: Dict) -> JobRequirementSummary:
        """å°†å­—å…¸è½¬æ¢ä¸ºJobRequirementSummaryå¯¹è±¡"""
        return JobRequirementSummary(**data)
    
    def _summary_to_dict(self, summary: JobRequirementSummary) -> Dict:
        """å°†JobRequirementSummaryå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "core_responsibilities": summary.core_responsibilities,
            "key_requirements": summary.key_requirements,
            "technical_skills": summary.technical_skills,
            "soft_skills": summary.soft_skills,
            "experience_level": summary.experience_level,
            "education_requirement": summary.education_requirement,
            "industry_background": summary.industry_background,
            "compensation_range": summary.compensation_range,
            "company_stage": summary.company_stage,
            "growth_potential": summary.growth_potential,
            "match_keywords": summary.match_keywords,
            "summary_confidence": summary.summary_confidence
        }
    
    async def summarize_single_job(self, job_data: Dict) -> JobRequirementSummary:
        """æ€»ç»“å•ä¸ªå²—ä½è¦æ±‚"""
        start_time = time.time()
        
        # æå–å²—ä½æè¿°å’Œè¦æ±‚
        job_description = job_data.get('job_description', '')
        job_requirements = job_data.get('job_requirements', '')
        
        if not job_description and not job_requirements:
            # ä½¿ç”¨æ ‡é¢˜å’Œå…¬å¸ä¿¡æ¯ä½œä¸ºfallback
            job_description = f"èŒä½: {job_data.get('title', '')}\nå…¬å¸: {job_data.get('company', '')}"
        
        # æ£€æŸ¥ç¼“å­˜
        job_hash = self._generate_job_hash(job_description, job_requirements)
        cached_summary = self._check_cache(job_hash)
        
        if cached_summary:
            processing_time = time.time() - start_time
            self.stats["processing_time"] += processing_time
            return cached_summary
        
        # AIåˆ†æå²—ä½è¦æ±‚
        summary = await self._ai_analyze_job_requirements(
            job_data.get('title', ''),
            job_data.get('company', ''),
            job_description,
            job_requirements,
            job_data.get('salary', ''),
            job_data.get('work_location', '')
        )
        
        # ä¿å­˜åˆ°ç¼“å­˜
        self._cache_summary(job_hash, summary)
        
        processing_time = time.time() - start_time
        self.stats["processing_time"] += processing_time
        self.stats["total_processed"] += 1
        self.stats["ai_calls"] += 1
        
        return summary
    
    async def summarize_batch_jobs(self, jobs: List[Dict]) -> List[JobRequirementSummary]:
        """æ‰¹é‡æ€»ç»“å²—ä½è¦æ±‚ï¼ˆAIæˆæœ¬ä¼˜åŒ–ï¼‰"""
        logger.info(f"ğŸ”„ å¼€å§‹æ‰¹é‡åˆ†æ {len(jobs)} ä¸ªå²—ä½è¦æ±‚...")
        
        summaries = []
        uncached_jobs = []
        cached_results = {}
        
        # ç¬¬ä¸€æ­¥ï¼šæ£€æŸ¥ç¼“å­˜ï¼Œåˆ†ç¦»å·²ç¼“å­˜å’Œæœªç¼“å­˜çš„å²—ä½
        for i, job in enumerate(jobs):
            job_description = job.get('job_description', '')
            job_requirements = job.get('job_requirements', '')
            job_hash = self._generate_job_hash(job_description, job_requirements)
            
            cached_summary = self._check_cache(job_hash)
            if cached_summary:
                cached_results[i] = cached_summary
            else:
                uncached_jobs.append((i, job))
        
        logger.info(f"ğŸ“š ç¼“å­˜å‘½ä¸­: {len(cached_results)}/{len(jobs)} ä¸ªå²—ä½")
        
        # ç¬¬äºŒæ­¥ï¼šæ‰¹é‡å¤„ç†æœªç¼“å­˜çš„å²—ä½
        if uncached_jobs:
            logger.info(f"ğŸ§  éœ€è¦AIåˆ†æ: {len(uncached_jobs)} ä¸ªå²—ä½")
            
            # åˆ†æ‰¹å¤„ç†ä»¥ä¼˜åŒ–APIè°ƒç”¨
            for batch_start in range(0, len(uncached_jobs), self.batch_size):
                batch = uncached_jobs[batch_start:batch_start + self.batch_size]
                batch_summaries = await self._ai_batch_analyze_jobs(batch)
                
                for (original_index, job), summary in zip(batch, batch_summaries):
                    # ä¿å­˜åˆ°ç¼“å­˜
                    job_description = job.get('job_description', '')
                    job_requirements = job.get('job_requirements', '')
                    job_hash = self._generate_job_hash(job_description, job_requirements)
                    self._cache_summary(job_hash, summary)
                    
                    cached_results[original_index] = summary
                
                self.stats["batch_calls"] += 1
                self.stats["ai_calls"] += len(batch)
        
        # ç¬¬ä¸‰æ­¥ï¼šæŒ‰åŸå§‹é¡ºåºç»„è£…ç»“æœ
        for i in range(len(jobs)):
            summaries.append(cached_results[i])
        
        self.stats["total_processed"] += len(jobs)
        
        # è®¡ç®—æˆæœ¬èŠ‚çœ
        cache_hit_rate = len(cached_results) / len(jobs) if jobs else 0
        self.stats["cost_savings"] = cache_hit_rate * 0.8  # å‡è®¾ç¼“å­˜èŠ‚çœ80%æˆæœ¬
        
        logger.info(f"âœ… æ‰¹é‡åˆ†æå®Œæˆï¼Œç¼“å­˜å‘½ä¸­ç‡: {cache_hit_rate:.1%}")
        
        return summaries
    
    async def _ai_analyze_job_requirements(self, title: str, company: str, 
                                          description: str, requirements: str,
                                          salary: str, location: str) -> JobRequirementSummary:
        """AIåˆ†æå•ä¸ªå²—ä½è¦æ±‚"""
        
        prompt = f"""
è¯·åˆ†æä»¥ä¸‹å²—ä½çš„æ ¸å¿ƒè¦æ±‚å¹¶ç”Ÿæˆç»“æ„åŒ–æ€»ç»“ï¼š

å²—ä½ä¿¡æ¯ï¼š
- èŒä½ï¼š{title}
- å…¬å¸ï¼š{company}
- è–ªèµ„ï¼š{salary}
- åœ°ç‚¹ï¼š{location}

èŒä½æè¿°ï¼š
{description}

ä»»èŒè¦æ±‚ï¼š
{requirements}

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºåˆ†æç»“æœï¼š
{{
    "core_responsibilities": ["æ ¸å¿ƒèŒè´£1", "æ ¸å¿ƒèŒè´£2", "..."],
    "key_requirements": ["å…³é”®è¦æ±‚1", "å…³é”®è¦æ±‚2", "..."],
    "technical_skills": ["æŠ€æœ¯æŠ€èƒ½1", "æŠ€æœ¯æŠ€èƒ½2", "..."],
    "soft_skills": ["è½¯æŠ€èƒ½1", "è½¯æŠ€èƒ½2", "..."],
    "experience_level": "å·¥ä½œç»éªŒè¦æ±‚ï¼ˆå¦‚ï¼š3-5å¹´ï¼‰",
    "education_requirement": "å­¦å†è¦æ±‚ï¼ˆå¦‚ï¼šæœ¬ç§‘ä»¥ä¸Šï¼‰",
    "industry_background": "è¡Œä¸šèƒŒæ™¯è¦æ±‚",
    "compensation_range": "è–ªèµ„èŒƒå›´åˆ†æ",
    "company_stage": "å…¬å¸å‘å±•é˜¶æ®µ",
    "growth_potential": "æˆé•¿æ½œåŠ›è¯„ä¼°",
    "match_keywords": ["åŒ¹é…å…³é”®è¯1", "åŒ¹é…å…³é”®è¯2", "..."],
    "summary_confidence": 0.85
}}

æ³¨æ„ï¼š
1. æå–æœ€é‡è¦çš„3-5ä¸ªæ ¸å¿ƒèŒè´£
2. è¯†åˆ«ç¡¬æ€§è¦æ±‚vsåŠ åˆ†é¡¹
3. åŒºåˆ†æŠ€æœ¯æŠ€èƒ½å’Œè½¯æŠ€èƒ½
4. è¯„ä¼°å²—ä½çš„æˆé•¿æ½œåŠ›
5. summary_confidenceè¡¨ç¤ºåˆ†æçš„ç½®ä¿¡åº¦(0-1)
"""
        
        try:
            response = await self.ai_client.analyze_async(prompt)
            
            # è§£æJSONå“åº”
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                result_data = json.loads(json_str)
                return JobRequirementSummary(**result_data)
            else:
                raise ValueError("AIå“åº”æ ¼å¼ä¸æ­£ç¡®ï¼Œæ— æ³•è§£æJSON")
                
        except Exception as e:
            logger.error(f"AIåˆ†æå¤±è´¥: {e}")
            raise e
    
    async def _ai_batch_analyze_jobs(self, job_batch: List[Tuple[int, Dict]]) -> List[JobRequirementSummary]:
        """AIæ‰¹é‡åˆ†æå²—ä½ï¼ˆæˆæœ¬ä¼˜åŒ–ç­–ç•¥ï¼‰"""
        
        # æ„å»ºæ‰¹é‡åˆ†ææç¤ºè¯
        job_texts = []
        for i, (_, job) in enumerate(job_batch):
            job_text = f"""
å²—ä½ {i+1}:
- èŒä½ï¼š{job.get('title', '')}
- å…¬å¸ï¼š{job.get('company', '')}
- æè¿°ï¼š{job.get('job_description', '')[:500]}...
- è¦æ±‚ï¼š{job.get('job_requirements', '')[:500]}...
"""
            job_texts.append(job_text)
        
        batch_prompt = f"""
è¯·æ‰¹é‡åˆ†æä»¥ä¸‹ {len(job_batch)} ä¸ªå²—ä½çš„æ ¸å¿ƒè¦æ±‚ï¼š

{chr(10).join(job_texts)}

è¯·ä¸ºæ¯ä¸ªå²—ä½è¾“å‡ºä»¥ä¸‹JSONæ ¼å¼çš„åˆ†æç»“æœï¼Œç”¨æ•°ç»„åŒ…å«ï¼š
[
    {{
        "core_responsibilities": ["èŒè´£1", "èŒè´£2"],
        "key_requirements": ["è¦æ±‚1", "è¦æ±‚2"],
        "technical_skills": ["æŠ€èƒ½1", "æŠ€èƒ½2"],
        "soft_skills": ["è½¯æŠ€èƒ½1", "è½¯æŠ€èƒ½2"],
        "experience_level": "ç»éªŒè¦æ±‚",
        "education_requirement": "å­¦å†è¦æ±‚",
        "industry_background": "è¡Œä¸šèƒŒæ™¯",
        "compensation_range": "è–ªèµ„åˆ†æ",
        "company_stage": "å…¬å¸é˜¶æ®µ",
        "growth_potential": "æˆé•¿æ½œåŠ›",
        "match_keywords": ["å…³é”®è¯1", "å…³é”®è¯2"],
        "summary_confidence": 0.8
    }},
    ...
]

æ³¨æ„ï¼šæ•°ç»„é•¿åº¦å¿…é¡»ç­‰äºå²—ä½æ•°é‡({len(job_batch)})
"""
        
        try:
            response = await self.ai_client.analyze_async(batch_prompt)
            
            # è§£ææ‰¹é‡JSONå“åº”
            import re
            json_match = re.search(r'\[.*\]', response, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                results_data = json.loads(json_str)
                
                summaries = []
                for i, result_data in enumerate(results_data):
                    if i < len(job_batch):
                        summaries.append(JobRequirementSummary(**result_data))
                
                # å¦‚æœç»“æœæ•°é‡ä¸è¶³ï¼ŒæŠ›å‡ºå¼‚å¸¸
                if len(summaries) < len(job_batch):
                    raise ValueError(f"AIè¿”å›çš„ç»“æœæ•°é‡ä¸è¶³: æœŸæœ›{len(job_batch)}ä¸ªï¼Œå®é™…{len(summaries)}ä¸ª")
                
                return summaries
            else:
                logger.warning("æ‰¹é‡AIå“åº”æ ¼å¼ä¸æ­£ç¡®ï¼Œä½¿ç”¨å•ç‹¬åˆ†æ")
                # é™çº§åˆ°å•ç‹¬åˆ†æ
                summaries = []
                for _, job in job_batch:
                    summary = await self._ai_analyze_job_requirements(
                        job.get('title', ''),
                        job.get('company', ''),
                        job.get('job_description', ''),
                        job.get('job_requirements', ''),
                        job.get('salary', ''),
                        job.get('work_location', '')
                    )
                    summaries.append(summary)
                return summaries
                
        except Exception as e:
            logger.error(f"æ‰¹é‡AIåˆ†æå¤±è´¥: {e}")
            raise e
    
    def _create_fallback_summary(self, title: str, description: str, requirements: str) -> JobRequirementSummary:
        """è¿™ä¸ªå‡½æ•°ä¸åº”è¯¥è¢«ä½¿ç”¨ï¼Œä¿ç•™ä»…ä¸ºå‘åå…¼å®¹"""
        raise NotImplementedError("fallbackæœºåˆ¶å·²è¢«ç§»é™¤ï¼Œåˆ†æå¤±è´¥åº”ç›´æ¥æŠ¥é”™")
    
    def _cache_summary(self, job_hash: str, summary: JobRequirementSummary) -> None:
        """ç¼“å­˜å²—ä½è¦æ±‚æ€»ç»“"""
        self.cache_data[job_hash] = {
            "summary": self._summary_to_dict(summary),
            "created_time": time.time(),
            "cache_hits": 0,
            "last_accessed": time.time()
        }
        
        # å®šæœŸä¿å­˜ç¼“å­˜
        if len(self.cache_data) % 10 == 0:
            self._save_cache()
    
    def get_cost_savings_report(self) -> Dict:
        """è·å–æˆæœ¬èŠ‚çœæŠ¥å‘Š"""
        total_jobs = self.stats["total_processed"]
        cache_hits = self.stats["cache_hits"]
        ai_calls = self.stats["ai_calls"]
        
        if total_jobs > 0:
            cache_hit_rate = cache_hits / total_jobs
            estimated_cost_without_cache = total_jobs * 0.01  # å‡è®¾æ¯æ¬¡åˆ†ææˆæœ¬0.01å…ƒ
            estimated_cost_with_cache = ai_calls * 0.01
            cost_savings = estimated_cost_without_cache - estimated_cost_with_cache
            
            return {
                "cache_statistics": {
                    "total_processed": total_jobs,
                    "cache_hits": cache_hits,
                    "cache_hit_rate": f"{cache_hit_rate:.1%}",
                    "ai_calls_made": ai_calls,
                    "ai_calls_saved": cache_hits
                },
                "cost_analysis": {
                    "estimated_cost_without_cache": f"Â¥{estimated_cost_without_cache:.2f}",
                    "actual_cost_with_cache": f"Â¥{estimated_cost_with_cache:.2f}",
                    "total_savings": f"Â¥{cost_savings:.2f}",
                    "savings_rate": f"{(cost_savings/estimated_cost_without_cache)*100:.1f}%" if estimated_cost_without_cache > 0 else "0%"
                },
                "performance_metrics": {
                    "batch_calls": self.stats["batch_calls"],
                    "avg_processing_time": f"{self.stats['processing_time']/total_jobs:.2f}s" if total_jobs > 0 else "0s",
                    "cache_size": len(self.cache_data)
                }
            }
        
        return {"message": "æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®ç”ŸæˆæŠ¥å‘Š"}
    
    def cleanup_old_cache(self, max_age_days: int = 30) -> int:
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        current_time = time.time()
        max_age_seconds = max_age_days * 24 * 3600
        
        old_keys = []
        for key, data in self.cache_data.items():
            if current_time - data.get("last_accessed", 0) > max_age_seconds:
                old_keys.append(key)
        
        for key in old_keys:
            del self.cache_data[key]
        
        if old_keys:
            self._save_cache()
            logger.info(f"ğŸ§¹ æ¸…ç†è¿‡æœŸç¼“å­˜: {len(old_keys)} æ¡è®°å½•")
        
        return len(old_keys)
    
    def __del__(self):
        """ææ„æ—¶ä¿å­˜ç¼“å­˜"""
        try:
            self._save_cache()
        except:
            pass