#!/usr/bin/env python3
"""
è°ƒè¯•Bossç›´è˜å²—ä½è¯¦æƒ…é¡µé¢ç»“æ„
åˆ†æjob-sec-textã€å²—ä½èŒè´£ã€ä»»èŒè¦æ±‚ã€brand-nameç­‰å­—æ®µ
"""

import asyncio
import logging
import sys
import os
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


async def debug_job_detail_fields():
    """è°ƒè¯•å²—ä½è¯¦æƒ…é¡µé¢çš„å­—æ®µç»“æ„"""
    try:
        from crawler.real_playwright_spider import RealPlaywrightBossSpider
        
        logger.info("ğŸ” å¼€å§‹è°ƒè¯•å²—ä½è¯¦æƒ…é¡µé¢å­—æ®µç»“æ„...")
        
        spider = RealPlaywrightBossSpider(headless=False)
        
        if not await spider.start():
            logger.error("âŒ æ— æ³•å¯åŠ¨Playwright")
            return
        
        # å…ˆæœç´¢è·å–ä¸€äº›å²—ä½é“¾æ¥
        keyword = "é£é™©ç®¡ç†"
        city = "shanghai"
        search_url = f"https://www.zhipin.com/web/geek/job?query={keyword}&city=101020100"
        
        logger.info(f"ğŸŒ é¦–å…ˆè®¿é—®æœç´¢é¡µé¢è·å–å²—ä½é“¾æ¥: {search_url}")
        await spider.page.goto(search_url, wait_until="domcontentloaded")
        await asyncio.sleep(5)
        
        # è·å–å‰3ä¸ªå²—ä½é“¾æ¥
        job_links = []
        try:
            link_elements = await spider.page.query_selector_all('a[href*="job_detail"]')
            for elem in link_elements[:3]:
                href = await elem.get_attribute('href')
                if href:
                    full_url = f"https://www.zhipin.com{href}" if href.startswith('/') else href
                    job_links.append(full_url)
        except Exception as e:
            logger.error(f"è·å–å²—ä½é“¾æ¥å¤±è´¥: {e}")
            return
        
        if not job_links:
            logger.error("æœªæ‰¾åˆ°å²—ä½é“¾æ¥")
            return
        
        logger.info(f"ğŸ“‹ æ‰¾åˆ° {len(job_links)} ä¸ªå²—ä½é“¾æ¥ï¼Œå¼€å§‹åˆ†æè¯¦æƒ…é¡µé¢...")
        
        analysis_results = []
        
        for i, job_url in enumerate(job_links[:2]):  # åªåˆ†æå‰2ä¸ªï¼Œé¿å…æ—¶é—´è¿‡é•¿
            logger.info(f"\n{'='*60}")
            logger.info(f"ğŸ¯ åˆ†æå²—ä½ {i+1}: {job_url}")
            
            try:
                # å¯¼èˆªåˆ°å²—ä½è¯¦æƒ…é¡µ
                await spider.page.goto(job_url, wait_until="domcontentloaded")
                await asyncio.sleep(3)
                
                analysis = {
                    "job_index": i+1,
                    "url": job_url,
                    "fields": {}
                }
                
                # åˆ†æå„ç§å¯èƒ½çš„å­—æ®µé€‰æ‹©å™¨
                field_selectors = {
                    "å²—ä½æ ‡é¢˜": ['.job-name', '.job-title', 'h1'],
                    "å…¬å¸åç§°_brand": ['.brand-name', '.company-name', '.company-brand'],
                    "è–ªèµ„": ['.salary', '.job-salary'],
                    "å²—ä½èŒè´£_job_sec": ['.job-sec-text', '.job-detail-text', '.job-description'],
                    "ä»»èŒè¦æ±‚_job_require": ['.job-require', '.job-requirement', '.requirement'],
                    "å…¬å¸ç®€ä»‹": ['.company-intro', '.company-detail', '.company-desc'],
                    "ç¦åˆ©å¾…é‡": ['.job-tags', '.welfare', '.benefits']
                }
                
                # æµ‹è¯•æ¯ä¸ªå­—æ®µçš„é€‰æ‹©å™¨
                for field_name, selectors in field_selectors.items():
                    logger.info(f"   ğŸ” åˆ†æå­—æ®µ: {field_name}")
                    field_results = []
                    
                    for selector in selectors:
                        try:
                            elements = await spider.page.query_selector_all(selector)
                            for elem_idx, elem in enumerate(elements):
                                text = await elem.inner_text()
                                if text and text.strip():
                                    text = text.strip()[:200]  # é™åˆ¶é•¿åº¦
                                    field_results.append({
                                        "selector": selector,
                                        "element_index": elem_idx,
                                        "text": text,
                                        "text_length": len(text)
                                    })
                                    logger.info(f"      âœ… {selector}[{elem_idx}]: {text[:50]}...")
                        except Exception as e:
                            logger.debug(f"      âŒ {selector}: {e}")
                    
                    analysis["fields"][field_name] = field_results
                
                # ç‰¹åˆ«åˆ†æåŒ…å«"å²—ä½èŒè´£"å’Œ"ä»»èŒè¦æ±‚"æ–‡æœ¬çš„å…ƒç´ 
                logger.info("   ğŸ¯ ç‰¹åˆ«æœç´¢åŒ…å«å…³é”®è¯çš„å…ƒç´ :")
                keywords = ["å²—ä½èŒè´£", "ä»»èŒè¦æ±‚", "å·¥ä½œèŒè´£", "èŒä½è¦æ±‚", "å²—ä½è¦æ±‚"]
                
                for keyword in keywords:
                    try:
                        # ä½¿ç”¨XPathæŸ¥æ‰¾åŒ…å«ç‰¹å®šæ–‡æœ¬çš„å…ƒç´ 
                        elements = await spider.page.query_selector_all(f'text="{keyword}"')
                        if not elements:
                            # å°è¯•éƒ¨åˆ†åŒ¹é…
                            elements = await spider.page.query_selector_all('*')
                            matching_elements = []
                            for elem in elements:
                                text = await elem.inner_text()
                                if text and keyword in text:
                                    matching_elements.append(elem)
                            elements = matching_elements[:3]  # é™åˆ¶æ•°é‡
                        
                        for elem_idx, elem in enumerate(elements[:2]):  # åªå–å‰2ä¸ª
                            try:
                                text = await elem.inner_text()
                                if text and len(text) > 10:
                                    # è·å–çˆ¶å…ƒç´ å’Œå­å…ƒç´ ä¿¡æ¯
                                    parent_text = ""
                                    try:
                                        parent = await elem.evaluate("el => el.parentElement")
                                        if parent:
                                            parent_text = await parent.inner_text()
                                    except:
                                        pass
                                    
                                    logger.info(f"      ğŸ¯ {keyword}[{elem_idx}]: {text[:100]}...")
                                    if parent_text and parent_text != text:
                                        logger.info(f"         çˆ¶å…ƒç´ : {parent_text[:50]}...")
                            except Exception as e:
                                logger.debug(f"         åˆ†æ{keyword}å…ƒç´ å¤±è´¥: {e}")
                    except Exception as e:
                        logger.debug(f"   æœç´¢{keyword}å¤±è´¥: {e}")
                
                # æˆªå›¾ä¿å­˜å½“å‰é¡µé¢
                screenshot_path = f"job_detail_{i+1}_{datetime.now().strftime('%H%M%S')}.png"
                await spider.page.screenshot(path=screenshot_path)
                logger.info(f"ğŸ“¸ é¡µé¢æˆªå›¾: {screenshot_path}")
                
                analysis_results.append(analysis)
                
            except Exception as e:
                logger.error(f"âŒ åˆ†æå²—ä½ {i+1} å¤±è´¥: {e}")
        
        # ä¿å­˜åˆ†æç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        analysis_path = f"job_detail_field_analysis_{timestamp}.json"
        
        with open(analysis_path, "w", encoding="utf-8") as f:
            json.dump(analysis_results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"\nğŸ“Š å­—æ®µåˆ†æå®Œæˆ!")
        logger.info(f"ğŸ“ åˆ†ææŠ¥å‘Š: {analysis_path}")
        
        # ç»™å‡ºé€‰æ‹©å™¨å»ºè®®
        logger.info(f"\nğŸ’¡ åŸºäºåˆ†æç»“æœçš„é€‰æ‹©å™¨å»ºè®®:")
        
        all_successful_selectors = {}
        for result in analysis_results:
            for field, field_results in result["fields"].items():
                if field_results:
                    if field not in all_successful_selectors:
                        all_successful_selectors[field] = {}
                    for item in field_results:
                        selector = item["selector"]
                        if selector in all_successful_selectors[field]:
                            all_successful_selectors[field][selector] += 1
                        else:
                            all_successful_selectors[field][selector] = 1
        
        for field, selectors in all_successful_selectors.items():
            if selectors:
                best_selector = max(selectors.items(), key=lambda x: x[1])
                logger.info(f"   {field}: æ¨è '{best_selector[0]}' (æˆåŠŸ{best_selector[1]}æ¬¡)")
        
        await spider.close()
        return True
        
    except Exception as e:
        logger.error(f"âŒ è°ƒè¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” Bossç›´è˜å²—ä½è¯¦æƒ…å­—æ®µåˆ†æå·¥å…·")
    print("=" * 50)
    
    success = asyncio.run(debug_job_detail_fields())
    
    if success:
        print("\nâœ… å­—æ®µåˆ†æå®Œæˆï¼Œè¯·æŸ¥çœ‹ç”Ÿæˆçš„æŠ¥å‘Š")
    else:
        print("\nâŒ åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")


if __name__ == "__main__":
    main()