#!/usr/bin/env python3
"""
å®æ—¶è°ƒè¯•Bossç›´è˜é€‰æ‹©å™¨é—®é¢˜
åˆ†æå½“å‰é¡µé¢çš„å®é™…DOMç»“æ„ï¼Œæ‰¾å‡ºæ­£ç¡®çš„é€‰æ‹©å™¨
"""

import asyncio
import logging
import sys
import os
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


async def analyze_job_elements():
    """åˆ†æå²—ä½å…ƒç´ çš„å®é™…ç»“æ„"""
    try:
        from crawler.real_playwright_spider import RealPlaywrightBossSpider
        
        logger.info("ğŸ” å¼€å§‹å®æ—¶åˆ†æBossç›´è˜é¡µé¢ç»“æ„...")
        
        # åˆ›å»ºçˆ¬è™«å®ä¾‹ï¼ˆå¯è§æ¨¡å¼ï¼‰
        spider = RealPlaywrightBossSpider(headless=False)
        
        if not await spider.start():
            logger.error("âŒ æ— æ³•å¯åŠ¨Playwright")
            return
        
        # æœç´¢æµ‹è¯•
        keyword = "é£é™©ç®¡ç†"
        city = "shanghai"
        city_code = spider.city_codes.get(city, "101020100")
        search_url = f"https://www.zhipin.com/web/geek/job?query={keyword}&city={city_code}"
        
        logger.info(f"ğŸŒ è®¿é—®: {search_url}")
        await spider.page.goto(search_url, wait_until="domcontentloaded")
        
        # ç­‰å¾…é¡µé¢åŠ è½½
        await asyncio.sleep(8)
        
        # å¤„ç†å¯èƒ½çš„å¼¹çª—
        try:
            await asyncio.sleep(2)
            # å°è¯•å…³é—­å¯èƒ½çš„å¼¹çª—
            close_buttons = await spider.page.query_selector_all('[class*="close"], .dialog-close, [aria-label="close"]')
            for btn in close_buttons:
                try:
                    await btn.click()
                    logger.info("âœ… å…³é—­äº†ä¸€ä¸ªå¼¹çª—")
                    await asyncio.sleep(1)
                except:
                    pass
        except:
            pass
        
        # æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤š
        await spider.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
        await asyncio.sleep(3)
        
        # åˆ†æé¡µé¢ç»“æ„
        logger.info("ğŸ” åˆ†æé¡µé¢ä¸­çš„æ‰€æœ‰å²—ä½å…ƒç´ ...")
        
        # å°è¯•æˆ‘ä»¬å½“å‰çš„é€‰æ‹©å™¨
        current_selectors = [
            '.job-list-container li',
            'li[class*="job"]',
            'div[class*="job-card"]',
            '.job-detail-box'
        ]
        
        job_elements = []
        used_selector = ""
        
        for selector in current_selectors:
            try:
                elements = await spider.page.query_selector_all(selector)
                if elements and len(elements) > 5:  # éœ€è¦æ‰¾åˆ°å¤šä¸ªå…ƒç´ 
                    logger.info(f"âœ… é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")
                    job_elements = elements[:10]  # åªåˆ†æå‰10ä¸ª
                    used_selector = selector
                    break
            except Exception as e:
                logger.debug(f"é€‰æ‹©å™¨ '{selector}' å¼‚å¸¸: {e}")
        
        if not job_elements:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°å²—ä½å…ƒç´ ï¼Œå°è¯•é€šç”¨åˆ†æ...")
            return False
        
        logger.info(f"ğŸ“‹ ä½¿ç”¨é€‰æ‹©å™¨: {used_selector}")
        logger.info(f"ğŸ“Š åˆ†æå‰5ä¸ªå²—ä½å…ƒç´ çš„å†…éƒ¨ç»“æ„...")
        
        analysis_results = []
        
        for i, job_elem in enumerate(job_elements[:5]):
            logger.info(f"\n{'='*60}")
            logger.info(f"ğŸ¯ åˆ†æå²—ä½å…ƒç´  {i+1}")
            
            try:
                # è·å–å…ƒç´ çš„æ–‡æœ¬å†…å®¹
                elem_text = await job_elem.inner_text()
                logger.info(f"ğŸ“ å…ƒç´ æ–‡æœ¬é¢„è§ˆ: {elem_text[:150]}...")
                
                # è·å–å…ƒç´ çš„HTMLç»“æ„ï¼ˆæˆªæ–­æ˜¾ç¤ºï¼‰
                elem_html = await job_elem.inner_html()
                
                # æµ‹è¯•å„ç§å¯èƒ½çš„é€‰æ‹©å™¨
                field_tests = {
                    "æ ‡é¢˜": [
                        '.job-name', '.job-title', 'h3', 'a[class*="name"]', 
                        '.position-name', '[class*="title"]', 'span.job-name',
                        'div[class*="name"]', '.job-card-left .name'
                    ],
                    "å…¬å¸": [
                        '.company-name', '.company-text', 'h3:not(.job-name)', 
                        '.company', '[class*="company"]', '.job-company',
                        'div[class*="company"]', 'span[class*="company"]'
                    ],
                    "è–ªèµ„": [
                        '.salary', '.red', '[class*="salary"]', '.job-limit',
                        '.price', '[class*="price"]', '[class*="pay"]',
                        'span[class*="salary"]', 'div[class*="salary"]'
                    ],
                    "åœ°ç‚¹": [
                        '.job-area', '.area', '[class*="area"]', '.location',
                        '.job-location', '[class*="location"]', '.address',
                        'span[class*="area"]', 'div[class*="location"]'
                    ]
                }
                
                element_analysis = {"index": i+1, "text_preview": elem_text[:200]}
                
                for field_name, selectors in field_tests.items():
                    found_results = []
                    
                    for selector in selectors:
                        try:
                            matches = await job_elem.query_selector_all(selector)
                            for match in matches:
                                text = await match.inner_text()
                                if text and text.strip() and len(text.strip()) > 1:
                                    # é¿å…é‡å¤å’Œæ— æ„ä¹‰çš„æ–‡æœ¬
                                    text = text.strip()
                                    if (len(text) < 200 and 
                                        text not in [t[1] for t in found_results] and
                                        not any(t in text.lower() for t in ['javascript', 'function', 'null', 'undefined'])):
                                        found_results.append((selector, text))
                        except Exception as e:
                            continue
                    
                    if found_results:
                        logger.info(f"   ğŸ¯ {field_name}å­—æ®µ:")
                        for selector, text in found_results[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªç»“æœ
                            logger.info(f"      {selector}: '{text[:50]}'")
                        element_analysis[field_name.lower()] = found_results[:5]
                    else:
                        logger.warning(f"   âŒ {field_name}: æœªæ‰¾åˆ°åŒ¹é…")
                        element_analysis[field_name.lower()] = []
                
                # åˆ†æé“¾æ¥
                link_selectors = [
                    'a', 'a[href*="job_detail"]', 'a[href*="job"]',
                    '[data-url]', '[onclick*="job"]', '.job-card-body'
                ]
                
                found_links = []
                for selector in link_selectors:
                    try:
                        links = await job_elem.query_selector_all(selector)
                        for link in links:
                            href = await link.get_attribute('href')
                            if href and ('job' in href or 'detail' in href):
                                found_links.append((selector, href))
                    except:
                        continue
                
                if found_links:
                    logger.info(f"   ğŸ”— æ‰¾åˆ°é“¾æ¥:")
                    for selector, href in found_links[:2]:
                        logger.info(f"      {selector}: {href[:60]}...")
                    element_analysis['links'] = found_links[:3]
                else:
                    logger.warning(f"   âŒ æœªæ‰¾åˆ°å²—ä½é“¾æ¥")
                    element_analysis['links'] = []
                
                analysis_results.append(element_analysis)
                
            except Exception as e:
                logger.error(f"   âŒ åˆ†æå²—ä½å…ƒç´  {i+1} å¤±è´¥: {e}")
        
        # ä¿å­˜åˆ†æç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        analysis_path = f"live_selector_analysis_{timestamp}.json"
        
        with open(analysis_path, "w", encoding="utf-8") as f:
            json.dump(analysis_results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"\nğŸ“Š åˆ†æå®Œæˆï¼ç»“æœä¿å­˜åˆ°: {analysis_path}")
        
        # åŸºäºåˆ†æç»“æœç»™å‡ºå»ºè®®
        logger.info("\nğŸ’¡ é€‰æ‹©å™¨ä¼˜åŒ–å»ºè®®:")
        
        # ç»Ÿè®¡æœ€æœ‰æ•ˆçš„é€‰æ‹©å™¨
        for field in ["æ ‡é¢˜", "å…¬å¸", "è–ªèµ„", "åœ°ç‚¹"]:
            field_key = field.lower()
            all_selectors = []
            
            for result in analysis_results:
                if field_key in result:
                    all_selectors.extend([item[0] for item in result[field_key]])
            
            # ç»Ÿè®¡é¢‘ç‡
            from collections import Counter
            selector_counts = Counter(all_selectors)
            
            if selector_counts:
                best_selector = selector_counts.most_common(1)[0]
                logger.info(f"   {field}: æ¨èé€‰æ‹©å™¨ '{best_selector[0]}' (å‡ºç°{best_selector[1]}æ¬¡)")
            else:
                logger.warning(f"   {field}: æœªæ‰¾åˆ°æœ‰æ•ˆé€‰æ‹©å™¨")
        
        # æˆªå›¾ä¿å­˜å½“å‰é¡µé¢
        screenshot_path = f"live_page_{timestamp}.png"
        await spider.page.screenshot(path=screenshot_path)
        logger.info(f"ğŸ“¸ é¡µé¢æˆªå›¾ä¿å­˜: {screenshot_path}")
        
        await spider.close()
        return True
        
    except Exception as e:
        logger.error(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” Bossç›´è˜å®æ—¶é€‰æ‹©å™¨åˆ†æå·¥å…·")
    print("=" * 50)
    
    success = asyncio.run(analyze_job_elements())
    
    if success:
        print("\nâœ… åˆ†æå®Œæˆ")
        print("ğŸ’¡ åŸºäºåˆ†æç»“æœï¼Œæˆ‘ä»¬å¯ä»¥æ›´æ–°é€‰æ‹©å™¨ä»¥ä¿®å¤æ•°æ®æå–é—®é¢˜")
        print("ğŸ“ è¯·æŸ¥çœ‹ç”Ÿæˆçš„JSONæ–‡ä»¶è·å–è¯¦ç»†åˆ†æç»“æœ")
    else:
        print("\nâŒ åˆ†æå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")


if __name__ == "__main__":
    main()