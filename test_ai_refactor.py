#!/usr/bin/env python3
"""
AIç³»ç»Ÿé‡æ„éªŒè¯æµ‹è¯•
éªŒè¯é‡æ„åçš„ç»“æœä¸åŸºå‡†æ•°æ®æ˜¯å¦ä¸€è‡´
"""

import os
import sys
import json
import asyncio
from typing import Dict, Any
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from analyzer.job_analyzer import JobAnalyzer

# ä»åŸºå‡†æ•°æ®æ–‡ä»¶è¯»å–æµ‹è¯•æ ·æœ¬
def load_baseline_data():
    """åŠ è½½åŸºå‡†æ•°æ®"""
    baseline_file = "data/ai_baseline_20250807_130703.json"
    
    if not os.path.exists(baseline_file):
        print(f"âŒ åŸºå‡†æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {baseline_file}")
        return None
    
    with open(baseline_file, 'r', encoding='utf-8') as f:
        return json.load(f)

def compare_analysis_results(baseline_result, new_result, test_name):
    """æ¯”è¾ƒåˆ†æç»“æœ"""
    print(f"\nğŸ” å¯¹æ¯”æµ‹è¯•: {test_name}")
    print("=" * 50)
    
    # æå–å…³é”®å­—æ®µè¿›è¡Œæ¯”è¾ƒ
    if isinstance(baseline_result, list) and len(baseline_result) > 0:
        # JobAnalyzerè¿”å›çš„æ˜¯åˆ—è¡¨ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ çš„åˆ†æç»“æœ
        baseline_analysis = baseline_result[0].get('analysis', {})
    elif isinstance(baseline_result, dict) and 'analysis' in baseline_result:
        baseline_analysis = baseline_result['analysis']
    else:
        baseline_analysis = baseline_result
    
    if isinstance(new_result, list) and len(new_result) > 0:
        new_analysis = new_result[0].get('analysis', {})
    elif isinstance(new_result, dict) and 'analysis' in new_result:
        new_analysis = new_result['analysis']
    else:
        new_analysis = new_result
    
    # æ¯”è¾ƒå…³é”®æŒ‡æ ‡
    comparisons = []
    
    # æ¯”è¾ƒæ€»åˆ†
    baseline_score = baseline_analysis.get('overall_score', 0)
    new_score = new_analysis.get('overall_score', 0)
    score_diff = abs(float(baseline_score) - float(new_score))
    comparisons.append(("overall_score", baseline_score, new_score, score_diff))
    
    # æ¯”è¾ƒæ¨èç­‰çº§
    baseline_rec = baseline_analysis.get('recommendation', '')
    new_rec = new_analysis.get('recommendation', '')
    rec_match = baseline_rec == new_rec
    comparisons.append(("recommendation", baseline_rec, new_rec, rec_match))
    
    # æ¯”è¾ƒç»´åº¦è¯„åˆ†ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if 'dimension_scores' in baseline_analysis and 'dimension_scores' in new_analysis:
        baseline_dims = baseline_analysis['dimension_scores']
        new_dims = new_analysis['dimension_scores']
        
        for dim in ['job_match', 'skill_match', 'experience_match']:
            if dim in baseline_dims and dim in new_dims:
                baseline_dim_score = baseline_dims[dim]
                new_dim_score = new_dims[dim] 
                dim_diff = abs(float(baseline_dim_score) - float(new_dim_score))
                comparisons.append((f"dimension.{dim}", baseline_dim_score, new_dim_score, dim_diff))
    
    # è¾“å‡ºå¯¹æ¯”ç»“æœ
    print("ğŸ“Š å¯¹æ¯”ç»“æœ:")
    all_passed = True
    
    for metric, baseline_val, new_val, diff_or_match in comparisons:
        if metric == "recommendation":
            status = "âœ…" if diff_or_match else "âŒ"
            if not diff_or_match:
                all_passed = False
            print(f"  {status} {metric}: '{baseline_val}' â†’ '{new_val}' ({'åŒ¹é…' if diff_or_match else 'ä¸åŒ¹é…'})")
        else:
            # æ•°å€¼æ¯”è¾ƒï¼Œå…è®¸å°å¹…å·®å¼‚
            tolerance = 1.0  # å…è®¸1åˆ†çš„å·®å¼‚
            status = "âœ…" if diff_or_match <= tolerance else "âŒ" 
            if diff_or_match > tolerance:
                all_passed = False
            print(f"  {status} {metric}: {baseline_val} â†’ {new_val} (å·®å¼‚: {diff_or_match:.1f})")
    
    return all_passed

class AIRefactorValidator:
    """AIé‡æ„éªŒè¯å™¨"""
    
    def __init__(self, baseline_data):
        self.baseline_data = baseline_data
        self.test_results = {}
    
    def test_job_analyzer_refactored(self):
        """æµ‹è¯•é‡æ„åçš„JobAnalyzer"""
        print("ğŸ§ª æµ‹è¯•é‡æ„åçš„JobAnalyzer...")
        
        baseline_test = self.baseline_data.get('job_analyzer_deepseek')
        if not baseline_test:
            print("âŒ åŸºå‡†æ•°æ®ä¸­æ²¡æœ‰job_analyzer_deepseekæµ‹è¯•")
            return False
            
        try:
            # è·å–åŸºå‡†è¾“å…¥æ•°æ®
            baseline_input = baseline_test['input']
            job_info = baseline_input['job_info']
            resume_analysis = baseline_input['resume_analysis']
            baseline_output = baseline_test['output']
            
            # åˆ›å»ºé‡æ„åçš„JobAnalyzer
            analyzer = JobAnalyzer(ai_provider='deepseek')
            analyzer.set_resume_analysis(resume_analysis)
            
            # è¿è¡Œåˆ†æ
            new_result = analyzer.analyze_jobs([job_info])
            
            # å¯¹æ¯”ç»“æœ
            passed = compare_analysis_results(baseline_output, new_result, "JobAnalyzeré‡æ„")
            
            self.test_results['job_analyzer_refactored'] = {
                'passed': passed,
                'baseline_score': baseline_output[0]['analysis']['overall_score'] if isinstance(baseline_output, list) else baseline_output.get('analysis', {}).get('overall_score', 0),
                'new_score': new_result[0]['analysis']['overall_score'] if new_result else 0,
                'method': 'analyze_jobs'
            }
            
            return passed
            
        except Exception as e:
            print(f"âŒ JobAnalyzeræµ‹è¯•å¤±è´¥: {e}")
            self.test_results['job_analyzer_refactored'] = {
                'passed': False,
                'error': str(e)
            }
            return False
    
    def test_direct_ai_methods(self):
        """æµ‹è¯•JobAnalyzerä¸­æ–°æ·»åŠ çš„ç›´æ¥AIæ–¹æ³•"""
        print("\nğŸ§ª æµ‹è¯•JobAnalyzerçš„ç›´æ¥AIæ–¹æ³•...")
        
        baseline_ai_service = self.baseline_data.get('ai_service_job_match')
        if not baseline_ai_service:
            print("âŒ åŸºå‡†æ•°æ®ä¸­æ²¡æœ‰ai_service_job_matchæµ‹è¯•")
            return False
        
        try:
            # è·å–åŸºå‡†è¾“å…¥æ•°æ®
            baseline_input = baseline_ai_service['input']
            job_info = baseline_input['job_info']
            resume_analysis = baseline_input['resume_analysis']
            baseline_output = baseline_ai_service['output']
            
            # åˆ›å»ºJobAnalyzerå¹¶æµ‹è¯•æ–°æ–¹æ³•
            analyzer = JobAnalyzer(ai_provider='deepseek')
            
            # æµ‹è¯•analyze_job_matchæ–¹æ³•ï¼ˆä»AIServiceç§»è¿‡æ¥çš„ï¼‰
            new_result = analyzer.analyze_job_match(job_info, resume_analysis)
            
            # å¯¹æ¯”ç»“æœ
            passed = compare_analysis_results(baseline_output, new_result, "ç›´æ¥AIæ–¹æ³•")
            
            self.test_results['direct_ai_method'] = {
                'passed': passed,
                'baseline_score': baseline_output.get('overall_score', 0),
                'new_score': new_result.get('overall_score', 0),
                'method': 'analyze_job_match'
            }
            
            return passed
            
        except Exception as e:
            print(f"âŒ ç›´æ¥AIæ–¹æ³•æµ‹è¯•å¤±è´¥: {e}")
            self.test_results['direct_ai_method'] = {
                'passed': False,
                'error': str(e)
            }
            return False
    
    def run_validation(self):
        """è¿è¡Œå®Œæ•´éªŒè¯"""
        print("ğŸš€ å¼€å§‹AIç³»ç»Ÿé‡æ„éªŒè¯")
        print("=" * 60)
        
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        tests = [
            self.test_job_analyzer_refactored,
            self.test_direct_ai_methods
        ]
        
        results = []
        for test in tests:
            try:
                result = test()
                results.append(result)
            except Exception as e:
                print(f"âŒ æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {e}")
                results.append(False)
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        self.generate_report(results)
        
        return all(results)
    
    def generate_report(self, results):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("ğŸ“‹ é‡æ„éªŒè¯æŠ¥å‘Š")
        print("=" * 60)
        
        passed_count = sum(1 for r in results if r)
        total_count = len(results)
        
        print(f"æ€»æµ‹è¯•æ•°: {total_count}")
        print(f"é€šè¿‡æµ‹è¯•: {passed_count}")
        print(f"å¤±è´¥æµ‹è¯•: {total_count - passed_count}")
        print(f"é€šè¿‡ç‡: {passed_count/total_count*100:.1f}%")
        
        print("\nğŸ“Š è¯¦ç»†ç»“æœ:")
        for test_name, test_data in self.test_results.items():
            if test_data.get('passed', False):
                print(f"  âœ… {test_name}: é€šè¿‡")
                if 'baseline_score' in test_data and 'new_score' in test_data:
                    print(f"      è¯„åˆ†å¯¹æ¯”: {test_data['baseline_score']} â†’ {test_data['new_score']}")
            else:
                print(f"  âŒ {test_name}: å¤±è´¥")
                if 'error' in test_data:
                    print(f"      é”™è¯¯: {test_data['error']}")
        
        if all(results):
            print(f"\nğŸ‰ é‡æ„æˆåŠŸï¼æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Œå¯ä»¥å®‰å…¨ç§»é™¤AIServiceå±‚ã€‚")
        else:
            print(f"\nâš ï¸ é‡æ„å­˜åœ¨é—®é¢˜ï¼Œè¯·æ£€æŸ¥å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹ã€‚")

def main():
    """ä¸»å‡½æ•°"""
    # åŠ è½½åŸºå‡†æ•°æ®
    baseline_data = load_baseline_data()
    if not baseline_data:
        return
    
    # è¿è¡ŒéªŒè¯
    validator = AIRefactorValidator(baseline_data)
    success = validator.run_validation()
    
    return success

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)