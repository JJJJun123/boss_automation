#!/usr/bin/env python3
"""
è°ƒè¯•å•ä¸ªå²—ä½æå–é—®é¢˜
"""

import asyncio
import logging
from crawler.unified_crawler_interface import unified_search_jobs

# è®¾ç½®è¯¦ç»†æ—¥å¿—
logging.basicConfig(
    level=logging.DEBUG,
    format='%(name)s - %(levelname)s - %(message)s'
)

# åªæ˜¾ç¤ºå…³é”®æ¨¡å—çš„æ—¥å¿—
logging.getLogger('crawler.enhanced_extractor').setLevel(logging.DEBUG)
logging.getLogger('crawler.smart_selector').setLevel(logging.DEBUG)
logging.getLogger('crawler.real_playwright_spider').setLevel(logging.INFO)

logger = logging.getLogger(__name__)


async def debug_single_job():
    """è°ƒè¯•å•ä¸ªå²—ä½æå–"""
    print("ğŸ” è°ƒè¯•å•ä¸ªå²—ä½æå–é—®é¢˜")
    print("="*50)
    
    try:
        results = await unified_search_jobs(
            keyword="æ•°æ®åˆ†æå¸ˆ",
            city="shanghai", 
            max_jobs=1,  # åªæµ‹è¯•1ä¸ªå²—ä½
            engine="enhanced_playwright"
        )
        
        print(f"\nğŸ“Š æœ€ç»ˆç»“æœ:")
        print(f"å²—ä½æ•°é‡: {len(results)}")
        
        if results:
            job = results[0]
            print("âœ… æˆåŠŸæå–å²—ä½:")
            for key, value in job.items():
                print(f"  {key}: {value}")
        else:
            print("âŒ æœªæå–åˆ°ä»»ä½•å²—ä½")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(debug_single_job())