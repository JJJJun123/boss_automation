#!/usr/bin/env python3
"""
Bossç›´è˜è‡ªåŠ¨åŒ–Webåº”ç”¨åç«¯
Flask + SocketIO å®ç°
"""

import os
import sys
import logging
import threading
from flask import Flask, request, jsonify, session, render_template
from flask_cors import CORS
from flask_socketio import SocketIO, emit
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config.config_manager import ConfigManager
from crawler.unified_crawler_interface import unified_search_jobs, get_crawler_capabilities
from analyzer.job_analyzer import JobAnalyzer
from analyzer.enhanced_job_analyzer import EnhancedJobAnalyzer


# åˆ›å»ºFlaskåº”ç”¨
app = Flask(__name__)
app.secret_key = 'boss-zhipin-automation-secret-key-2024'  # æ·»åŠ secret keyç”¨äºsession

# é…ç½®CORS
CORS(app, origins=["http://localhost:3000", "http://127.0.0.1:3000", "http://localhost:5000", "http://127.0.0.1:5000"])

# é…ç½®SocketIO
socketio = SocketIO(app, cors_allowed_origins=["http://localhost:3000", "http://127.0.0.1:3000", "http://localhost:5000", "http://127.0.0.1:5000"])

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å…¨å±€å˜é‡
config_manager = None
current_spider = None
current_job = None  # å­˜å‚¨å½“å‰åˆ†æä»»åŠ¡çŠ¶æ€


def init_config():
    """åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨"""
    global config_manager
    try:
        config_manager = ConfigManager()
        logger.info("é…ç½®ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        return True
    except Exception as e:
        logger.error(f"é…ç½®ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        return False


def emit_progress(message, progress=None, data=None):
    """å‘é€è¿›åº¦æ›´æ–°åˆ°å‰ç«¯"""
    payload = {
        'message': message,
        'timestamp': datetime.now().strftime('%H:%M:%S')
    }
    if progress is not None:
        payload['progress'] = progress
    if data is not None:
        payload['data'] = data
    
    socketio.emit('progress_update', payload)
    logger.info(f"Progress: {message}")


@app.route('/')
def serve_frontend():
    """æä¾›å‰ç«¯é¡µé¢"""
    return render_template('index.html')


@app.route('/api/health')
def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return jsonify({
        'status': 'healthy',
        'timestamp': datetime.now().isoformat(),
        'version': '1.0.0'
    })


@app.route('/api/config', methods=['GET'])
def get_config():
    """è·å–å½“å‰é…ç½®"""
    try:
        if not config_manager:
            return jsonify({'error': 'é…ç½®ç®¡ç†å™¨æœªåˆå§‹åŒ–'}), 500
        
        search_config = config_manager.get_search_config()
        ai_config = config_manager.get_ai_config()
        
        # ç§»é™¤æ•æ„Ÿä¿¡æ¯
        ai_config.pop('api_key', None)
        
        # è·å–å¯ç”¨çš„AIæ¨¡å‹
        from analyzer.ai_client_factory import AIClientFactory
        available_models = AIClientFactory.get_available_models()
        
        return jsonify({
            'search': search_config,
            'ai': ai_config,
            'app': config_manager.get_app_config(),
            'available_ai_models': available_models
        })
    except Exception as e:
        logger.error(f"è·å–é…ç½®å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/config', methods=['POST'])
def update_config():
    """æ›´æ–°ç”¨æˆ·é…ç½®"""
    try:
        if not config_manager:
            return jsonify({'error': 'é…ç½®ç®¡ç†å™¨æœªåˆå§‹åŒ–'}), 500
        
        data = request.get_json()
        
        # æ›´æ–°æœç´¢é…ç½®
        if 'search' in data:
            for key, value in data['search'].items():
                config_manager.set_user_preference(f'search.{key}', value)
        
        # æ›´æ–°AIé…ç½®
        if 'ai_analysis' in data:
            for key, value in data['ai_analysis'].items():
                config_manager.set_user_preference(f'ai_analysis.{key}', value)
        
        # ä¿å­˜é…ç½®
        config_manager.save_user_preferences()
        
        return jsonify({'message': 'é…ç½®æ›´æ–°æˆåŠŸ'})
    except Exception as e:
        logger.error(f"æ›´æ–°é…ç½®å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/upload_resume', methods=['POST'])
def upload_resume():
    """å¤„ç†ç®€å†ä¸Šä¼ å’Œåˆ†æ"""
    try:
        if 'resume' not in request.files:
            return jsonify({'success': False, 'error': 'æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶'})
        
        file = request.files['resume']
        if file.filename == '':
            return jsonify({'success': False, 'error': 'æœªé€‰æ‹©æ–‡ä»¶'})
        
        logger.info(f"æ¥æ”¶åˆ°æ–‡ä»¶: {file.filename}, ç±»å‹: {file.content_type}")
        
        # ä½¿ç”¨æ–°çš„è§£æå™¨
        from analyzer.resume.resume_parser_v2 import ResumeParserV2
        from analyzer.resume.resume_analyzer_v2 import ResumeAnalyzerV2
        
        parser = ResumeParserV2()
        
        # è§£æç®€å†æ–‡æœ¬ï¼ˆç›´æ¥ä»å†…å­˜è§£æï¼Œä¸ä¿å­˜æ–‡ä»¶ï¼‰
        resume_text = parser.parse_uploaded_file(file)
        
        if "æ–‡ä»¶è§£æå¤±è´¥" in resume_text:
            logger.error(f"æ–‡ä»¶è§£æé”™è¯¯: {resume_text}")
            return jsonify({
                'success': False, 
                'error': resume_text
            })
        
        logger.info(f"ç®€å†è§£ææˆåŠŸï¼Œæ–‡æœ¬é•¿åº¦: {len(resume_text)} å­—ç¬¦")
        
        # AIåˆ†æç®€å†
        analyzer = ResumeAnalyzerV2()
        ai_analysis = analyzer.analyze_resume(resume_text)
        
        # ä»AIåˆ†æç»“æœä¸­æå–åŸºæœ¬ä¿¡æ¯ç”¨äºæ˜¾ç¤º
        # ç®€åŒ–å¤„ç†ï¼Œç›´æ¥ä½¿ç”¨AIåˆ†æçš„ç»“æœ
        resume_data = {
            'name': 'é™ˆä¿Šæ—­', # ä¸´æ—¶ç¡¬ç¼–ç ï¼Œåç»­å¯ä»ç®€å†æ–‡æœ¬ä¸­ç®€å•æå–
            'current_position': 'å¾…AIåˆ†æ',
            'experience_years': 'å¾…AIåˆ†æ',
            'phone': 'å·²è„±æ•',
            'email': 'å·²è„±æ•', 
            'technical_skills': [],
            'education': 'å¾…AIåˆ†æ',
            'filename': file.filename,
            'upload_time': datetime.now().isoformat()
        }
        
        # å­˜å‚¨åˆ°sessionï¼ˆé¿å…å­˜å‚¨å¤§æ–‡æœ¬ï¼‰
        session['resume_data'] = resume_data
        # åªå­˜å‚¨æ‘˜è¦ä¿¡æ¯ï¼Œä¸å­˜å‚¨å®Œæ•´æ–‡æœ¬
        session['resume_summary'] = {
            'length': len(resume_text),
            'has_text': True,
            'analyzed': True
        }
        # åªå­˜å‚¨å…³é”®åˆ†æç»“æœï¼Œé¿å…sessionè¿‡å¤§
        session['ai_analysis_summary'] = {
            'competitiveness_score': ai_analysis.get('competitiveness_score', 0),
            'recommended_jobs': ai_analysis.get('recommended_jobs', []),
            'analyzed_at': datetime.now().isoformat()
        }
        
        # æ›´æ–°å…¨å±€åˆ†æå™¨
        global job_analyzer_instance
        if 'job_analyzer_instance' not in globals():
            # è·å–AIé…ç½®
            try:
                config_manager = ConfigManager()
                ai_config = config_manager.get_app_config('ai', {})
                use_enhanced_analyzer = ai_config.get('use_enhanced_analyzer', True)
                
                if use_enhanced_analyzer:
                    print("ğŸš€ åˆ›å»ºå¢å¼ºå‹ç®€å†åˆ†æå™¨ï¼ˆGLM+DeepSeekæ··åˆæ¨¡å¼ï¼‰")
                    job_analyzer_instance = EnhancedJobAnalyzer(
                        extraction_provider="glm",
                        analysis_provider="deepseek"
                    )
                else:
                    print("ğŸ”„ åˆ›å»ºä¼ ç»Ÿç®€å†åˆ†æå™¨")
                    job_analyzer_instance = JobAnalyzer()
            except Exception as e:
                print(f"âš ï¸ é…ç½®è¯»å–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å¢å¼ºåˆ†æå™¨: {e}")
                job_analyzer_instance = EnhancedJobAnalyzer(
                    extraction_provider="glm",
                    analysis_provider="deepseek"
                )
        
        # è®¾ç½®ç®€å†åˆ†ææ•°æ®ï¼ˆå…¼å®¹ä¸¤ç§åˆ†æå™¨ï¼‰
        if hasattr(job_analyzer_instance, 'set_resume_analysis'):
            job_analyzer_instance.set_resume_analysis(ai_analysis)
        else:
            job_analyzer_instance.resume_analysis = ai_analysis
        
        logger.info(f"ç®€å†åˆ†æå®Œæˆ: {resume_data['name']}, ç«äº‰åŠ›è¯„åˆ†: {ai_analysis.get('competitiveness_score')}/10")
        
        return jsonify({
            'success': True,
            'resume_data': resume_data,
            'ai_analysis': ai_analysis
        })
        
    except Exception as e:
        logger.error(f"ç®€å†ä¸Šä¼ å¤„ç†å¤±è´¥: {e}", exc_info=True)
        return jsonify({
            'success': False, 
            'error': f"å¤„ç†å¤±è´¥: {str(e)}"
        })

@app.route('/api/delete_resume', methods=['POST'])
def delete_resume():
    try:
        # æ¸…é™¤sessionä¸­çš„ç®€å†æ•°æ®
        session.pop('resume_data', None)
        session.pop('ai_analysis', None)
        
        # æ¸…é™¤JobAnalyzerä¸­çš„ç®€å†åˆ†æ
        global job_analyzer_instance
        if 'job_analyzer_instance' in globals():
            job_analyzer_instance.resume_analysis = None
        
        # è¿™é‡Œå¯ä»¥æ·»åŠ åˆ é™¤æ–‡ä»¶çš„é€»è¾‘
        
        return jsonify({'success': True})
        
    except Exception as e:
        print(f"åˆ é™¤ç®€å†å¤±è´¥: {e}")
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/jobs/search', methods=['POST'])
def start_job_search():
    """å¼€å§‹å²—ä½æœç´¢å’Œåˆ†æ"""
    global current_job, current_spider
    
    try:
        if current_job and current_job.get('status') == 'running':
            return jsonify({'error': 'å·²æœ‰ä»»åŠ¡æ­£åœ¨è¿è¡Œä¸­'}), 400
        
        # è·å–è¯·æ±‚å‚æ•°
        data = request.get_json() or {}
        
        # å¯åŠ¨åå°ä»»åŠ¡
        current_job = {'status': 'starting', 'start_time': datetime.now()}
        
        # åœ¨æ–°çº¿ç¨‹ä¸­æ‰§è¡Œæœç´¢ä»»åŠ¡
        thread = threading.Thread(target=run_job_search_task, args=(data,))
        thread.daemon = True
        thread.start()
        
        return jsonify({
            'message': 'ä»»åŠ¡å·²å¯åŠ¨',
            'task_id': current_job.get('task_id', 'default')
        })
        
    except Exception as e:
        logger.error(f"å¯åŠ¨æœç´¢ä»»åŠ¡å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500


def run_job_search_task(params):
    """åœ¨åå°è¿è¡Œå²—ä½æœç´¢ä»»åŠ¡"""
    global current_job, current_spider
    
    try:
        current_job['status'] = 'running'
        emit_progress("ğŸš€ å¼€å§‹åˆå§‹åŒ–çˆ¬è™«...", 5)
        
        # 1. ä»å‰ç«¯å‚æ•°è·å–æœç´¢é…ç½®ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        search_config = config_manager.get_search_config()
        ai_config = config_manager.get_ai_config()
        
        # ä½¿ç”¨å‰ç«¯ä¼ æ¥çš„å‚æ•°è¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„å€¼
        keyword = params.get('keyword', search_config['keyword'])
        max_jobs = params.get('max_jobs', search_config['max_jobs'])
        selected_city = params.get('city', 'shanghai')  # é»˜è®¤ä¸Šæµ·
        ai_model = params.get('ai_model')  # ç”¨æˆ·é€‰æ‹©çš„AIæ¨¡å‹
        
        # è·å–åŸå¸‚ä»£ç 
        city_codes = search_config['city_codes']
        city_code = city_codes.get(selected_city, {}).get('code', '101210100')
        city_name = city_codes.get(selected_city, {}).get('name', 'ä¸Šæµ·')
        
        emit_progress(f"ğŸ” æœç´¢è®¾ç½®: {keyword} | {city_name} | {max_jobs}ä¸ªå²—ä½", 10)
        
        # 2. ä½¿ç”¨ç»Ÿä¸€çˆ¬è™«å¼•æ“æœç´¢å²—ä½
        emit_progress("ğŸ•·ï¸ å¯åŠ¨ç»Ÿä¸€çˆ¬è™«å¼•æ“...", 20)
        
        # åŸå¸‚ä»£ç æ˜ å°„åˆ°åŸå¸‚åç§°
        city_map = {
            "101280600": "shenzhen",    # æ·±åœ³
            "101020100": "shanghai",    # ä¸Šæµ·
            "101010100": "beijing",     # åŒ—äº¬
            "101210100": "hangzhou"     # æ­å·
        }
        city_name = city_map.get(city_code, "shanghai")
        
        # ä½¿ç”¨ç»Ÿä¸€çˆ¬è™«æ¥å£è¿›è¡Œæœç´¢
        import asyncio
        jobs = asyncio.run(unified_search_jobs(keyword, city_name, max_jobs))
        
        emit_progress(f"ğŸ” æœç´¢å®Œæˆ: æ‰¾åˆ° {len(jobs)} ä¸ªå²—ä½", 50)
        
        if not jobs:
            raise Exception("æœªæ‰¾åˆ°ä»»ä½•å²—ä½")
        
        emit_progress(f"ğŸ“Š æ‰¾åˆ° {len(jobs)} ä¸ªå²—ä½ï¼Œå¼€å§‹AIåˆ†æ...", 50)
        
        # 5. AIåˆ†æ - æ”¯æŒåŠ¨æ€æ¨¡å‹é€‰æ‹©
        global job_analyzer_instance
        
        # å¦‚æœç”¨æˆ·æŒ‡å®šäº†AIæ¨¡å‹ï¼Œæˆ–è€…æ²¡æœ‰ç°æœ‰å®ä¾‹ï¼Œåˆ›å»ºæ–°çš„åˆ†æå™¨
        if ai_model or 'job_analyzer_instance' not in globals():
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨æ··åˆAIæ¨¡å¼ï¼ˆGLM+DeepSeekï¼‰
            use_enhanced_analyzer = ai_config.get('use_enhanced_analyzer', True)  # é»˜è®¤å¯ç”¨
            
            if use_enhanced_analyzer:
                print(f"ğŸš€ åˆ›å»ºå¢å¼ºå‹JobAnalyzerå®ä¾‹ï¼ˆGLM+DeepSeekæ··åˆæ¨¡å¼ï¼‰")
                
                # ä¿å­˜ä¹‹å‰çš„ç®€å†åˆ†ææ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
                previous_resume_analysis = None
                if 'job_analyzer_instance' in globals() and hasattr(job_analyzer_instance, 'resume_analysis'):
                    previous_resume_analysis = job_analyzer_instance.resume_analysis
                
                # åˆ›å»ºå¢å¼ºåˆ†æå™¨å®ä¾‹
                job_analyzer_instance = EnhancedJobAnalyzer(
                    extraction_provider="glm",  # GLM-4.5ç”¨äºä¿¡æ¯æå–
                    analysis_provider="deepseek"  # DeepSeekç”¨äºåˆ†æ
                )
                
                # æ¢å¤ç®€å†åˆ†ææ•°æ®
                if previous_resume_analysis:
                    job_analyzer_instance.resume_analysis = previous_resume_analysis
                    print("ğŸ¯ å·²æ¢å¤ç®€å†æ•°æ®åˆ°å¢å¼ºåˆ†æå™¨å®ä¾‹")
            else:
                print(f"ğŸ”„ åˆ›å»ºä¼ ç»ŸJobAnalyzerå®ä¾‹ï¼Œæ¨¡å‹: {ai_model or ai_config['provider']}")
                
                # ä¿å­˜ä¹‹å‰çš„ç®€å†åˆ†ææ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
                previous_resume_analysis = None
                if 'job_analyzer_instance' in globals() and hasattr(job_analyzer_instance, 'resume_analysis'):
                    previous_resume_analysis = job_analyzer_instance.resume_analysis
                
                # åˆ›å»ºä¼ ç»Ÿåˆ†æå™¨å®ä¾‹
                if ai_model:
                    job_analyzer_instance = JobAnalyzer(model_name=ai_model)
                else:
                    job_analyzer_instance = JobAnalyzer(ai_provider=ai_config['provider'])
                
                # æ¢å¤ç®€å†åˆ†ææ•°æ®
                if previous_resume_analysis:
                    job_analyzer_instance.resume_analysis = previous_resume_analysis
                    print("ğŸ¯ å·²æ¢å¤ç®€å†æ•°æ®åˆ°ä¼ ç»Ÿåˆ†æå™¨å®ä¾‹")
        else:
            # ä½¿ç”¨ç°æœ‰å®ä¾‹
            if hasattr(job_analyzer_instance, 'resume_analysis') and job_analyzer_instance.resume_analysis:
                print("ğŸ¯ ä½¿ç”¨å·²åŠ è½½çš„ç®€å†æ•°æ®è¿›è¡Œæ™ºèƒ½åŒ¹é…")
        
        analyzer = job_analyzer_instance
        
        # ä¸ºæ‰€æœ‰å²—ä½åˆå§‹åŒ–åˆ†æç»“æœ
        all_jobs_with_analysis = []
        
        # ä½¿ç”¨æ–°çš„æ‰¹é‡åˆ†ææ–¹æ³•ï¼ˆåŒ…å«AIå²—ä½è¦æ±‚æ€»ç»“å’Œæˆæœ¬ä¼˜åŒ–ï¼‰
        emit_progress("ğŸ§  å¯åŠ¨AIå²—ä½è¦æ±‚æ€»ç»“å’Œæ™ºèƒ½åˆ†æ...", 50)
        
        try:
            # ä½¿ç”¨æ–°çš„åˆ†ææ–¹æ³•ï¼ˆåŒ…å«å²—ä½è¦æ±‚æ€»ç»“ï¼‰
            all_jobs_with_analysis = analyzer.analyze_jobs(jobs)
            
            # å¸‚åœºåˆ†æå·²å®Œæˆï¼Œä¸å†éœ€è¦å•ç‹¬çš„æˆæœ¬æŠ¥å‘Š
            emit_progress(f"ğŸ“Š å¸‚åœºåˆ†æå®Œæˆ", 80)
            
        except Exception as e:
            logger.error(f"æ–°åˆ†ææ–¹æ³•å¤±è´¥ï¼Œé™çº§åˆ°ä¼ ç»Ÿåˆ†æ: {e}")
            # é‡æ–°åˆå§‹åŒ–ï¼Œé¿å…é‡å¤æ•°æ®
            all_jobs_with_analysis = []
            # é™çº§åˆ°ä¼ ç»Ÿåˆ†ææ–¹æ³•
            for i, job in enumerate(jobs):
                progress = 50 + (i / len(jobs)) * 30
                emit_progress(f"ğŸ¤– åˆ†æç¬¬ {i+1}/{len(jobs)} ä¸ªå²—ä½...", progress)
                
                try:
                    analysis_result = analyzer.ai_client.analyze_job_match(
                        job, analyzer.user_requirements
                    )
                    job['analysis'] = analysis_result
                except Exception as e:
                    logger.error(f"åˆ†æå²—ä½å¤±è´¥: {e}")
                    job['analysis'] = {
                        "score": 0,
                        "recommendation": "åˆ†æå¤±è´¥",
                        "reason": f"åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}",
                        "summary": "æ— æ³•åˆ†ææ­¤å²—ä½"
                    }
                
                all_jobs_with_analysis.append(job)
        
        # 6. è¿‡æ»¤å’Œæ’åº
        emit_progress("ğŸ¯ è¿‡æ»¤å’Œæ’åºç»“æœ...", 85)
        # ç­›é€‰åˆæ ¼å²—ä½
        filtered_jobs = analyzer.filter_and_sort_jobs(all_jobs_with_analysis, ai_config['min_score'])
        
        # 7. ä¿å­˜ç»“æœ
        emit_progress("ğŸ’¾ ä¿å­˜ç»“æœ...", 95)
        # ä½¿ç”¨æ–°çš„ä¿å­˜å‡½æ•°ï¼Œä¿å­˜æ‰€æœ‰å²—ä½
        from utils.data_saver import save_all_job_results
        save_all_job_results(all_jobs_with_analysis, filtered_jobs)  # ä¿å­˜æ‰€æœ‰å²—ä½
        
        # 8. è·å–å¸‚åœºåˆ†æç»“æœ
        market_analysis = analyzer.get_market_analysis() if analyzer else None
        
        # 9. å®Œæˆ
        current_job.update({
            'status': 'completed',
            'end_time': datetime.now(),
            'results': filtered_jobs,
            'analyzed_jobs': all_jobs_with_analysis,  # å­˜å‚¨æ‰€æœ‰å²—ä½
            'total_jobs': len(jobs),
            'analyzed_jobs_count': len(all_jobs_with_analysis),
            'qualified_jobs': len(filtered_jobs),
            'market_analysis': market_analysis  # ä¿å­˜å¸‚åœºåˆ†æ
        })
        
        emit_progress(f"âœ… ä»»åŠ¡å®Œæˆ! æ‰¾åˆ° {len(filtered_jobs)} ä¸ªåˆé€‚å²—ä½", 100, {
            'results': filtered_jobs,
            'all_jobs': all_jobs_with_analysis,  # è¿”å›æ‰€æœ‰å²—ä½
            'market_analysis': market_analysis,  # ä¼ é€’å¸‚åœºåˆ†æ
            'stats': {
                'total': len(jobs),  # ä½¿ç”¨åŸå§‹æŠ“å–çš„å²—ä½æ•°é‡
                'analyzed': len(all_jobs_with_analysis),  # åˆ†æçš„å²—ä½æ•°é‡
                'qualified': len(filtered_jobs)  # åˆæ ¼çš„å²—ä½æ•°é‡
            }
        })
        
    except Exception as e:
        logger.error(f"æœç´¢ä»»åŠ¡å¤±è´¥: {e}")
        current_job.update({
            'status': 'failed',
            'error': str(e),
            'end_time': datetime.now()
        })
        emit_progress(f"âŒ ä»»åŠ¡å¤±è´¥: {str(e)}", None)
        
    finally:
        # æ¸…ç†èµ„æº
        if current_spider:
            try:
                current_spider.close()
            except:
                pass
            current_spider = None


@app.route('/api/jobs/all')
def get_all_jobs():
    """è·å–æ‰€æœ‰æœç´¢åˆ°çš„å²—ä½ï¼ˆæœªè¿‡æ»¤ï¼‰"""
    try:
        from utils.data_saver import load_all_job_results
        
        # å°è¯•ä»ä¿å­˜çš„æ–‡ä»¶ä¸­è¯»å–æ‰€æœ‰å²—ä½
        job_data = load_all_job_results()
        if job_data and 'all_jobs' in job_data:
            all_jobs = job_data['all_jobs']
            logger.info(f"âœ… ä»æ–‡ä»¶åŠ è½½äº† {len(all_jobs)} ä¸ªå²—ä½")
            return jsonify({
                'jobs': all_jobs,
                'total': len(all_jobs),
                'metadata': job_data.get('metadata', {})
            })
        
        # å¦‚æœæ–‡ä»¶ä¸­æ²¡æœ‰æ•°æ®ï¼Œfallbackåˆ°current_job
        if current_job and 'analyzed_jobs' in current_job:
            jobs = current_job.get('analyzed_jobs', [])
            logger.info(f"âš ï¸ ä»å†…å­˜åŠ è½½äº† {len(jobs)} ä¸ªå²—ä½")
            return jsonify({
                'jobs': jobs,
                'total': len(jobs)
            })
        
        return jsonify({'error': 'æ²¡æœ‰å¯ç”¨çš„æœç´¢ç»“æœï¼Œè¯·å…ˆè¿›è¡Œæœç´¢'}), 404
        
    except Exception as e:
        logger.error(f"è·å–æ‰€æœ‰å²—ä½å¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/jobs/results')
def get_job_results():
    """è·å–æœ€æ–°çš„å²—ä½æœç´¢ç»“æœ"""
    try:
        if not current_job:
            return jsonify({'error': 'æ²¡æœ‰å¯ç”¨çš„æœç´¢ç»“æœ'}), 404
        
        return jsonify({
            'status': current_job.get('status'),
            'results': current_job.get('results', []),
            'stats': {
                'total_jobs': current_job.get('total_jobs', 0),
                'analyzed_jobs': current_job.get('analyzed_jobs', 0),
                'qualified_jobs': current_job.get('qualified_jobs', 0)
            },
            'start_time': current_job.get('start_time'),
            'end_time': current_job.get('end_time')
        })
        
    except Exception as e:
        logger.error(f"è·å–ç»“æœå¤±è´¥: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/api/jobs/status')
def get_job_status():
    """è·å–å½“å‰ä»»åŠ¡çŠ¶æ€"""
    if not current_job:
        return jsonify({'status': 'idle'})
    
    return jsonify({
        'status': current_job.get('status', 'idle'),
        'start_time': current_job.get('start_time'),
        'error': current_job.get('error')
    })


@socketio.on('connect')
def handle_connect():
    """WebSocketè¿æ¥å¤„ç†"""
    logger.info('å®¢æˆ·ç«¯å·²è¿æ¥')
    emit('connected', {'message': 'è¿æ¥æˆåŠŸ'})


@socketio.on('disconnect')
def handle_disconnect():
    """WebSocketæ–­å¼€è¿æ¥å¤„ç†"""
    logger.info('å®¢æˆ·ç«¯å·²æ–­å¼€è¿æ¥')


if __name__ == '__main__':
    # åˆå§‹åŒ–é…ç½®
    if not init_config():
        logger.error("é…ç½®åˆå§‹åŒ–å¤±è´¥ï¼Œé€€å‡ºç¨‹åº")
        sys.exit(1)
    
    # å¯åŠ¨åº”ç”¨
    logger.info("å¯åŠ¨Bossç›´è˜è‡ªåŠ¨åŒ–Webåº”ç”¨...")
    socketio.run(app, 
                host='127.0.0.1', 
                port=5000, 
                debug=True,
                use_reloader=False,
                allow_unsafe_werkzeug=True)  # é¿å…é‡è½½æ—¶çš„é—®é¢˜