#!/usr/bin/env python3
"""
å¢å¼ºçš„å²—ä½è¯¦æƒ…æŠ“å–æ–¹æ¡ˆ
ç»•è¿‡åçˆ¬è™«æœºåˆ¶ï¼Œç²¾ç¡®æå–å²—ä½æè¿°ã€ä»»èŒè¦æ±‚å’Œå…¬å¸è¯¦æƒ…
"""

import asyncio
import logging
from typing import Dict, Optional

logger = logging.getLogger(__name__)


async def fetch_job_details_enhanced(self, url: str, job_data: Dict) -> Dict:
    """
    å¢å¼ºçš„å²—ä½è¯¦æƒ…æŠ“å–æ–¹æ³•
    è®¿é—®è¯¦æƒ…é¡µå¹¶ç²¾ç¡®æå–å²—ä½èŒè´£ã€ä»»èŒè¦æ±‚ç­‰ä¿¡æ¯
    
    Args:
        url: å²—ä½è¯¦æƒ…é¡µURL
        job_data: å·²æœ‰çš„å²—ä½æ•°æ®
    
    Returns:
        Dict: æ›´æ–°åçš„å²—ä½æ•°æ®
    """
    if not url or not url.startswith('http'):
        return job_data
    
    try:
        # åˆ›å»ºæ–°é¡µé¢é¿å…å½±å“ä¸»é¡µé¢
        detail_page = await self.browser.new_page()
        
        # è®¾ç½®æ›´çœŸå®çš„æµè§ˆå™¨è¡Œä¸º
        await detail_page.set_viewport_size({"width": 1920, "height": 1080})
        
        # è®¿é—®è¯¦æƒ…é¡µ
        logger.info(f"ğŸ” è®¿é—®å²—ä½è¯¦æƒ…é¡µ: {url}")
        await detail_page.goto(url, wait_until="networkidle", timeout=15000)
        
        # ç­‰å¾…å…³é”®å†…å®¹åŠ è½½
        await detail_page.wait_for_selector('.job-sec-text, .job-detail', timeout=5000)
        await asyncio.sleep(1.5)  # é¢å¤–ç­‰å¾…åŠ¨æ€å†…å®¹
        
        # æ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸º - æ»šåŠ¨é¡µé¢
        await detail_page.evaluate("""
            () => {
                // å¹³æ»‘æ»šåŠ¨åˆ°å²—ä½è¯¦æƒ…åŒºåŸŸ
                const jobSection = document.querySelector('.job-sec-text');
                if (jobSection) {
                    jobSection.scrollIntoView({ behavior: 'smooth', block: 'center' });
                }
                // éšæœºæ»šåŠ¨ä¸€äº›è·ç¦»
                window.scrollBy(0, Math.random() * 200 + 100);
            }
        """)
        await asyncio.sleep(0.5)
        
        # 1. æå–å²—ä½èŒè´£ï¼ˆjob_descriptionï¼‰
        job_description = await self._extract_job_description(detail_page)
        if job_description and len(job_description) > 50:
            job_data['job_description'] = job_description
            logger.info(f"âœ… è·å–å²—ä½èŒè´£: {len(job_description)}å­—ç¬¦")
        
        # 2. æå–ä»»èŒè¦æ±‚ï¼ˆjob_requirementsï¼‰
        job_requirements = await self._extract_job_requirements(detail_page)
        if job_requirements and len(job_requirements) > 30:
            job_data['job_requirements'] = job_requirements
            logger.info(f"âœ… è·å–ä»»èŒè¦æ±‚: {len(job_requirements)}å­—ç¬¦")
        
        # 3. æå–å…¬å¸è¯¦æƒ…ï¼ˆcompany_detailsï¼‰
        company_details = await self._extract_company_details(detail_page)
        if company_details:
            job_data['company_details'] = company_details
            job_data['company'] = company_details.split(' ')[0]  # æ›´æ–°å…¬å¸åç§°
            logger.info(f"âœ… è·å–å…¬å¸è¯¦æƒ…: {company_details}")
        
        # 4. æå–å…¶ä»–è¡¥å……ä¿¡æ¯
        additional_info = await self._extract_additional_info(detail_page)
        job_data.update(additional_info)
        
        await detail_page.close()
        return job_data
        
    except Exception as e:
        logger.warning(f"âš ï¸ è·å–è¯¦æƒ…é¡µå¤±è´¥: {e}")
        if 'detail_page' in locals():
            try:
                await detail_page.close()
            except:
                pass
        return job_data


async def _extract_job_description(self, page) -> str:
    """æå–å²—ä½èŒè´£"""
    # Bossç›´è˜å²—ä½èŒè´£çš„ç²¾ç¡®é€‰æ‹©å™¨
    selectors = [
        # æœ€ç²¾ç¡®ï¼šæŸ¥æ‰¾åŒ…å«"å²—ä½èŒè´£"æ–‡æœ¬çš„åŒºåŸŸ
        "//div[contains(text(), 'å²—ä½èŒè´£')]/following-sibling::div[1]",
        "//div[contains(text(), 'å·¥ä½œèŒè´£')]/following-sibling::div[1]",
        "//div[contains(text(), 'èŒä½æè¿°')]/following-sibling::div[1]",
        # æ ‡å‡†é€‰æ‹©å™¨
        ".job-sec-text:first-child",
        ".job-detail .job-sec:first-child .job-sec-text",
        # é€šè¿‡ç»“æ„å®šä½
        ".job-detail-section:first-child .text",
        "section.job-sec:nth-child(1) .job-sec-text",
        # å¤‡ç”¨é€‰æ‹©å™¨
        ".job-sec .job-sec-text",
        ".detail-content .text:first-child"
    ]
    
    for selector in selectors:
        try:
            if selector.startswith("//"):
                # XPathé€‰æ‹©å™¨
                elem = await page.query_selector(f"xpath={selector}")
            else:
                # CSSé€‰æ‹©å™¨
                elem = await page.query_selector(selector)
            
            if elem:
                text = await elem.inner_text()
                if text and len(text.strip()) > 50:
                    # æ¸…ç†æ–‡æœ¬
                    cleaned_text = text.strip()
                    # å¦‚æœåŒ…å«æ˜æ˜¾çš„åˆ†éš”ç¬¦ï¼Œåªå–å²—ä½èŒè´£éƒ¨åˆ†
                    if "ä»»èŒè¦æ±‚" in cleaned_text:
                        cleaned_text = cleaned_text.split("ä»»èŒè¦æ±‚")[0].strip()
                    if "å²—ä½è¦æ±‚" in cleaned_text:
                        cleaned_text = cleaned_text.split("å²—ä½è¦æ±‚")[0].strip()
                    
                    return cleaned_text[:1500]  # é™åˆ¶é•¿åº¦
        except:
            continue
    
    # å¦‚æœä¸Šè¿°æ–¹æ³•éƒ½å¤±è´¥ï¼Œå°è¯•é€šè¿‡JavaScriptæå–
    try:
        js_result = await page.evaluate("""
            () => {
                // æŸ¥æ‰¾åŒ…å«å²—ä½èŒè´£çš„æ–‡æœ¬èŠ‚ç‚¹
                const walker = document.createTreeWalker(
                    document.body,
                    NodeFilter.SHOW_TEXT,
                    null,
                    false
                );
                
                let node;
                let foundJobDesc = false;
                let description = '';
                
                while (node = walker.nextNode()) {
                    const text = node.textContent.trim();
                    if (text.includes('å²—ä½èŒè´£') || text.includes('å·¥ä½œèŒè´£') || text.includes('èŒä½æè¿°')) {
                        foundJobDesc = true;
                        continue;
                    }
                    
                    if (foundJobDesc && text.length > 20) {
                        // æ‰¾åˆ°ä¸‹ä¸€ä¸ªåŒ…å«å®è´¨å†…å®¹çš„æ–‡æœ¬èŠ‚ç‚¹
                        const parentClass = node.parentElement?.className || '';
                        if (parentClass.includes('job-sec-text') || parentClass.includes('text') || parentClass.includes('detail')) {
                            description = text;
                            break;
                        }
                    }
                }
                
                return description;
            }
        """)
        
        if js_result and len(js_result) > 50:
            return js_result[:1500]
    except:
        pass
    
    return ""


async def _extract_job_requirements(self, page) -> str:
    """æå–ä»»èŒè¦æ±‚"""
    # Bossç›´è˜ä»»èŒè¦æ±‚çš„ç²¾ç¡®é€‰æ‹©å™¨
    selectors = [
        # æœ€ç²¾ç¡®ï¼šæŸ¥æ‰¾åŒ…å«"ä»»èŒè¦æ±‚"æ–‡æœ¬çš„åŒºåŸŸ
        "//div[contains(text(), 'ä»»èŒè¦æ±‚')]/following-sibling::div[1]",
        "//div[contains(text(), 'å²—ä½è¦æ±‚')]/following-sibling::div[1]",
        "//div[contains(text(), 'èŒä½è¦æ±‚')]/following-sibling::div[1]",
        # æ ‡å‡†é€‰æ‹©å™¨ï¼ˆé€šå¸¸æ˜¯ç¬¬äºŒä¸ªjob-sec-textï¼‰
        ".job-sec-text:nth-child(2)",
        ".job-detail .job-sec:nth-child(2) .job-sec-text",
        # é€šè¿‡ç»“æ„å®šä½
        ".job-detail-section:nth-child(2) .text",
        "section.job-sec:nth-child(2) .job-sec-text",
        # å¤‡ç”¨é€‰æ‹©å™¨
        ".job-require-text",
        ".requirement-content"
    ]
    
    for selector in selectors:
        try:
            if selector.startswith("//"):
                elem = await page.query_selector(f"xpath={selector}")
            else:
                elem = await page.query_selector(selector)
            
            if elem:
                text = await elem.inner_text()
                if text and len(text.strip()) > 30:
                    return text.strip()[:1000]
        except:
            continue
    
    # JavaScriptå¤‡ç”¨æ–¹æ¡ˆ
    try:
        js_result = await page.evaluate("""
            () => {
                const walker = document.createTreeWalker(
                    document.body,
                    NodeFilter.SHOW_TEXT,
                    null,
                    false
                );
                
                let node;
                let foundRequirements = false;
                let requirements = '';
                
                while (node = walker.nextNode()) {
                    const text = node.textContent.trim();
                    if (text.includes('ä»»èŒè¦æ±‚') || text.includes('å²—ä½è¦æ±‚') || text.includes('èŒä½è¦æ±‚')) {
                        foundRequirements = true;
                        continue;
                    }
                    
                    if (foundRequirements && text.length > 20) {
                        const parentClass = node.parentElement?.className || '';
                        if (parentClass.includes('job-sec-text') || parentClass.includes('text') || parentClass.includes('detail')) {
                            requirements = text;
                            break;
                        }
                    }
                }
                
                return requirements;
            }
        """)
        
        if js_result and len(js_result) > 30:
            return js_result[:1000]
    except:
        pass
    
    return ""


async def _extract_company_details(self, page) -> str:
    """æå–å…¬å¸è¯¦æƒ…"""
    # Bossç›´è˜å…¬å¸åç§°çš„ç²¾ç¡®é€‰æ‹©å™¨
    selectors = [
        ".brand-name",
        ".company-name",
        ".company-brand",
        "h1 .company-name",
        ".detail-company .company-name",
        ".company-info .name",
        ".job-company h3",
        # é€šè¿‡ç»“æ„å®šä½
        ".sider-company .company-name",
        ".job-box .company-info h3"
    ]
    
    for selector in selectors:
        try:
            elem = await page.query_selector(selector)
            if elem:
                text = await elem.inner_text()
                if text and len(text.strip()) > 1:
                    company_name = text.strip()
                    
                    # å°è¯•è·å–æ›´å¤šå…¬å¸ä¿¡æ¯
                    company_info_elem = await page.query_selector('.company-info, .sider-company')
                    if company_info_elem:
                        info_text = await company_info_elem.inner_text()
                        # æå–è¡Œä¸šã€è§„æ¨¡ç­‰ä¿¡æ¯
                        lines = info_text.split('\n')
                        useful_lines = [line.strip() for line in lines if line.strip() and len(line.strip()) < 50]
                        if len(useful_lines) > 1:
                            return f"{company_name} | {' | '.join(useful_lines[1:3])}"
                    
                    return company_name
        except:
            continue
    
    return ""


async def _extract_additional_info(self, page) -> Dict:
    """æå–å…¶ä»–è¡¥å……ä¿¡æ¯"""
    additional_info = {}
    
    try:
        # æå–ç¦åˆ©ä¿¡æ¯
        benefits_elem = await page.query_selector('.job-tags, .welfare-list, .tag-list')
        if benefits_elem:
            benefits_text = await benefits_elem.inner_text()
            if benefits_text:
                additional_info['benefits'] = benefits_text.strip()
        
        # æå–å·¥ä½œåœ°å€è¯¦æƒ…
        address_elem = await page.query_selector('.location-address, .work-addr, .job-address')
        if address_elem:
            address_text = await address_elem.inner_text()
            if address_text and 'åœ°å€' not in address_text:  # è¿‡æ»¤æ‰"å·¥ä½œåœ°å€"è¿™ç§æ ‡ç­¾
                additional_info['detailed_address'] = address_text.strip()
        
        # æå–å‘å¸ƒæ—¶é—´
        time_elem = await page.query_selector('.job-time, .time')
        if time_elem:
            time_text = await time_elem.inner_text()
            if time_text:
                additional_info['publish_time'] = time_text.strip()
    
    except Exception as e:
        logger.debug(f"æå–è¡¥å……ä¿¡æ¯å¤±è´¥: {e}")
    
    return additional_info


# é›†æˆåˆ°real_playwright_spider.pyçš„æ–¹æ³•
def integrate_enhanced_extraction():
    """
    å°†å¢å¼ºçš„æå–æ–¹æ³•é›†æˆåˆ°real_playwright_spider.py
    
    åœ¨_extract_single_jobæ–¹æ³•çš„æœ€åï¼Œåœ¨è¿”å›job_dataä¹‹å‰è°ƒç”¨ï¼š
    
    # è·å–è¯¦æƒ…é¡µçš„çœŸå®ä¿¡æ¯
    if url and url.startswith('http'):
        job_data = await self.fetch_job_details_enhanced(url, job_data)
    """
    pass