#!/usr/bin/env python3
"""
æµ‹è¯•Playwrightçˆ¬è™«çš„ç™»å½•åŠŸèƒ½
"""

import asyncio
import sys
import os
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

from crawler.real_playwright_spider import RealPlaywrightBossSpider


async def test_login_and_search():
    """æµ‹è¯•ç™»å½•å’Œæœç´¢åŠŸèƒ½"""
    logger.info("ğŸ§ª å¼€å§‹æµ‹è¯•Playwrightçˆ¬è™«ç™»å½•åŠŸèƒ½...")
    
    # åˆå§‹åŒ–çˆ¬è™«ï¼ˆéæ— å¤´æ¨¡å¼ï¼Œç¡®ä¿å¯è§ï¼‰
    spider = RealPlaywrightBossSpider(headless=False)
    
    try:
        # 1. å¯åŠ¨æµè§ˆå™¨
        logger.info("\nğŸ“‹ æ­¥éª¤1: å¯åŠ¨æµè§ˆå™¨")
        success = await spider.start()
        if not success:
            logger.error("âŒ æµè§ˆå™¨å¯åŠ¨å¤±è´¥")
            return
        
        logger.info("âœ… æµè§ˆå™¨å¯åŠ¨æˆåŠŸ")
        
        # 2. æœç´¢å²—ä½ï¼ˆä¼šè‡ªåŠ¨è§¦å‘ç™»å½•æµç¨‹ï¼‰
        logger.info("\nğŸ“‹ æ­¥éª¤2: å¼€å§‹æœç´¢å²—ä½ï¼ˆå°†è‡ªåŠ¨æ£€æµ‹å¹¶å¤„ç†ç™»å½•ï¼‰")
        
        keyword = "æ•°æ®åˆ†æ"
        city = "shanghai"
        max_jobs = 10  # ç™»å½•ååº”è¯¥èƒ½è·å–æ›´å¤šå²—ä½
        
        logger.info(f"ğŸ” æœç´¢å‚æ•°: {keyword} | {city} | {max_jobs}ä¸ªå²—ä½")
        jobs = await spider.search_jobs(keyword, city, max_jobs)
        
        # 3. éªŒè¯ç»“æœ
        logger.info(f"\nğŸ“Š æœç´¢ç»“æœ: æ‰¾åˆ° {len(jobs)} ä¸ªå²—ä½")
        
        if len(jobs) == 0:
            logger.error("âŒ æœªæ‰¾åˆ°ä»»ä½•å²—ä½ï¼Œå¯èƒ½ç™»å½•æˆ–æœç´¢å¤±è´¥")
        elif len(jobs) < 5:
            logger.warning(f"âš ï¸ åªæ‰¾åˆ° {len(jobs)} ä¸ªå²—ä½ï¼Œå¯èƒ½å­˜åœ¨é—®é¢˜")
        else:
            logger.info(f"âœ… æˆåŠŸæ‰¾åˆ° {len(jobs)} ä¸ªå²—ä½ï¼Œç™»å½•å’Œæœç´¢åŠŸèƒ½æ­£å¸¸")
        
        # æ˜¾ç¤ºå‰3ä¸ªå²—ä½ä¿¡æ¯
        for i, job in enumerate(jobs[:3], 1):
            logger.info(f"\nå²—ä½ #{i}:")
            logger.info(f"  èŒä½: {job.get('title', 'æœªçŸ¥')}")
            logger.info(f"  å…¬å¸: {job.get('company', 'æœªçŸ¥')}")
            logger.info(f"  åœ°ç‚¹: {job.get('work_location', 'æœªçŸ¥')}")
            logger.info(f"  è–ªèµ„: {job.get('salary', 'æœªçŸ¥')}")
            
            # éªŒè¯URL
            url = job.get('url', '')
            if url and url.startswith('https://www.zhipin.com/job_detail/'):
                logger.info(f"  URL: {url[:80]}...")
            else:
                logger.warning(f"  URL: {url or 'æ— '}")
        
        # 4. æµ‹è¯•cookiesä¿å­˜
        logger.info("\nğŸ“‹ æ­¥éª¤3: éªŒè¯cookiesä¿å­˜")
        cookies_file = os.path.join(os.path.dirname(__file__), 'crawler', 'cookies', 'boss_cookies.json')
        if os.path.exists(cookies_file):
            logger.info(f"âœ… Cookiesæ–‡ä»¶å·²ä¿å­˜: {cookies_file}")
            # è·å–æ–‡ä»¶å¤§å°
            file_size = os.path.getsize(cookies_file)
            logger.info(f"  æ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
        else:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°cookiesæ–‡ä»¶")
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # æ¸…ç†èµ„æº
        await spider.close()
        logger.info("\nğŸ æµ‹è¯•å®Œæˆ")


def main():
    """ä¸»å‡½æ•°"""
    logger.info("=" * 50)
    logger.info("Bossç›´è˜è‡ªåŠ¨åŒ– - ç™»å½•åŠŸèƒ½æµ‹è¯•")
    logger.info("=" * 50)
    logger.info("\nè¯´æ˜:")
    logger.info("1. ç¨‹åºä¼šè‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨")
    logger.info("2. å¦‚æœæ˜¯é¦–æ¬¡è¿è¡Œï¼Œéœ€è¦æ‰‹åŠ¨ç™»å½•Bossç›´è˜")
    logger.info("3. ç™»å½•æˆåŠŸåï¼Œcookiesä¼šè¢«ä¿å­˜")
    logger.info("4. ä¸‹æ¬¡è¿è¡Œæ—¶ä¼šè‡ªåŠ¨ä½¿ç”¨ä¿å­˜çš„cookies")
    logger.info("=" * 50)
    
    # è¿è¡Œå¼‚æ­¥æµ‹è¯•
    asyncio.run(test_login_and_search())


if __name__ == "__main__":
    main()