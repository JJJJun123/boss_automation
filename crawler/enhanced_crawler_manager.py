#!/usr/bin/env python3
"""
å¢å¼ºçˆ¬è™«ç®¡ç†å™¨
ç»Ÿä¸€ç®¡ç†æ‰€æœ‰çˆ¬è™«ä¼˜åŒ–ç»„ä»¶ï¼Œæä¾›é«˜çº§æ¥å£
"""

import asyncio
import logging
import time
from typing import Dict, List, Optional, Any, Callable
from dataclasses import asdict
from .concurrent_manager import ConcurrentManager, get_concurrent_manager
from .performance_monitor import PerformanceMonitor, CrawlerPerformance
from .real_playwright_spider import RealPlaywrightBossSpider
from config.config_manager import ConfigManager

logger = logging.getLogger(__name__)


class EnhancedCrawlerManager:
    """å¢å¼ºçˆ¬è™«ç®¡ç†å™¨ - ç»Ÿä¸€ç®¡ç†æ¥å£"""
    
    def __init__(self):
        self.concurrent_manager: Optional[ConcurrentManager] = None
        self.performance_monitor = PerformanceMonitor()
        self.is_initialized = False
        
        # è·å–é…ç½®
        self.config_manager = ConfigManager()
        self.crawler_config = self.config_manager.get_app_config('crawler')
        self.browser_config = self.crawler_config.get('browser', {})
        
        # ç®¡ç†å™¨é…ç½®
        self.config = {
            "max_concurrent_tasks": 3,
            "max_browsers": 2,
            "cache_enabled": True,
            "performance_monitoring": True,
            "auto_optimization": True
        }
        
        # å›è°ƒå‡½æ•°
        self.callbacks = {
            "on_task_start": [],
            "on_task_complete": [],
            "on_task_error": [],
            "on_performance_alert": []
        }
        
        # è¿è¡Œç»Ÿè®¡
        self.runtime_stats = {
            "total_searches": 0,
            "successful_searches": 0,
            "cached_results": 0,
            "avg_response_time": 0.0,
            "uptime_start": time.time()
        }
    
    async def initialize(self, config: Dict[str, Any] = None):
        """åˆå§‹åŒ–ç®¡ç†å™¨"""
        if self.is_initialized:
            return
        
        # æ›´æ–°é…ç½®
        if config:
            self.config.update(config)
        
        logger.info("ğŸš€ åˆå§‹åŒ–å¢å¼ºçˆ¬è™«ç®¡ç†å™¨...")
        
        # åˆå§‹åŒ–å¹¶å‘ç®¡ç†å™¨
        self.concurrent_manager = ConcurrentManager(
            max_concurrent_tasks=self.config["max_concurrent_tasks"],
            max_browsers=self.config["max_browsers"]
        )
        await self.concurrent_manager.start()
        
        # å¯åŠ¨æ€§èƒ½ç›‘æ§
        if self.config["performance_monitoring"]:
            await self.performance_monitor.start_monitoring()
        
        self.is_initialized = True
        logger.info("âœ… å¢å¼ºçˆ¬è™«ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def shutdown(self):
        """å…³é—­ç®¡ç†å™¨"""
        if not self.is_initialized:
            return
        
        logger.info("ğŸ›‘ å…³é—­å¢å¼ºçˆ¬è™«ç®¡ç†å™¨...")
        
        # å…³é—­å¹¶å‘ç®¡ç†å™¨
        if self.concurrent_manager:
            await self.concurrent_manager.stop()
        
        # åœæ­¢æ€§èƒ½ç›‘æ§
        await self.performance_monitor.stop_monitoring()
        
        self.is_initialized = False
        logger.info("âœ… å¢å¼ºçˆ¬è™«ç®¡ç†å™¨å·²å…³é—­")
    
    async def search_jobs_enhanced(
        self,
        keyword: str,
        city: str = "shanghai",
        max_jobs: int = 20,
        priority: int = 1,
        use_cache: bool = None,
        callback: Callable = None
    ) -> List[Dict]:
        """
        å¢å¼ºå²—ä½æœç´¢æ¥å£
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            city: åŸå¸‚
            max_jobs: æœ€å¤§å²—ä½æ•°
            priority: ä¼˜å…ˆçº§
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            å²—ä½æ•°æ®åˆ—è¡¨
        """
        if not self.is_initialized:
            await self.initialize()
        
        search_start_time = time.time()
        
        try:
            # è§¦å‘å¼€å§‹å›è°ƒ
            await self._trigger_callbacks("on_task_start", {
                "keyword": keyword,
                "city": city,
                "max_jobs": max_jobs,
                "start_time": search_start_time
            })
            
            # æ›´æ–°ç»Ÿè®¡
            self.runtime_stats["total_searches"] += 1
            
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ç¼“å­˜
            if use_cache is None:
                use_cache = self.config["cache_enabled"]
            
            if use_cache:
                # ä½¿ç”¨å¹¶å‘ç®¡ç†å™¨ï¼ˆåŒ…å«ç¼“å­˜ï¼‰
                results = await self._search_with_concurrent_manager(
                    keyword, city, max_jobs, priority, callback
                )
            else:
                # ç›´æ¥ä½¿ç”¨çˆ¬è™«ï¼ˆä¸ä½¿ç”¨ç¼“å­˜ï¼‰
                results = await self._search_direct(
                    keyword, city, max_jobs, callback
                )
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            search_duration = time.time() - search_start_time
            success = len(results) > 0
            
            if success:
                self.runtime_stats["successful_searches"] += 1
            
            # æ›´æ–°å¹³å‡å“åº”æ—¶é—´
            self._update_avg_response_time(search_duration)
            
            # è®°å½•æ€§èƒ½æ•°æ®
            if self.config["performance_monitoring"]:
                await self._record_performance_data(
                    keyword, city, max_jobs, search_start_time, 
                    time.time(), results, success
                )
            
            # è§¦å‘å®Œæˆå›è°ƒ
            await self._trigger_callbacks("on_task_complete", {
                "keyword": keyword,
                "city": city,
                "results_count": len(results),
                "duration": search_duration,
                "success": success
            })
            
            logger.info(f"ğŸ¯ æœç´¢å®Œæˆ: {keyword}@{city} - {len(results)} ä¸ªå²—ä½ ({search_duration:.2f}s)")
            return results
            
        except Exception as e:
            # è§¦å‘é”™è¯¯å›è°ƒ
            await self._trigger_callbacks("on_task_error", {
                "keyword": keyword,
                "city": city,
                "error": str(e),
                "duration": time.time() - search_start_time
            })
            
            logger.error(f"âŒ æœç´¢å¤±è´¥: {keyword}@{city} - {e}")
            return []
    
    async def _search_with_concurrent_manager(
        self, keyword: str, city: str, max_jobs: int, 
        priority: int, callback: Callable
    ) -> List[Dict]:
        """ä½¿ç”¨å¹¶å‘ç®¡ç†å™¨æœç´¢"""
        from .concurrent_manager import concurrent_search_jobs
        
        if callback:
            callback("æ­£åœ¨æäº¤å¹¶å‘ä»»åŠ¡...")
        
        results = await concurrent_search_jobs(keyword, city, max_jobs, priority)
        
        if callback:
            callback(f"å¹¶å‘æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªå²—ä½")
        
        return results
    
    async def _search_direct(
        self, keyword: str, city: str, max_jobs: int, callback: Callable
    ) -> List[Dict]:
        """ç›´æ¥æœç´¢ï¼ˆä¸ä½¿ç”¨ç¼“å­˜ï¼‰"""
        if callback:
            callback("å¯åŠ¨ç›´æ¥æœç´¢æ¨¡å¼...")
        
        headless = self.browser_config.get('headless', False)
        spider = RealPlaywrightBossSpider(headless=headless)
        
        try:
            if callback:
                callback("æ­£åœ¨å¯åŠ¨æµè§ˆå™¨...")
            
            await spider.start()
            
            if callback:
                callback("æ­£åœ¨æœç´¢å²—ä½...")
            
            results = await spider.search_jobs(keyword, city, max_jobs)
            
            if callback:
                callback(f"æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªå²—ä½")
            
            return results
            
        finally:
            await spider.close()
    
    async def _record_performance_data(
        self, keyword: str, city: str, max_jobs: int,
        start_time: float, end_time: float, results: List[Dict],
        success: bool
    ):
        """è®°å½•æ€§èƒ½æ•°æ®"""
        try:
            # åˆ†æç»“æœè´¨é‡
            cache_hit = any(
                job.get("engine_source", "").find("ç¼“å­˜") != -1 
                for job in results
            )
            
            # ä¼°ç®—é¡µé¢åŠ è½½æ—¶é—´å’Œæå–æ—¶é—´ï¼ˆç®€åŒ–ç‰ˆï¼‰
            total_duration = end_time - start_time
            estimated_page_load = min(total_duration * 0.6, 10.0)
            estimated_extraction = min(total_duration * 0.4, 5.0)
            
            performance = CrawlerPerformance(
                task_id=f"{keyword}_{city}_{int(start_time)}",
                keyword=keyword,
                city=city,
                start_time=start_time,
                end_time=end_time,
                total_jobs=len(results),
                success_rate=1.0 if success else 0.0,
                avg_page_load_time=estimated_page_load,
                avg_extraction_time=estimated_extraction,
                cache_hit_rate=1.0 if cache_hit else 0.0,
                retry_count=0,  # ç®€åŒ–ç‰ˆï¼Œå®é™…åº”è¯¥ä»çˆ¬è™«è·å–
                error_types={},
                bottlenecks=[]
            )
            
            await self.performance_monitor.record_crawler_performance(performance)
            
        except Exception as e:
            logger.debug(f"è®°å½•æ€§èƒ½æ•°æ®å¤±è´¥: {e}")
    
    def _update_avg_response_time(self, duration: float):
        """æ›´æ–°å¹³å‡å“åº”æ—¶é—´"""
        if self.runtime_stats["successful_searches"] == 1:
            self.runtime_stats["avg_response_time"] = duration
        else:
            # ç§»åŠ¨å¹³å‡
            alpha = 0.1
            self.runtime_stats["avg_response_time"] = (
                alpha * duration + 
                (1 - alpha) * self.runtime_stats["avg_response_time"]
            )
    
    async def _trigger_callbacks(self, event_type: str, data: Dict[str, Any]):
        """è§¦å‘å›è°ƒå‡½æ•°"""
        if event_type in self.callbacks:
            for callback in self.callbacks[event_type]:
                try:
                    if asyncio.iscoroutinefunction(callback):
                        await callback(data)
                    else:
                        callback(data)
                except Exception as e:
                    logger.debug(f"å›è°ƒå‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
    
    def add_callback(self, event_type: str, callback: Callable):
        """æ·»åŠ å›è°ƒå‡½æ•°"""
        if event_type in self.callbacks:
            self.callbacks[event_type].append(callback)
        else:
            logger.warning(f"æœªçŸ¥äº‹ä»¶ç±»å‹: {event_type}")
    
    def remove_callback(self, event_type: str, callback: Callable):
        """ç§»é™¤å›è°ƒå‡½æ•°"""
        if event_type in self.callbacks and callback in self.callbacks[event_type]:
            self.callbacks[event_type].remove(callback)
    
    async def batch_search(
        self, search_tasks: List[Dict[str, Any]], 
        max_concurrent: int = None
    ) -> Dict[str, List[Dict]]:
        """
        æ‰¹é‡æœç´¢å²—ä½
        
        Args:
            search_tasks: æœç´¢ä»»åŠ¡åˆ—è¡¨ï¼Œæ¯ä¸ªä»»åŠ¡åŒ…å«keyword, city, max_jobsç­‰å‚æ•°
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
            
        Returns:
            ä»»åŠ¡IDåˆ°ç»“æœçš„æ˜ å°„
        """
        if not self.is_initialized:
            await self.initialize()
        
        if max_concurrent is None:
            max_concurrent = self.config["max_concurrent_tasks"]
        
        semaphore = asyncio.Semaphore(max_concurrent)
        results = {}
        
        async def search_single(task_info: Dict[str, Any]) -> tuple[str, List[Dict]]:
            async with semaphore:
                task_id = f"{task_info.get('keyword')}_{task_info.get('city')}_{int(time.time())}"
                result = await self.search_jobs_enhanced(**task_info)
                return task_id, result
        
        logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡æœç´¢: {len(search_tasks)} ä¸ªä»»åŠ¡")
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰æœç´¢ä»»åŠ¡
        tasks = [search_single(task_info) for task_info in search_tasks]
        completed_tasks = await asyncio.gather(*tasks, return_exceptions=True)
        
        # æ•´ç†ç»“æœ
        for task_result in completed_tasks:
            if isinstance(task_result, Exception):
                logger.error(f"æ‰¹é‡æœç´¢ä»»åŠ¡å¤±è´¥: {task_result}")
                continue
            
            task_id, task_results = task_result
            results[task_id] = task_results
        
        successful_count = len([r for r in results.values() if r])
        logger.info(f"âœ… æ‰¹é‡æœç´¢å®Œæˆ: {successful_count}/{len(search_tasks)} æˆåŠŸ")
        
        return results
    
    async def get_system_status(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        status = {
            "initialized": self.is_initialized,
            "uptime": time.time() - self.runtime_stats["uptime_start"],
            "config": self.config.copy(),
            "runtime_stats": self.runtime_stats.copy()
        }
        
        if self.concurrent_manager:
            status["concurrent_manager"] = self.concurrent_manager.get_status()
        
        if self.performance_monitor:
            status["performance_monitor"] = self.performance_monitor.get_current_performance()
        
        return status
    
    async def get_performance_report(self, hours: int = 24) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        if not self.performance_monitor:
            return {"error": "æ€§èƒ½ç›‘æ§æœªå¯ç”¨"}
        
        return self.performance_monitor.get_performance_report(hours)
    
    async def optimize_system(self) -> Dict[str, Any]:
        """è‡ªåŠ¨ç³»ç»Ÿä¼˜åŒ–"""
        if not self.config["auto_optimization"]:
            return {"message": "è‡ªåŠ¨ä¼˜åŒ–å·²ç¦ç”¨"}
        
        optimization_actions = []
        
        # è·å–å½“å‰æ€§èƒ½çŠ¶æ€
        current_performance = self.performance_monitor.get_current_performance()
        performance_score = current_performance.get("performance_score", 100)
        
        # æ ¹æ®æ€§èƒ½è¯„åˆ†è¿›è¡Œä¼˜åŒ–
        if performance_score < 70:
            # æ€§èƒ½è¾ƒå·®ï¼Œéœ€è¦ä¼˜åŒ–
            if current_performance.get("system", {}).get("cpu_percent", 0) > 80:
                # CPUä½¿ç”¨ç‡è¿‡é«˜ï¼Œå‡å°‘å¹¶å‘ä»»åŠ¡
                old_concurrent = self.config["max_concurrent_tasks"]
                self.config["max_concurrent_tasks"] = max(1, old_concurrent - 1)
                optimization_actions.append(f"å‡å°‘å¹¶å‘ä»»åŠ¡æ•°: {old_concurrent} -> {self.config['max_concurrent_tasks']}")
            
            if current_performance.get("system", {}).get("memory_percent", 0) > 85:
                # å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œæ¸…ç†ç¼“å­˜
                if self.concurrent_manager:
                    await self.concurrent_manager.cache_manager.clear_expired_cache()
                    optimization_actions.append("æ¸…ç†è¿‡æœŸç¼“å­˜")
        
        elif performance_score > 90:
            # æ€§èƒ½è‰¯å¥½ï¼Œå¯ä»¥å¢åŠ å¹¶å‘
            if self.config["max_concurrent_tasks"] < 5:
                old_concurrent = self.config["max_concurrent_tasks"]
                self.config["max_concurrent_tasks"] += 1
                optimization_actions.append(f"å¢åŠ å¹¶å‘ä»»åŠ¡æ•°: {old_concurrent} -> {self.config['max_concurrent_tasks']}")
        
        return {
            "performance_score": performance_score,
            "optimization_actions": optimization_actions,
            "optimized_config": self.config.copy()
        }
    
    def get_recommendations(self) -> List[str]:
        """è·å–ä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # åŸºäºè¿è¡Œç»Ÿè®¡çš„å»ºè®®
        success_rate = (
            self.runtime_stats["successful_searches"] / 
            max(1, self.runtime_stats["total_searches"])
        )
        
        if success_rate < 0.8:
            recommendations.append("æˆåŠŸç‡è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–å¢åŠ é‡è¯•æœºåˆ¶")
        
        if self.runtime_stats["avg_response_time"] > 30:
            recommendations.append("å¹³å‡å“åº”æ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®ä¼˜åŒ–çˆ¬è™«é€»è¾‘æˆ–å¢åŠ å¹¶å‘æ•°")
        
        # åŸºäºæ€§èƒ½ç›‘æ§çš„å»ºè®®
        if self.performance_monitor:
            current_perf = self.performance_monitor.get_current_performance()
            if current_perf.get("performance_score", 100) < 80:
                recommendations.append("ç³»ç»Ÿæ€§èƒ½è¯„åˆ†è¾ƒä½ï¼Œå»ºè®®è¿è¡Œè‡ªåŠ¨ä¼˜åŒ–")
        
        if not recommendations:
            recommendations.append("ç³»ç»Ÿè¿è¡Œè‰¯å¥½ï¼Œæš‚æ— ä¼˜åŒ–å»ºè®®")
        
        return recommendations


# å…¨å±€ç®¡ç†å™¨å®ä¾‹
_enhanced_crawler_manager: Optional[EnhancedCrawlerManager] = None


async def get_enhanced_crawler_manager() -> EnhancedCrawlerManager:
    """è·å–å…¨å±€å¢å¼ºçˆ¬è™«ç®¡ç†å™¨å®ä¾‹"""
    global _enhanced_crawler_manager
    if _enhanced_crawler_manager is None:
        _enhanced_crawler_manager = EnhancedCrawlerManager()
        await _enhanced_crawler_manager.initialize()
    return _enhanced_crawler_manager


# å¯¹å¤–é«˜çº§æ¥å£
async def enhanced_search_jobs(
    keyword: str, 
    city: str = "shanghai", 
    max_jobs: int = 20,
    **kwargs
) -> List[Dict]:
    """
    å¢å¼ºå²—ä½æœç´¢ï¼ˆé«˜çº§æ¥å£ï¼‰
    
    Args:
        keyword: æœç´¢å…³é”®è¯
        city: åŸå¸‚
        max_jobs: æœ€å¤§å²—ä½æ•°
        **kwargs: å…¶ä»–å‚æ•°
        
    Returns:
        å²—ä½æ•°æ®åˆ—è¡¨
    """
    manager = await get_enhanced_crawler_manager()
    return await manager.search_jobs_enhanced(keyword, city, max_jobs, **kwargs)


async def batch_enhanced_search(search_requests: List[Dict[str, Any]]) -> Dict[str, List[Dict]]:
    """
    æ‰¹é‡å¢å¼ºæœç´¢ï¼ˆé«˜çº§æ¥å£ï¼‰
    
    Args:
        search_requests: æœç´¢è¯·æ±‚åˆ—è¡¨
        
    Returns:
        æœç´¢ç»“æœæ˜ å°„
    """
    manager = await get_enhanced_crawler_manager()
    return await manager.batch_search(search_requests)


async def get_crawler_status() -> Dict[str, Any]:
    """è·å–çˆ¬è™«ç³»ç»ŸçŠ¶æ€"""
    manager = await get_enhanced_crawler_manager()
    return await manager.get_system_status()


async def optimize_crawler_system() -> Dict[str, Any]:
    """ä¼˜åŒ–çˆ¬è™«ç³»ç»Ÿ"""
    manager = await get_enhanced_crawler_manager()
    return await manager.optimize_system()