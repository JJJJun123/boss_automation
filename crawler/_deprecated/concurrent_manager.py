#!/usr/bin/env python3
"""
å¹¶å‘ç®¡ç†å™¨
å®ç°å¹¶å‘æŠ“å–ã€ç¼“å­˜æœºåˆ¶å’Œæ€§èƒ½ä¼˜åŒ–
"""

import asyncio
import logging
import time
import hashlib
import json
import os
from typing import Dict, List, Optional, Any, Callable
from datetime import datetime, timedelta
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor
from playwright.async_api import Browser, Page, BrowserContext

logger = logging.getLogger(__name__)


@dataclass
class CacheItem:
    """ç¼“å­˜é¡¹æ•°æ®ç»“æ„"""
    key: str
    data: Any
    timestamp: float
    expires_at: float
    access_count: int = 0
    last_access: float = 0.0
    
    def is_expired(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¿‡æœŸ"""
        return time.time() > self.expires_at
    
    def is_fresh(self, max_age: int = 3600) -> bool:
        """æ£€æŸ¥æ•°æ®æ˜¯å¦æ–°é²œ"""
        return time.time() - self.timestamp < max_age


@dataclass
class TaskInfo:
    """ä»»åŠ¡ä¿¡æ¯"""
    task_id: str
    keyword: str
    city: str
    max_jobs: int
    priority: int = 1
    created_at: float = 0.0
    status: str = "pending"  # pending, running, completed, failed
    result: Optional[List[Dict]] = None
    error: Optional[str] = None
    
    def __post_init__(self):
        if self.created_at == 0.0:
            self.created_at = time.time()


class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str = None, max_size: int = 1000):
        self.cache_dir = cache_dir or os.path.join(os.path.dirname(__file__), 'cache')
        self.max_size = max_size
        self.memory_cache: Dict[str, CacheItem] = {}
        self.cache_stats = {
            'hits': 0,
            'misses': 0,
            'evictions': 0,
            'disk_reads': 0,
            'disk_writes': 0
        }
        self.ensure_cache_dir()
    
    def ensure_cache_dir(self):
        """ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨"""
        os.makedirs(self.cache_dir, exist_ok=True)
    
    def _generate_cache_key(self, keyword: str, city: str, max_jobs: int) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_data = f"{keyword}:{city}:{max_jobs}"
        return hashlib.md5(key_data.encode('utf-8')).hexdigest()
    
    def _get_cache_file_path(self, cache_key: str) -> str:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        return os.path.join(self.cache_dir, f"{cache_key}.json")
    
    async def get(self, keyword: str, city: str, max_jobs: int, max_age: int = 3600) -> Optional[List[Dict]]:
        """
        è·å–ç¼“å­˜æ•°æ®
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            city: åŸå¸‚
            max_jobs: æœ€å¤§å²—ä½æ•°
            max_age: æœ€å¤§ç¼“å­˜æ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            ç¼“å­˜çš„å²—ä½æ•°æ®ï¼Œå¦‚æœæ²¡æœ‰æˆ–è¿‡æœŸåˆ™è¿”å›None
        """
        cache_key = self._generate_cache_key(keyword, city, max_jobs)
        
        # é¦–å…ˆæ£€æŸ¥å†…å­˜ç¼“å­˜
        if cache_key in self.memory_cache:
            cache_item = self.memory_cache[cache_key]
            cache_item.access_count += 1
            cache_item.last_access = time.time()
            
            if not cache_item.is_expired() and cache_item.is_fresh(max_age):
                self.cache_stats['hits'] += 1
                logger.info(f"âœ… å†…å­˜ç¼“å­˜å‘½ä¸­: {keyword}@{city}")
                return cache_item.data
            else:
                # è¿‡æœŸæ•°æ®ï¼Œä»å†…å­˜ä¸­ç§»é™¤
                del self.memory_cache[cache_key]
        
        # æ£€æŸ¥ç£ç›˜ç¼“å­˜
        cache_file = self._get_cache_file_path(cache_key)
        if os.path.exists(cache_file):
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)
                
                cache_item = CacheItem(
                    key=cache_key,
                    data=cache_data['data'],
                    timestamp=cache_data['timestamp'],
                    expires_at=cache_data['expires_at']
                )
                
                if not cache_item.is_expired() and cache_item.is_fresh(max_age):
                    # å°†ç£ç›˜ç¼“å­˜åŠ è½½åˆ°å†…å­˜
                    self.memory_cache[cache_key] = cache_item
                    self.cache_stats['hits'] += 1
                    self.cache_stats['disk_reads'] += 1
                    logger.info(f"âœ… ç£ç›˜ç¼“å­˜å‘½ä¸­: {keyword}@{city}")
                    return cache_item.data
                else:
                    # è¿‡æœŸæ–‡ä»¶ï¼Œåˆ é™¤
                    os.remove(cache_file)
                    
            except Exception as e:
                logger.debug(f"è¯»å–ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")
                try:
                    os.remove(cache_file)
                except:
                    pass
        
        self.cache_stats['misses'] += 1
        logger.debug(f"âŒ ç¼“å­˜æœªå‘½ä¸­: {keyword}@{city}")
        return None
    
    async def set(self, keyword: str, city: str, max_jobs: int, data: List[Dict], ttl: int = 7200) -> bool:
        """
        è®¾ç½®ç¼“å­˜æ•°æ®
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            city: åŸå¸‚
            max_jobs: æœ€å¤§å²—ä½æ•°
            data: è¦ç¼“å­˜çš„æ•°æ®
            ttl: ç”Ÿå­˜æ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            æ˜¯å¦æˆåŠŸè®¾ç½®ç¼“å­˜
        """
        if not data:
            return False
        
        cache_key = self._generate_cache_key(keyword, city, max_jobs)
        current_time = time.time()
        expires_at = current_time + ttl
        
        cache_item = CacheItem(
            key=cache_key,
            data=data,
            timestamp=current_time,
            expires_at=expires_at
        )
        
        try:
            # è®¾ç½®å†…å­˜ç¼“å­˜
            self.memory_cache[cache_key] = cache_item
            
            # æ£€æŸ¥å†…å­˜ç¼“å­˜å¤§å°ï¼Œå¿…è¦æ—¶æ¸…ç†
            if len(self.memory_cache) > self.max_size:
                await self._evict_memory_cache()
            
            # ä¿å­˜åˆ°ç£ç›˜
            cache_file = self._get_cache_file_path(cache_key)
            cache_data = {
                'key': cache_key,
                'data': data,
                'timestamp': current_time,
                'expires_at': expires_at,
                'keyword': keyword,
                'city': city,
                'max_jobs': max_jobs
            }
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            
            self.cache_stats['disk_writes'] += 1
            logger.info(f"ğŸ’¾ ç¼“å­˜å·²ä¿å­˜: {keyword}@{city} ({len(data)} ä¸ªå²—ä½)")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
            return False
    
    async def _evict_memory_cache(self):
        """æ¸…ç†å†…å­˜ç¼“å­˜"""
        if len(self.memory_cache) <= self.max_size:
            return
        
        # æŒ‰æœ€åè®¿é—®æ—¶é—´æ’åºï¼Œç§»é™¤æœ€æ—§çš„é¡¹ç›®
        sorted_items = sorted(
            self.memory_cache.items(),
            key=lambda x: x[1].last_access or x[1].timestamp
        )
        
        # ç§»é™¤25%çš„æœ€æ—§é¡¹ç›®
        evict_count = max(1, len(sorted_items) // 4)
        for i in range(evict_count):
            key = sorted_items[i][0]
            del self.memory_cache[key]
            self.cache_stats['evictions'] += 1
        
        logger.info(f"ğŸ§¹ æ¸…ç†å†…å­˜ç¼“å­˜: ç§»é™¤ {evict_count} ä¸ªé¡¹ç›®")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_requests = self.cache_stats['hits'] + self.cache_stats['misses']
        hit_rate = self.cache_stats['hits'] / total_requests if total_requests > 0 else 0.0
        
        return {
            'memory_cache_size': len(self.memory_cache),
            'hit_rate': hit_rate,
            'total_requests': total_requests,
            **self.cache_stats
        }
    
    async def clear_expired_cache(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        # æ¸…ç†å†…å­˜ç¼“å­˜
        expired_keys = [
            key for key, item in self.memory_cache.items()
            if item.is_expired()
        ]
        for key in expired_keys:
            del self.memory_cache[key]
        
        # æ¸…ç†ç£ç›˜ç¼“å­˜
        if os.path.exists(self.cache_dir):
            for filename in os.listdir(self.cache_dir):
                if filename.endswith('.json'):
                    file_path = os.path.join(self.cache_dir, filename)
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            cache_data = json.load(f)
                        
                        expires_at = cache_data.get('expires_at', 0)
                        if time.time() > expires_at:
                            os.remove(file_path)
                    except:
                        # å¦‚æœæ–‡ä»¶æŸåï¼Œç›´æ¥åˆ é™¤
                        try:
                            os.remove(file_path)
                        except:
                            pass
        
        if expired_keys:
            logger.info(f"ğŸ§¹ æ¸…ç†è¿‡æœŸç¼“å­˜: {len(expired_keys)} ä¸ªé¡¹ç›®")


class ConcurrentManager:
    """å¹¶å‘ç®¡ç†å™¨"""
    
    def __init__(self, max_concurrent_tasks: int = 3, max_browsers: int = 2):
        self.max_concurrent_tasks = max_concurrent_tasks
        self.max_browsers = max_browsers
        self.cache_manager = CacheManager()
        
        # ä»»åŠ¡ç®¡ç†
        self.task_queue: asyncio.Queue = asyncio.Queue()
        self.active_tasks: Dict[str, TaskInfo] = {}
        self.completed_tasks: Dict[str, TaskInfo] = {}
        
        # æµè§ˆå™¨æ± 
        self.browser_pool: List[Browser] = []
        self.browser_semaphore = asyncio.Semaphore(max_browsers)
        self.context_pool: List[BrowserContext] = []
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_tasks': 0,
            'completed_tasks': 0,
            'failed_tasks': 0,
            'cache_hits': 0,
            'concurrent_peak': 0,
            'avg_task_time': 0.0
        }
        
        # å·¥ä½œçº¿ç¨‹
        self.worker_tasks: List[asyncio.Task] = []
        self.is_running = False
    
    async def start(self):
        """å¯åŠ¨å¹¶å‘ç®¡ç†å™¨"""
        if self.is_running:
            return
        
        self.is_running = True
        logger.info(f"ğŸš€ å¯åŠ¨å¹¶å‘ç®¡ç†å™¨ (æœ€å¤§ä»»åŠ¡: {self.max_concurrent_tasks}, æœ€å¤§æµè§ˆå™¨: {self.max_browsers})")
        
        # å¯åŠ¨å·¥ä½œçº¿ç¨‹
        for i in range(self.max_concurrent_tasks):
            worker = asyncio.create_task(self._worker(f"worker-{i+1}"))
            self.worker_tasks.append(worker)
        
        # å¯åŠ¨ç¼“å­˜æ¸…ç†ä»»åŠ¡
        cache_cleaner = asyncio.create_task(self._cache_cleaner())
        self.worker_tasks.append(cache_cleaner)
    
    async def stop(self):
        """åœæ­¢å¹¶å‘ç®¡ç†å™¨"""
        if not self.is_running:
            return
        
        self.is_running = False
        logger.info("ğŸ›‘ åœæ­¢å¹¶å‘ç®¡ç†å™¨...")
        
        # åœæ­¢å·¥ä½œçº¿ç¨‹
        for task in self.worker_tasks:
            task.cancel()
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        if self.worker_tasks:
            await asyncio.gather(*self.worker_tasks, return_exceptions=True)
        
        # å…³é—­æµè§ˆå™¨æ± 
        await self._cleanup_browser_pool()
        
        logger.info("âœ… å¹¶å‘ç®¡ç†å™¨å·²åœæ­¢")
    
    async def submit_task(self, keyword: str, city: str, max_jobs: int, priority: int = 1) -> str:
        """
        æäº¤æŠ“å–ä»»åŠ¡
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            city: åŸå¸‚
            max_jobs: æœ€å¤§å²—ä½æ•°
            priority: ä¼˜å…ˆçº§ï¼ˆæ•°å­—è¶Šå¤§ä¼˜å…ˆçº§è¶Šé«˜ï¼‰
            
        Returns:
            ä»»åŠ¡ID
        """
        task_id = f"{keyword}_{city}_{max_jobs}_{int(time.time())}"
        
        task_info = TaskInfo(
            task_id=task_id,
            keyword=keyword,
            city=city,
            max_jobs=max_jobs,
            priority=priority
        )
        
        # é¦–å…ˆæ£€æŸ¥ç¼“å­˜
        cached_data = await self.cache_manager.get(keyword, city, max_jobs, max_age=3600)
        if cached_data:
            task_info.status = "completed"
            task_info.result = cached_data
            self.completed_tasks[task_id] = task_info
            self.stats['cache_hits'] += 1
            logger.info(f"âœ… ä»»åŠ¡ {task_id} ç›´æ¥ä»ç¼“å­˜è¿”å›ç»“æœ")
            return task_id
        
        # æ·»åŠ åˆ°ä»»åŠ¡é˜Ÿåˆ—
        await self.task_queue.put(task_info)
        self.active_tasks[task_id] = task_info
        self.stats['total_tasks'] += 1
        
        logger.info(f"ğŸ“ å·²æäº¤ä»»åŠ¡: {task_id} (ä¼˜å…ˆçº§: {priority})")
        return task_id
    
    async def get_task_result(self, task_id: str, timeout: int = 300) -> Optional[List[Dict]]:
        """
        è·å–ä»»åŠ¡ç»“æœ
        
        Args:
            task_id: ä»»åŠ¡ID
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            ä»»åŠ¡ç»“æœæ•°æ®
        """
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            # æ£€æŸ¥å·²å®Œæˆä»»åŠ¡
            if task_id in self.completed_tasks:
                task_info = self.completed_tasks[task_id]
                if task_info.status == "completed":
                    return task_info.result
                elif task_info.status == "failed":
                    logger.error(f"âŒ ä»»åŠ¡ {task_id} æ‰§è¡Œå¤±è´¥: {task_info.error}")
                    return None
            
            await asyncio.sleep(1)
        
        logger.error(f"â° ä»»åŠ¡ {task_id} ç­‰å¾…è¶…æ—¶")
        return None
    
    async def _worker(self, worker_name: str):
        """å·¥ä½œçº¿ç¨‹"""
        logger.info(f"ğŸ‘· å·¥ä½œçº¿ç¨‹ {worker_name} å·²å¯åŠ¨")
        
        while self.is_running:
            try:
                # è·å–ä»»åŠ¡
                task_info = await asyncio.wait_for(self.task_queue.get(), timeout=5.0)
                
                # æ›´æ–°ç»Ÿè®¡
                current_active = len([t for t in self.active_tasks.values() if t.status == "running"])
                self.stats['concurrent_peak'] = max(self.stats['concurrent_peak'], current_active + 1)
                
                # æ‰§è¡Œä»»åŠ¡
                await self._execute_task(task_info, worker_name)
                
                # æ ‡è®°ä»»åŠ¡å®Œæˆ
                self.task_queue.task_done()
                
            except asyncio.TimeoutError:
                continue
            except Exception as e:
                logger.error(f"âŒ å·¥ä½œçº¿ç¨‹ {worker_name} å‡ºé”™: {e}")
                continue
        
        logger.info(f"ğŸ‘· å·¥ä½œçº¿ç¨‹ {worker_name} å·²åœæ­¢")
    
    async def _execute_task(self, task_info: TaskInfo, worker_name: str):
        """æ‰§è¡Œå•ä¸ªä»»åŠ¡"""
        task_info.status = "running"
        start_time = time.time()
        
        try:
            logger.info(f"ğŸƒ {worker_name} å¼€å§‹æ‰§è¡Œä»»åŠ¡: {task_info.task_id}")
            
            # è·å–æµè§ˆå™¨å®ä¾‹
            async with self.browser_semaphore:
                browser, context = await self._get_browser_context()
                
                try:
                    # å¯¼å…¥çˆ¬è™«ç±»
                    from .real_playwright_spider import RealPlaywrightBossSpider
                    
                    # åˆ›å»ºçˆ¬è™«å®ä¾‹ï¼Œä½¿ç”¨å…±äº«æµè§ˆå™¨
                    spider = RealPlaywrightBossSpider(headless=True)
                    spider.browser = browser
                    spider.playwright = None  # ä½¿ç”¨å…±äº«æµè§ˆå™¨ï¼Œä¸éœ€è¦ç‹¬ç«‹playwrightå®ä¾‹
                    
                    # åˆ›å»ºæ–°é¡µé¢
                    spider.page = await context.new_page()
                    
                    # æ‰§è¡Œæœç´¢
                    results = await spider.search_jobs(
                        task_info.keyword,
                        task_info.city,
                        task_info.max_jobs
                    )
                    
                    # å…³é—­é¡µé¢
                    await spider.page.close()
                    
                    if results:
                        # ä¿å­˜åˆ°ç¼“å­˜
                        await self.cache_manager.set(
                            task_info.keyword,
                            task_info.city,
                            task_info.max_jobs,
                            results
                        )
                        
                        task_info.result = results
                        task_info.status = "completed"
                        self.stats['completed_tasks'] += 1
                        
                        execution_time = time.time() - start_time
                        self._update_avg_task_time(execution_time)
                        
                        logger.info(f"âœ… {worker_name} ä»»åŠ¡å®Œæˆ: {task_info.task_id} ({len(results)} ä¸ªå²—ä½, {execution_time:.2f}s)")
                    else:
                        raise Exception("æœªæ‰¾åˆ°ä»»ä½•å²—ä½æ•°æ®")
                
                finally:
                    # è¿”å›æµè§ˆå™¨ä¸Šä¸‹æ–‡åˆ°æ± ä¸­
                    await self._return_browser_context(browser, context)
        
        except Exception as e:
            task_info.status = "failed"
            task_info.error = str(e)
            self.stats['failed_tasks'] += 1
            logger.error(f"âŒ {worker_name} ä»»åŠ¡å¤±è´¥: {task_info.task_id} - {e}")
        
        finally:
            # ç§»åŠ¨åˆ°å·²å®Œæˆä»»åŠ¡
            if task_info.task_id in self.active_tasks:
                del self.active_tasks[task_info.task_id]
            self.completed_tasks[task_info.task_id] = task_info
    
    async def _get_browser_context(self) -> tuple[Browser, BrowserContext]:
        """è·å–æµè§ˆå™¨ä¸Šä¸‹æ–‡"""
        if not self.browser_pool:
            # åˆ›å»ºæ–°æµè§ˆå™¨
            from playwright.async_api import async_playwright
            playwright = await async_playwright().start()
            browser = await playwright.chromium.launch(
                headless=True,
                args=[
                    '--disable-blink-features=AutomationControlled',
                    '--disable-web-security',
                    '--disable-features=VizDisplayCompositor'
                ]
            )
            self.browser_pool.append(browser)
        
        browser = self.browser_pool[0]  # ç®€åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæµè§ˆå™¨
        
        # åˆ›å»ºæ–°çš„ä¸Šä¸‹æ–‡
        context = await browser.new_context(
            viewport={'width': 1280, 'height': 800},
            user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        
        return browser, context
    
    async def _return_browser_context(self, browser: Browser, context: BrowserContext):
        """è¿”å›æµè§ˆå™¨ä¸Šä¸‹æ–‡"""
        try:
            await context.close()
        except Exception as e:
            logger.debug(f"å…³é—­æµè§ˆå™¨ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
    
    async def _cleanup_browser_pool(self):
        """æ¸…ç†æµè§ˆå™¨æ± """
        for browser in self.browser_pool:
            try:
                await browser.close()
            except Exception as e:
                logger.debug(f"å…³é—­æµè§ˆå™¨å¤±è´¥: {e}")
        
        self.browser_pool.clear()
    
    async def _cache_cleaner(self):
        """ç¼“å­˜æ¸…ç†ä»»åŠ¡"""
        while self.is_running:
            try:
                await self.cache_manager.clear_expired_cache()
                await asyncio.sleep(3600)  # æ¯å°æ—¶æ¸…ç†ä¸€æ¬¡
            except Exception as e:
                logger.debug(f"ç¼“å­˜æ¸…ç†å‡ºé”™: {e}")
                await asyncio.sleep(300)  # å‡ºé”™å5åˆ†é’Ÿé‡è¯•
    
    def _update_avg_task_time(self, execution_time: float):
        """æ›´æ–°å¹³å‡ä»»åŠ¡æ‰§è¡Œæ—¶é—´"""
        if self.stats['completed_tasks'] == 1:
            self.stats['avg_task_time'] = execution_time
        else:
            # ä½¿ç”¨ç§»åŠ¨å¹³å‡
            alpha = 0.1  # å¹³æ»‘å› å­
            self.stats['avg_task_time'] = (
                alpha * execution_time + 
                (1 - alpha) * self.stats['avg_task_time']
            )
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–ç®¡ç†å™¨çŠ¶æ€"""
        return {
            'is_running': self.is_running,
            'active_tasks': len(self.active_tasks),
            'queue_size': self.task_queue.qsize(),
            'completed_tasks': len(self.completed_tasks),
            'browser_pool_size': len(self.browser_pool),
            'cache_stats': self.cache_manager.get_cache_stats(),
            'performance_stats': self.stats.copy()
        }
    
    def get_task_status(self, task_id: str) -> Optional[Dict[str, Any]]:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        task_info = self.active_tasks.get(task_id) or self.completed_tasks.get(task_id)
        if task_info:
            result = asdict(task_info)
            # ä¸è¿”å›å®Œæ•´ç»“æœæ•°æ®ï¼Œåªè¿”å›ç»Ÿè®¡ä¿¡æ¯
            if result.get('result'):
                result['result_count'] = len(result['result'])
                result['result'] = None
            return result
        return None


# å…¨å±€å•ä¾‹
_concurrent_manager: Optional[ConcurrentManager] = None


async def get_concurrent_manager() -> ConcurrentManager:
    """è·å–å…¨å±€å¹¶å‘ç®¡ç†å™¨å®ä¾‹"""
    global _concurrent_manager
    if _concurrent_manager is None:
        _concurrent_manager = ConcurrentManager()
        await _concurrent_manager.start()
    return _concurrent_manager


async def concurrent_search_jobs(keyword: str, city: str = "shanghai", 
                                max_jobs: int = 20, priority: int = 1) -> List[Dict]:
    """
    å¹¶å‘æœç´¢å²—ä½ï¼ˆå¯¹å¤–æ¥å£ï¼‰
    
    Args:
        keyword: æœç´¢å…³é”®è¯
        city: åŸå¸‚
        max_jobs: æœ€å¤§å²—ä½æ•°
        priority: ä¼˜å…ˆçº§
        
    Returns:
        å²—ä½æ•°æ®åˆ—è¡¨
    """
    manager = await get_concurrent_manager()
    task_id = await manager.submit_task(keyword, city, max_jobs, priority)
    result = await manager.get_task_result(task_id)
    return result or []