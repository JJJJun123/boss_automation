#!/usr/bin/env python3
"""
çœŸæ­£çš„Playwrightè‡ªåŠ¨åŒ–Bossç›´è˜çˆ¬è™«
å®ç°å¯è§çš„æµè§ˆå™¨æ“ä½œå’ŒçœŸå®æ•°æ®æå–
"""

import asyncio
import logging
import urllib.parse
import time
from typing import List, Dict, Optional
from playwright.async_api import async_playwright, Browser, Page
from .enhanced_extractor import EnhancedDataExtractor
from .session_manager import SessionManager
from .retry_handler import RetryHandler, RetryConfig, ErrorType, RetryStrategy, retry_on_error

logger = logging.getLogger(__name__)


class RealPlaywrightBossSpider:
    """çœŸæ­£çš„Playwright Bossç›´è˜çˆ¬è™«"""
    
    def __init__(self, headless: bool = False):
        self.headless = headless
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        self.playwright = None
        self.enhanced_extractor = EnhancedDataExtractor()  # é›†æˆå¢å¼ºæå–å™¨
        self.session_manager = SessionManager()  # é›†æˆä¼šè¯ç®¡ç†å™¨
        self.retry_handler = RetryHandler()  # é›†æˆé‡è¯•å¤„ç†å™¨
        
        # Bossç›´è˜åŸå¸‚ä»£ç æ˜ å°„ (ä¸app_config.yamlä¿æŒä¸€è‡´)
        self.city_codes = {
            "shanghai": "101020100",   # ä¸Šæµ· (ä¿®å¤ï¼šä¹‹å‰é”™è¯¯ä¸º101210100)
            "beijing": "101010100",    # åŒ—äº¬ (æ­£ç¡®)
            "shenzhen": "101280600",   # æ·±åœ³ (æ­£ç¡®)
            "hangzhou": "101210100"    # æ­å· (ä¿®å¤ï¼šä¹‹å‰é”™è¯¯ä¸º101210300->å˜‰å…´)
        }
        
    @retry_on_error(max_attempts=3, base_delay=2.0, strategy=RetryStrategy.EXPONENTIAL_BACKOFF)
    async def start(self) -> bool:
        """å¯åŠ¨æµè§ˆå™¨ - å¸¦é‡è¯•æœºåˆ¶"""
        logger.info("ğŸ­ å¯åŠ¨çœŸæ­£çš„Playwrightæµè§ˆå™¨...")
        
        self.playwright = await async_playwright().start()
        
        # å¯åŠ¨Chromeæµè§ˆå™¨ï¼Œç¡®ä¿ç”¨æˆ·å¯ä»¥çœ‹åˆ°
        self.browser = await self.playwright.chromium.launch(
            headless=self.headless,
            args=[
                '--disable-blink-features=AutomationControlled',
                '--disable-web-security',
                '--disable-features=VizDisplayCompositor',
                '--start-maximized'  # æœ€å¤§åŒ–çª—å£
            ]
        )
        
        logger.info("ğŸ–¥ï¸ Chromeæµè§ˆå™¨çª—å£å·²æ‰“å¼€ï¼Œä½ åº”è¯¥èƒ½çœ‹åˆ°å®ƒï¼")
        
        # åˆ›å»ºæ–°é¡µé¢
        context = await self.browser.new_context(
            viewport={'width': 1280, 'height': 800},
            user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        self.page = await context.new_page()
        
        # ç¡®ä¿çª—å£åœ¨å‰å°
        await self.page.bring_to_front()
        logger.info("ğŸ“± æµè§ˆå™¨çª—å£å·²è®¾ç½®ä¸ºå‰å°æ˜¾ç¤º")
        
        logger.info("âœ… Playwrightæµè§ˆå™¨å¯åŠ¨æˆåŠŸ")
        return True
    
    async def search_jobs(self, keyword: str, city: str, max_jobs: int = 20) -> List[Dict]:
        """æœç´¢å²—ä½ - å¸¦å®Œå–„çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶"""
        
        # ä½¿ç”¨é‡è¯•æœºåˆ¶æ‰§è¡Œæ ¸å¿ƒæœç´¢é€»è¾‘
        search_config = RetryConfig(
            max_attempts=3,
            base_delay=5.0,
            strategy=RetryStrategy.EXPONENTIAL_BACKOFF,
            allowed_error_types=[
                ErrorType.NETWORK_ERROR,
                ErrorType.TIMEOUT_ERROR, 
                ErrorType.PAGE_LOAD_ERROR,
                ErrorType.ELEMENT_NOT_FOUND
            ]
        )
        
        try:
            return await self.retry_handler.execute_with_retry(
                self._search_jobs_core,
                keyword, city, max_jobs,
                config=search_config,
                context={'operation': 'search_jobs', 'keyword': keyword, 'city': city}
            )
        except Exception as e:
            logger.error(f"âŒ æœç´¢å²—ä½æœ€ç»ˆå¤±è´¥: {e}")
            # è®°å½•è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ç”¨äºåˆ†æ
            await self._log_search_failure(keyword, city, e)
            return []
    
    async def _search_jobs_core(self, keyword: str, city: str, max_jobs: int) -> List[Dict]:
        """æ ¸å¿ƒæœç´¢é€»è¾‘ï¼ˆå†…éƒ¨æ–¹æ³•ï¼Œä¾›é‡è¯•ä½¿ç”¨ï¼‰"""
        if not self.page:
            raise RuntimeError("æµè§ˆå™¨æœªå¯åŠ¨")
        
        # é¦–å…ˆç¡®ä¿å·²ç™»å½•
        logger.info("ğŸ” æ£€æŸ¥ç™»å½•çŠ¶æ€...")
        if not await self._ensure_logged_in():
            raise RuntimeError("ç™»å½•å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æœç´¢")
        
        # è·å–åŸå¸‚ä»£ç 
        city_code = self.city_codes.get(city, "101210100")  # é»˜è®¤ä¸Šæµ·
        
        logger.info(f"ğŸ” å¼€å§‹æœç´¢: {keyword} | åŸå¸‚: {city} ({city_code}) | æ•°é‡: {max_jobs}")
        
        # æ„å»ºæœç´¢URL
        encoded_keyword = urllib.parse.quote(keyword)
        search_url = f"https://www.zhipin.com/web/geek/job?query={encoded_keyword}&city={city_code}"
        
        logger.info(f"ğŸŒ å¯¼èˆªåˆ°: {search_url}")
        
        # å¯¼èˆªåˆ°æœç´¢é¡µé¢
        await self._navigate_to_search_page(search_url)
        
        # å¤„ç†é¡µé¢åŠ è½½å’Œé¢„å¤„ç†
        await self._prepare_search_page()
        
        # ä½¿ç”¨å¢å¼ºæå–å™¨æå–å²—ä½æ•°æ®
        logger.info("ğŸš€ å¯ç”¨å¢å¼ºæ•°æ®æå–å¼•æ“...")
        jobs = await self.enhanced_extractor.extract_job_listings_enhanced(self.page, max_jobs)
        
        # éªŒè¯ç»“æœ
        if not jobs:
            await self._handle_no_jobs_found()
            return []
        
        logger.info(f"âœ… æˆåŠŸæå– {len(jobs)} ä¸ªå²—ä½åŸºç¡€ä¿¡æ¯")
        
        # è·å–è¯¦æƒ…é¡µä¿¡æ¯
        logger.info("ğŸ“„ å¼€å§‹è·å–å²—ä½è¯¦æƒ…...")
        jobs_with_details = await self._fetch_job_details(jobs)
        
        logger.info(f"âœ… å®Œæˆè¯¦æƒ…è·å–ï¼Œå…± {len(jobs_with_details)} ä¸ªå²—ä½")
        return jobs_with_details
    
    @retry_on_error(max_attempts=3, base_delay=2.0)
    async def _navigate_to_search_page(self, search_url: str) -> None:
        """å¯¼èˆªåˆ°æœç´¢é¡µé¢"""
        logger.info("ğŸ”— æ­£åœ¨å¯¼èˆªåˆ°Bossç›´è˜æœç´¢é¡µé¢...")
        logger.info("ğŸ‘€ è¯·è§‚å¯Ÿæµè§ˆå™¨çª—å£ï¼Œä½ åº”è¯¥èƒ½çœ‹åˆ°é¡µé¢åŠ è½½è¿‡ç¨‹")
        
        await self.page.goto(search_url, wait_until="domcontentloaded", timeout=15000)
        
        # æˆªå›¾è®°å½•å½“å‰é¡µé¢
        screenshot_path = f"boss_search_{int(time.time())}.png"
        await self.page.screenshot(path=screenshot_path)
        logger.info(f"ğŸ“¸ å·²æˆªå›¾å½“å‰é¡µé¢: {screenshot_path}")
    
    async def _prepare_search_page(self) -> None:
        """å‡†å¤‡æœç´¢é¡µé¢ï¼ˆé¡µé¢åŠ è½½ã€æ»šåŠ¨ç­‰ï¼‰"""
        # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½å®Œæˆ
        logger.info("â³ ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½...")
        
        # æ£€æŸ¥é¡µé¢æ˜¯å¦è¿˜åœ¨åŠ è½½çŠ¶æ€
        max_wait_time = 60  # æœ€å¤§ç­‰å¾…60ç§’
        wait_start = time.time()
        
        while time.time() - wait_start < max_wait_time:
            try:
                # æ£€æŸ¥é¡µé¢æ ‡é¢˜æ˜¯å¦è¿˜æ˜¯"è¯·ç¨å€™"
                title = await self.page.title()
                if title != "è¯·ç¨å€™":
                    logger.info(f"âœ… é¡µé¢åŠ è½½å®Œæˆï¼Œæ ‡é¢˜: {title}")
                    break
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å²—ä½å†…å®¹å‡ºç°
                job_indicators = await self.page.query_selector_all('li, .job-card, [data-jobid], .job-item')
                if job_indicators:
                    logger.info(f"âœ… æ£€æµ‹åˆ° {len(job_indicators)} ä¸ªæ½œåœ¨å²—ä½å…ƒç´ ")
                    break
                
                logger.info("â³ é¡µé¢ä»åœ¨åŠ è½½ä¸­ï¼Œç»§ç»­ç­‰å¾…...")
                await asyncio.sleep(3)
                
            except Exception as e:
                logger.debug(f"æ£€æŸ¥é¡µé¢çŠ¶æ€æ—¶å‡ºé”™: {e}")
                await asyncio.sleep(2)
        
        # é¢å¤–ç­‰å¾…ç¡®ä¿åŠ¨æ€å†…å®¹åŠ è½½
        await asyncio.sleep(5)
        
        # æ™ºèƒ½æ»šåŠ¨é¡µé¢ä»¥åŠ è½½æ›´å¤šå²—ä½
        logger.info("ğŸ“œ æ»šåŠ¨é¡µé¢ä»¥è§¦å‘æ›´å¤šå²—ä½åŠ è½½...")
        await self._smart_scroll_page()
        
        # æ»šåŠ¨å›é¡¶éƒ¨
        await self.page.evaluate("window.scrollTo(0, 0)")
        await asyncio.sleep(3)
        
        logger.info("ğŸ“„ é¡µé¢å·²å‡†å¤‡å®Œæˆï¼Œå¼€å§‹å¤„ç†å¯èƒ½çš„å¼¹çª—...")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½•æˆ–æœ‰éªŒè¯ç 
        await self._handle_login_or_captcha()
    
    async def _smart_scroll_page(self) -> None:
        """æ™ºèƒ½æ»šåŠ¨é¡µé¢ç­–ç•¥"""
        try:
            # å®‰å…¨åœ°è·å–é¡µé¢é«˜åº¦
            initial_height = await self.page.evaluate("""
                () => {
                    return document.body ? document.body.scrollHeight : window.innerHeight;
                }
            """)
            
            for scroll_attempt in range(3):
                # å®‰å…¨åœ°æ»šåŠ¨åˆ°é¡µé¢åº•éƒ¨
                await self.page.evaluate("""
                    () => {
                        const height = document.body ? document.body.scrollHeight : document.documentElement.scrollHeight;
                        window.scrollTo(0, height);
                    }
                """)
                await asyncio.sleep(2)
                
                # å®‰å…¨åœ°æ£€æŸ¥æ˜¯å¦æœ‰æ–°å†…å®¹åŠ è½½
                new_height = await self.page.evaluate("""
                    () => {
                        return document.body ? document.body.scrollHeight : document.documentElement.scrollHeight;
                    }
                """)
                logger.info(f"   æ»šåŠ¨ {scroll_attempt + 1}/3ï¼Œé¡µé¢é«˜åº¦: {initial_height} -> {new_height}")
                
                # å¦‚æœé¡µé¢é«˜åº¦æ²¡æœ‰å˜åŒ–ï¼Œå¯èƒ½å·²ç»åŠ è½½å®Œæ¯•
                if new_height == initial_height:
                    logger.info("   é¡µé¢é«˜åº¦æœªå˜åŒ–ï¼Œå¯èƒ½å·²åŠ è½½å®Œæ¯•")
                    break
                
                initial_height = new_height
        except Exception as e:
            logger.warning(f"æ™ºèƒ½æ»šåŠ¨é¡µé¢å¤±è´¥: {e}")
            # é™çº§åˆ°åŸºç¡€æ»šåŠ¨
            try:
                await self.page.evaluate("window.scrollTo(0, window.innerHeight * 2)")
                await asyncio.sleep(2)
            except:
                pass
    
    async def _handle_no_jobs_found(self) -> None:
        """å¤„ç†æœªæ‰¾åˆ°å²—ä½çš„æƒ…å†µ"""
        screenshot_path = await self.take_screenshot()
        logger.warning(f"âš ï¸ æœªæ‰¾åˆ°å²—ä½ï¼Œå·²æˆªå›¾: {screenshot_path}")
        
        # æ£€æŸ¥é¡µé¢æ˜¯å¦æœ‰é”™è¯¯ä¿¡æ¯
        error_indicators = [
            '.empty-result', '.no-result', '.error-page', 
            ':has-text("æ²¡æœ‰æ‰¾åˆ°")', ':has-text("æš‚æ— æ•°æ®")'
        ]
        
        for selector in error_indicators:
            try:
                element = await self.page.query_selector(selector)
                if element and await element.is_visible():
                    error_text = await element.inner_text()
                    logger.warning(f"é¡µé¢æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯: {error_text}")
                    break
            except:
                continue
        
        logger.error("âŒ çœŸå®æŠ“å–å¤±è´¥ï¼Œæœªæ‰¾åˆ°ä»»ä½•å²—ä½æ•°æ®")
        logger.info("ğŸš« ä¸ç”Ÿæˆç¤ºä¾‹æ•°æ®ï¼Œä¿æŒæ•°æ®çœŸå®æ€§")
    
    async def _log_search_failure(self, keyword: str, city: str, exception: Exception) -> None:
        """è®°å½•æœç´¢å¤±è´¥çš„è¯¦ç»†ä¿¡æ¯"""
        try:
            failure_info = {
                'timestamp': time.time(),
                'keyword': keyword,
                'city': city,
                'error_type': type(exception).__name__,
                'error_message': str(exception),
                'page_url': self.page.url if self.page else 'unknown',
                'retry_stats': self.retry_handler.get_retry_stats()
            }
            
            # ä¿å­˜å¤±è´¥ä¿¡æ¯åˆ°æ–‡ä»¶
            import json
            failure_file = f"search_failure_{int(time.time())}.json"
            with open(failure_file, 'w', encoding='utf-8') as f:
                json.dump(failure_info, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ” æœç´¢å¤±è´¥è¯¦æƒ…å·²ä¿å­˜: {failure_file}")
            
        except Exception as e:
            logger.debug(f"è®°å½•æœç´¢å¤±è´¥ä¿¡æ¯æ—¶å‡ºé”™: {e}")
    
    async def _ensure_logged_in(self) -> bool:
        """ç¡®ä¿å·²ç™»å½•Bossç›´è˜ - ä½¿ç”¨å¢å¼ºä¼šè¯ç®¡ç†"""
        try:
            # é¦–å…ˆå¯¼èˆªåˆ°Bossç›´è˜é¦–é¡µ
            logger.info("ğŸ  å¯¼èˆªåˆ°Bossç›´è˜é¦–é¡µ...")
            await self.page.goto("https://www.zhipin.com", wait_until="domcontentloaded", timeout=15000)
            await asyncio.sleep(3)
            
            # å°è¯•åŠ è½½å·²ä¿å­˜çš„ä¼šè¯
            if await self.session_manager.load_session(self.page.context, "zhipin.com"):
                logger.info("ğŸª å·²åŠ è½½ä¿å­˜çš„ä¼šè¯ï¼Œåˆ·æ–°é¡µé¢...")
                await self.page.reload()
                await asyncio.sleep(3)
                
                # æ£€æŸ¥æ˜¯å¦ç™»å½•æˆåŠŸ
                if await self.session_manager.check_login_status(self.page, "zhipin.com"):
                    logger.info("âœ… ä½¿ç”¨ä¿å­˜çš„ä¼šè¯ç™»å½•æˆåŠŸ!")
                    return True
                else:
                    logger.warning("âš ï¸ ä¿å­˜çš„ä¼šè¯å·²å¤±æ•ˆï¼Œéœ€è¦é‡æ–°ç™»å½•")
            
            # ç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨ç™»å½•
            if await self.session_manager.wait_for_login(self.page, timeout=300, domain="zhipin.com"):
                # ä¿å­˜æ–°çš„ä¼šè¯
                await self.session_manager.save_session(self.page.context, self.page, "zhipin.com")
                return True
            else:
                logger.error("âŒ ç™»å½•å¤±è´¥")
                return False
            
        except Exception as e:
            logger.error(f"âŒ ç™»å½•è¿‡ç¨‹å‡ºé”™: {e}")
            return False
    
    def get_session_info(self) -> Dict:
        """è·å–å½“å‰ä¼šè¯ä¿¡æ¯"""
        return self.session_manager.get_session_info()
    
    async def _fetch_job_details(self, jobs: List[Dict]) -> List[Dict]:
        """è·å–å²—ä½è¯¦ç»†ä¿¡æ¯"""
        jobs_with_details = []
        
        for i, job in enumerate(jobs):
            try:
                logger.info(f"ğŸ“‹ è·å–ç¬¬ {i+1}/{len(jobs)} ä¸ªå²—ä½è¯¦æƒ…: {job.get('title', 'æœªçŸ¥å²—ä½')}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„URL
                job_url = job.get('url', '')
                if not job_url or not job_url.startswith('http'):
                    logger.warning(f"âš ï¸ å²—ä½ {i+1} æ²¡æœ‰æœ‰æ•ˆURLï¼Œè·³è¿‡è¯¦æƒ…è·å–")
                    jobs_with_details.append(job)
                    continue
                
                # è·å–è¯¦æƒ…é¡µæ•°æ®
                details = await self._extract_job_detail_page(job_url)
                
                # åˆå¹¶åŸºç¡€ä¿¡æ¯å’Œè¯¦æƒ…ä¿¡æ¯
                enhanced_job = {**job, **details}
                jobs_with_details.append(enhanced_job)
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                await asyncio.sleep(1)
                
            except Exception as e:
                logger.error(f"âŒ è·å–å²—ä½ {i+1} è¯¦æƒ…å¤±è´¥: {e}")
                # ä¿ç•™åŸå§‹æ•°æ®
                jobs_with_details.append(job)
                continue
        
        return jobs_with_details
    
    async def _extract_job_detail_page(self, job_url: str) -> Dict:
        """æå–å²—ä½è¯¦æƒ…é¡µä¿¡æ¯"""
        try:
            logger.debug(f"ğŸ”— è®¿é—®è¯¦æƒ…é¡µ: {job_url}")
            
            # å¯¼èˆªåˆ°è¯¦æƒ…é¡µ
            await self.page.goto(job_url, wait_until="domcontentloaded", timeout=10000)
            await asyncio.sleep(2)
            
            # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
            await self._wait_for_detail_page_load()
            
            # æå–å·¥ä½œèŒè´£
            job_description = await self._extract_job_description()
            
            # æå–ä»»èŒèµ„æ ¼  
            job_requirements = await self._extract_job_requirements()
            
            # æå–å…¬å¸ä¿¡æ¯
            company_details = await self._extract_company_details()
            
            # æå–ç¦åˆ©å¾…é‡
            benefits = await self._extract_benefits()
            
            return {
                'job_description': job_description,
                'job_requirements': job_requirements, 
                'company_details': company_details,
                'benefits': benefits,
                'detail_extraction_success': True
            }
            
        except Exception as e:
            logger.error(f"âŒ æå–è¯¦æƒ…é¡µå¤±è´¥: {e}")
            return {
                'job_description': 'è¯¦æƒ…é¡µåŠ è½½å¤±è´¥ï¼Œè¯·ç›´æ¥è®¿é—®å²—ä½é“¾æ¥æŸ¥çœ‹',
                'job_requirements': 'è¯¦æƒ…é¡µåŠ è½½å¤±è´¥ï¼Œè¯·ç›´æ¥è®¿é—®å²—ä½é“¾æ¥æŸ¥çœ‹',
                'company_details': 'è¯¦æƒ…é¡µåŠ è½½å¤±è´¥',
                'benefits': 'è¯¦æƒ…é¡µåŠ è½½å¤±è´¥',
                'detail_extraction_success': False
            }
    
    async def _wait_for_detail_page_load(self) -> None:
        """ç­‰å¾…è¯¦æƒ…é¡µåŠ è½½å®Œæˆ"""
        try:
            # ç­‰å¾…å…³é”®å…ƒç´ å‡ºç°
            key_selectors = [
                '.job-sec-text',  # å²—ä½æè¿°åŒºåŸŸ
                '.job-detail-section',  # è¯¦æƒ…åŒºåŸŸ
                '.job-primary',  # ä¸»è¦ä¿¡æ¯åŒºåŸŸ
                '.job-banner'  # æ¨ªå¹…åŒºåŸŸ
            ]
            
            # å°è¯•ç­‰å¾…ä»»æ„ä¸€ä¸ªå…³é”®é€‰æ‹©å™¨å‡ºç°
            for selector in key_selectors:
                try:
                    await self.page.wait_for_selector(selector, timeout=3000)
                    logger.debug(f"âœ… è¯¦æƒ…é¡µå…³é”®å…ƒç´ å·²åŠ è½½: {selector}")
                    break
                except:
                    continue
            
            # é¢å¤–ç­‰å¾…ç¡®ä¿åŠ¨æ€å†…å®¹åŠ è½½
            await asyncio.sleep(2)
            
        except Exception as e:
            logger.debug(f"ç­‰å¾…è¯¦æƒ…é¡µåŠ è½½æ—¶å‡ºé”™: {e}")
    
    async def _extract_job_description(self) -> str:
        """æå–å·¥ä½œèŒè´£"""
        selectors = [
            '.job-sec-text',  # Bossç›´è˜å¸¸ç”¨çš„èŒè´£æè¿°é€‰æ‹©å™¨
            '.job-detail-text .text',
            '.job-description .text-desc',
            '.job-detail .job-sec .text-desc',
            '[class*="job-sec"] .text',
            '.text-desc',
            '.job-content .text'
        ]
        
        for selector in selectors:
            try:
                elements = await self.page.query_selector_all(selector)
                if elements:
                    # è·å–æ‰€æœ‰åŒ¹é…å…ƒç´ çš„æ–‡æœ¬
                    texts = []
                    for element in elements:
                        text = await element.inner_text()
                        if text and text.strip():
                            texts.append(text.strip())
                    
                    if texts:
                        # æŸ¥æ‰¾åŒ…å«"èŒè´£"ã€"å·¥ä½œå†…å®¹"ç­‰å…³é”®è¯çš„éƒ¨åˆ†
                        for text in texts:
                            if any(keyword in text for keyword in ['èŒè´£', 'å·¥ä½œå†…å®¹', 'å²—ä½èŒè´£', 'ä¸»è¦å·¥ä½œ']):
                                logger.debug(f"âœ… æ‰¾åˆ°å·¥ä½œèŒè´£: {selector}")
                                return text
                        
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šå…³é”®è¯ï¼Œè¿”å›ç¬¬ä¸€ä¸ªè¾ƒé•¿çš„æ–‡æœ¬
                        for text in texts:
                            if len(text) > 50:  # èŒè´£æè¿°é€šå¸¸è¾ƒé•¿
                                logger.debug(f"âœ… æ‰¾åˆ°å·¥ä½œæè¿°: {selector}")
                                return text
                                
            except Exception as e:
                logger.debug(f"æå–å·¥ä½œèŒè´£å¤±è´¥ {selector}: {e}")
                continue
        
        return "å·¥ä½œèŒè´£ä¿¡æ¯æœªæ‰¾åˆ°ï¼Œè¯·æŸ¥çœ‹å²—ä½è¯¦æƒ…é¡µ"
    
    async def _extract_job_requirements(self) -> str:
        """æå–ä»»èŒèµ„æ ¼"""
        selectors = [
            '.job-sec-text',
            '.job-detail-text .text', 
            '.job-requirements .text-desc',
            '.job-detail .job-sec .text-desc',
            '[class*="job-sec"] .text',
            '.text-desc',
            '.job-content .text'
        ]
        
        for selector in selectors:
            try:
                elements = await self.page.query_selector_all(selector)
                if elements:
                    texts = []
                    for element in elements:
                        text = await element.inner_text()
                        if text and text.strip():
                            texts.append(text.strip())
                    
                    if texts:
                        # æŸ¥æ‰¾åŒ…å«"è¦æ±‚"ã€"èµ„æ ¼"ã€"æ¡ä»¶"ç­‰å…³é”®è¯çš„éƒ¨åˆ†
                        for text in texts:
                            if any(keyword in text for keyword in ['ä»»èŒ', 'è¦æ±‚', 'èµ„æ ¼', 'æ¡ä»¶', 'æŠ€èƒ½', 'ç»éªŒ']):
                                logger.debug(f"âœ… æ‰¾åˆ°ä»»èŒè¦æ±‚: {selector}")
                                return text
                        
                        # å¦‚æœæœ‰å¤šä¸ªæ–‡æœ¬å—ï¼Œå–ç¬¬äºŒä¸ªï¼ˆç¬¬ä¸€ä¸ªé€šå¸¸æ˜¯èŒè´£ï¼‰
                        if len(texts) >= 2:
                            logger.debug(f"âœ… æ‰¾åˆ°ä»»èŒè¦æ±‚ï¼ˆç¬¬äºŒæ®µï¼‰: {selector}")
                            return texts[1]
                            
            except Exception as e:
                logger.debug(f"æå–ä»»èŒè¦æ±‚å¤±è´¥ {selector}: {e}")
                continue
        
        return "ä»»èŒè¦æ±‚ä¿¡æ¯æœªæ‰¾åˆ°ï¼Œè¯·æŸ¥çœ‹å²—ä½è¯¦æƒ…é¡µ"
    
    async def _extract_company_details(self) -> str:
        """æå–å…¬å¸è¯¦æƒ…"""
        selectors = [
            '.company-info .company-text',
            '.company-description',
            '.company-detail-text',
            '.company-info .text'
        ]
        
        for selector in selectors:
            try:
                element = await self.page.query_selector(selector)
                if element:
                    text = await element.inner_text()
                    if text and text.strip():
                        logger.debug(f"âœ… æ‰¾åˆ°å…¬å¸è¯¦æƒ…: {selector}")
                        return text.strip()
            except Exception as e:
                logger.debug(f"æå–å…¬å¸è¯¦æƒ…å¤±è´¥ {selector}: {e}")
                continue
        
        return "å…¬å¸è¯¦æƒ…ä¿¡æ¯æœªæ‰¾åˆ°"
    
    async def _extract_benefits(self) -> str:
        """æå–ç¦åˆ©å¾…é‡"""
        selectors = [
            '.job-tags .tag',
            '.welfare-list .welfare-item',
            '.job-welfare .tag-item',
            '.benefits .benefit-item'
        ]
        
        benefits = []
        for selector in selectors:
            try:
                elements = await self.page.query_selector_all(selector)
                for element in elements:
                    text = await element.inner_text()
                    if text and text.strip():
                        benefits.append(text.strip())
            except Exception as e:
                logger.debug(f"æå–ç¦åˆ©å¾…é‡å¤±è´¥ {selector}: {e}")
                continue
        
        if benefits:
            logger.debug(f"âœ… æ‰¾åˆ°ç¦åˆ©å¾…é‡: {len(benefits)} é¡¹")
            return " | ".join(benefits[:10])  # é™åˆ¶æ•°é‡é¿å…è¿‡é•¿
        
        return "ç¦åˆ©å¾…é‡ä¿¡æ¯æœªæ‰¾åˆ°"
    
    async def _handle_login_or_captcha(self):
        """å¤„ç†ç™»å½•æˆ–éªŒè¯ç """
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç™»å½•å¼¹çª—
            login_modal = await self.page.query_selector('.login-dialog, .dialog-wrap')
            if login_modal:
                logger.info("ğŸ” æ£€æµ‹åˆ°ç™»å½•å¼¹çª—ï¼Œç­‰å¾…ç”¨æˆ·å¤„ç†...")
                # ç­‰å¾…ä¸€æ®µæ—¶é—´è®©ç”¨æˆ·å¤„ç†
                await asyncio.sleep(5)
            
            # æ£€æŸ¥éªŒè¯ç 
            captcha = await self.page.query_selector('.captcha, .verify-wrap')
            if captcha:
                logger.info("ğŸ”’ æ£€æµ‹åˆ°éªŒè¯ç ï¼Œç­‰å¾…ç”¨æˆ·å¤„ç†...")
                await asyncio.sleep(5)
                
        except Exception as e:
            logger.warning(f"âš ï¸ å¤„ç†ç™»å½•/éªŒè¯ç æ—¶å‡ºé”™: {e}")
    
    async def get_performance_report(self) -> Dict:
        """è·å–çˆ¬è™«æ€§èƒ½æŠ¥å‘Š"""
        return self.enhanced_extractor.get_performance_report()
    
    def get_system_status(self) -> Dict:
        """è·å–ç³»ç»Ÿæ•´ä½“çŠ¶æ€æŠ¥å‘Š"""
        return {
            'crawler_status': {
                'browser_active': self.browser is not None,
                'page_active': self.page is not None,
                'current_url': self.page.url if self.page else None
            },
            'session_info': self.session_manager.get_session_info(),
            'retry_stats': self.retry_handler.get_retry_stats(),
            'extractor_performance': self.enhanced_extractor.get_performance_report(),
            'city_codes': self.city_codes,
            'enhancement_status': {
                'smart_selector_enabled': True,
                'enhanced_extractor_enabled': True,
                'session_manager_enabled': True,
                'retry_handler_enabled': True,
                'version': 'v2.0-enhanced'
            }
        }
    
    
    def _generate_sample_jobs(self, keyword: str, city: str, max_jobs: int) -> List[Dict]:
        """ç”Ÿæˆç¤ºä¾‹å²—ä½æ•°æ®ï¼ˆå½“çœŸå®æŠ“å–å¤±è´¥æ—¶ï¼‰"""
        
        city_names = {
            "shanghai": "ä¸Šæµ·",
            "beijing": "åŒ—äº¬",
            "shenzhen": "æ·±åœ³", 
            "hangzhou": "æ­å·"
        }
        
        city_name = city_names.get(city, "ä¸Šæµ·")
        
        companies = ["é˜¿é‡Œå·´å·´", "è…¾è®¯", "å­—èŠ‚è·³åŠ¨", "ç¾å›¢", "æ»´æ»´", "ç™¾åº¦", "ç½‘æ˜“", "å°ç±³", "åä¸º", "äº¬ä¸œ"]
        salaries = ["15-25KÂ·13è–ª", "20-30KÂ·14è–ª", "25-35KÂ·15è–ª", "18-28KÂ·13è–ª", "22-32KÂ·14è–ª"]
        
        jobs = []
        for i in range(max_jobs):
            company = companies[i % len(companies)]
            salary = salaries[i % len(salaries)]
            
            job = {
                "title": f"{keyword}å·¥ç¨‹å¸ˆ",
                "company": company,
                "salary": salary,
                "work_location": f"{city_name}Â·ä¸­å¿ƒåŒº",
                "url": f"https://www.zhipin.com/job_detail/{urllib.parse.quote(keyword)}_{i+1}",
                "tags": [keyword, "Python", "æ•°æ®åˆ†æ", "æœºå™¨å­¦ä¹ "][:3],
                "job_description": f"è´Ÿè´£{keyword}ç›¸å…³å·¥ä½œï¼ŒåŒ…æ‹¬æ•°æ®å¤„ç†ã€åˆ†æå»ºæ¨¡ç­‰ã€‚",
                "job_requirements": f"è¦æ±‚3-5å¹´{keyword}ç›¸å…³ç»éªŒï¼Œæœ¬ç§‘åŠä»¥ä¸Šå­¦å†ã€‚",
                "company_details": f"{company} - çŸ¥åäº’è”ç½‘å…¬å¸",
                "benefits": "äº”é™©ä¸€é‡‘,è‚¡ç¥¨æœŸæƒ,å¼¹æ€§å·¥ä½œ",
                "experience_required": "3-5å¹´",
                "education_required": "æœ¬ç§‘",
                "engine_source": "PlaywrightçœŸå®æŠ“å–ï¼ˆç¤ºä¾‹æ•°æ®ï¼‰"
            }
            jobs.append(job)
        
        return jobs
    
    async def take_screenshot(self, filename: str = None) -> str:
        """æˆªå–é¡µé¢æˆªå›¾"""
        try:
            if not self.page:
                return ""
            
            if not filename:
                timestamp = int(time.time())
                filename = f"boss_real_screenshot_{timestamp}.png"
            
            await self.page.screenshot(path=filename, full_page=True)
            logger.info(f"ğŸ“¸ æˆªå›¾ä¿å­˜: {filename}")
            return filename
            
        except Exception as e:
            logger.error(f"âŒ æˆªå›¾å¤±è´¥: {e}")
            return ""
    
    async def close(self):
        """å…³é—­æµè§ˆå™¨"""
        try:
            if self.page:
                await self.page.close()
            if self.browser:
                await self.browser.close()
            if self.playwright:
                await self.playwright.stop()
            
            logger.info("ğŸ”š æµè§ˆå™¨å·²å…³é—­")
            
        except Exception as e:
            logger.error(f"âŒ å…³é—­æµè§ˆå™¨å¤±è´¥: {e}")


# åŒæ­¥åŒ…è£…å™¨
class RealPlaywrightBossSpiderSync:
    """çœŸæ­£çš„Playwright Bossç›´è˜çˆ¬è™«åŒæ­¥ç‰ˆæœ¬"""
    
    def __init__(self, headless: bool = False):
        self.headless = headless
        self.spider = None
    
    def search_jobs(self, keyword: str, city: str, max_jobs: int = 20) -> List[Dict]:
        """æœç´¢å²—ä½ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
        async def _search():
            self.spider = RealPlaywrightBossSpider(headless=self.headless)
            
            try:
                if await self.spider.start():
                    return await self.spider.search_jobs(keyword, city, max_jobs)
                else:
                    return []
            finally:
                if self.spider:
                    await self.spider.close()
        
        return asyncio.run(_search())


# é›†æˆæ¥å£
def search_with_real_playwright(keyword: str, city: str = "shanghai", max_jobs: int = 20) -> List[Dict]:
    """ä½¿ç”¨çœŸæ­£çš„Playwrightæœç´¢Bossç›´è˜å²—ä½"""
    logger.info(f"ğŸ­ å¯åŠ¨çœŸæ­£çš„Playwrightè‡ªåŠ¨åŒ–æœç´¢: {keyword}")
    
    try:
        spider = RealPlaywrightBossSpiderSync(headless=False)  # å¯è§æ¨¡å¼
        jobs = spider.search_jobs(keyword, city, max_jobs)
        
        logger.info(f"âœ… çœŸå®æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(jobs)} ä¸ªå²—ä½")
        return jobs
        
    except Exception as e:
        logger.error(f"âŒ çœŸå®Playwrightæœç´¢å¤±è´¥: {e}")
        return []


if __name__ == "__main__":
    # æµ‹è¯•çœŸæ­£çš„Playwrightçˆ¬è™«
    import logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    print("ğŸ­ æµ‹è¯•çœŸæ­£çš„Playwright Bossç›´è˜çˆ¬è™«")
    print("=" * 50)
    
    # æµ‹è¯•æœç´¢
    jobs = search_with_real_playwright("æ•°æ®åˆ†æ", "shanghai", 3)
    
    print(f"\nâœ… æ‰¾åˆ° {len(jobs)} ä¸ªå²—ä½:")
    for i, job in enumerate(jobs, 1):
        print(f"\nğŸ“‹ å²—ä½ #{i}")
        print(f"  èŒä½: {job.get('title', 'æœªçŸ¥')}")
        print(f"  å…¬å¸: {job.get('company', 'æœªçŸ¥')}")
        print(f"  è–ªèµ„: {job.get('salary', 'æœªçŸ¥')}")
        print(f"  åœ°ç‚¹: {job.get('work_location', 'æœªçŸ¥')}")
        print(f"  é“¾æ¥: {job.get('url', 'æœªçŸ¥')}")