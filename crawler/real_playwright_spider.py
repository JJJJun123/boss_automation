#!/usr/bin/env python3
"""
çœŸæ­£çš„Playwrightè‡ªåŠ¨åŒ–Bossç›´è˜çˆ¬è™«
å®ç°å¯è§çš„æµè§ˆå™¨æ“ä½œå’ŒçœŸå®æ•°æ®æå–
"""

import asyncio
import logging
import urllib.parse
import time
from typing import List, Dict, Optional
from playwright.async_api import async_playwright, Browser, Page

logger = logging.getLogger(__name__)


class RealPlaywrightBossSpider:
    """çœŸæ­£çš„Playwright Bossç›´è˜çˆ¬è™«"""
    
    def __init__(self, headless: bool = False):
        self.headless = headless
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        self.playwright = None
        
        # Bossç›´è˜åŸå¸‚ä»£ç æ˜ å°„ (ä¸app_config.yamlä¿æŒä¸€è‡´)
        self.city_codes = {
            "shanghai": "101020100",   # ä¸Šæµ· (ä¿®å¤ï¼šä¹‹å‰é”™è¯¯ä¸º101210100)
            "beijing": "101010100",    # åŒ—äº¬ (æ­£ç¡®)
            "shenzhen": "101280600",   # æ·±åœ³ (æ­£ç¡®)
            "hangzhou": "101210100"    # æ­å· (ä¿®å¤ï¼šä¹‹å‰é”™è¯¯ä¸º101210300->å˜‰å…´)
        }
        
    async def start(self) -> bool:
        """å¯åŠ¨æµè§ˆå™¨"""
        try:
            logger.info("ğŸ­ å¯åŠ¨çœŸæ­£çš„Playwrightæµè§ˆå™¨...")
            
            self.playwright = await async_playwright().start()
            
            # å¯åŠ¨Chromeæµè§ˆå™¨ï¼Œç¡®ä¿ç”¨æˆ·å¯ä»¥çœ‹åˆ°
            self.browser = await self.playwright.chromium.launch(
                headless=self.headless,
                args=[
                    '--disable-blink-features=AutomationControlled',
                    '--disable-web-security',
                    '--disable-features=VizDisplayCompositor',
                    '--start-maximized'  # æœ€å¤§åŒ–çª—å£
                ]
            )
            
            logger.info("ğŸ–¥ï¸ Chromeæµè§ˆå™¨çª—å£å·²æ‰“å¼€ï¼Œä½ åº”è¯¥èƒ½çœ‹åˆ°å®ƒï¼")
            
            # åˆ›å»ºæ–°é¡µé¢
            context = await self.browser.new_context(
                viewport={'width': 1280, 'height': 800},
                user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            )
            self.page = await context.new_page()
            
            # ç¡®ä¿çª—å£åœ¨å‰å°
            await self.page.bring_to_front()
            logger.info("ğŸ“± æµè§ˆå™¨çª—å£å·²è®¾ç½®ä¸ºå‰å°æ˜¾ç¤º")
            
            logger.info("âœ… Playwrightæµè§ˆå™¨å¯åŠ¨æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å¯åŠ¨æµè§ˆå™¨å¤±è´¥: {e}")
            return False
    
    async def search_jobs(self, keyword: str, city: str, max_jobs: int = 20) -> List[Dict]:
        """æœç´¢å²—ä½"""
        try:
            if not self.page:
                logger.error("âŒ æµè§ˆå™¨æœªå¯åŠ¨")
                return []
            
            # é¦–å…ˆç¡®ä¿å·²ç™»å½•
            logger.info("ğŸ” æ£€æŸ¥ç™»å½•çŠ¶æ€...")
            if not await self._ensure_logged_in():
                logger.error("âŒ ç™»å½•å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æœç´¢")
                return []
            
            # è·å–åŸå¸‚ä»£ç 
            city_code = self.city_codes.get(city, "101210100")  # é»˜è®¤ä¸Šæµ·
            
            logger.info(f"ğŸ” å¼€å§‹æœç´¢: {keyword} | åŸå¸‚: {city} ({city_code}) | æ•°é‡: {max_jobs}")
            
            # æ„å»ºæœç´¢URL
            encoded_keyword = urllib.parse.quote(keyword)
            search_url = f"https://www.zhipin.com/web/geek/job?query={encoded_keyword}&city={city_code}"
            
            logger.info(f"ğŸŒ å¯¼èˆªåˆ°: {search_url}")
            
            # å¯¼èˆªåˆ°æœç´¢é¡µé¢
            logger.info("ğŸ”— æ­£åœ¨å¯¼èˆªåˆ°Bossç›´è˜æœç´¢é¡µé¢...")
            logger.info("ğŸ‘€ è¯·è§‚å¯Ÿæµè§ˆå™¨çª—å£ï¼Œä½ åº”è¯¥èƒ½çœ‹åˆ°é¡µé¢åŠ è½½è¿‡ç¨‹")
            await self.page.goto(search_url, wait_until="domcontentloaded", timeout=15000)
            
            # æˆªå›¾è®°å½•å½“å‰é¡µé¢
            screenshot_path = f"boss_search_{int(time.time())}.png"
            await self.page.screenshot(path=screenshot_path)
            logger.info(f"ğŸ“¸ å·²æˆªå›¾å½“å‰é¡µé¢: {screenshot_path}")
            
            # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆï¼ˆå¢åŠ ç­‰å¾…æ—¶é—´ï¼‰
            logger.info("â³ ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½...")
            await asyncio.sleep(8)  # å¢åŠ åˆ°8ç§’ï¼Œç¡®ä¿åŠ¨æ€å†…å®¹åŠ è½½
            
            # å°è¯•æ»šåŠ¨é¡µé¢ä»¥åŠ è½½æ›´å¤šå²—ä½ï¼ˆBossç›´è˜å¯èƒ½ä½¿ç”¨æ‡’åŠ è½½ï¼‰
            logger.info("ğŸ“œ æ»šåŠ¨é¡µé¢ä»¥è§¦å‘æ›´å¤šå²—ä½åŠ è½½...")
            for scroll_attempt in range(3):
                await self.page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                await asyncio.sleep(2)  # æ¯æ¬¡æ»šåŠ¨åç­‰å¾…2ç§’
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ–°å†…å®¹åŠ è½½
                page_height = await self.page.evaluate("document.body.scrollHeight")
                logger.info(f"   æ»šåŠ¨ {scroll_attempt + 1}/3ï¼Œé¡µé¢é«˜åº¦: {page_height}")
            
            # æ»šåŠ¨å›é¡¶éƒ¨
            await self.page.evaluate("window.scrollTo(0, 0)")
            await asyncio.sleep(2)
            
            logger.info("ğŸ“„ é¡µé¢å·²åŠ è½½ï¼Œå¼€å§‹å¤„ç†å¯èƒ½çš„å¼¹çª—...")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½•æˆ–æœ‰éªŒè¯ç 
            await self._handle_login_or_captcha()
            
            # æå–å²—ä½æ•°æ®
            jobs = await self._extract_jobs_from_page(max_jobs)
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å²—ä½ï¼Œå°è¯•æˆªå›¾è°ƒè¯•
            if not jobs:
                screenshot_path = await self.take_screenshot()
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°å²—ä½ï¼Œå·²æˆªå›¾: {screenshot_path}")
                logger.error("âŒ çœŸå®æŠ“å–å¤±è´¥ï¼Œæœªæ‰¾åˆ°ä»»ä½•å²—ä½æ•°æ®")
                
                # è¿”å›ç©ºåˆ—è¡¨ï¼Œä¸ç”Ÿæˆå‡æ•°æ®
                logger.info("ğŸš« ä¸ç”Ÿæˆç¤ºä¾‹æ•°æ®ï¼Œä¿æŒæ•°æ®çœŸå®æ€§")
                return []
            
            logger.info(f"âœ… æˆåŠŸæå– {len(jobs)} ä¸ªå²—ä½")
            return jobs
            
        except Exception as e:
            logger.error(f"âŒ æœç´¢å²—ä½å¤±è´¥: {e}")
            return []
    
    async def _ensure_logged_in(self) -> bool:
        """ç¡®ä¿å·²ç™»å½•Bossç›´è˜"""
        try:
            # é¦–å…ˆå¯¼èˆªåˆ°Bossç›´è˜é¦–é¡µ
            logger.info("ğŸ  å¯¼èˆªåˆ°Bossç›´è˜é¦–é¡µ...")
            await self.page.goto("https://www.zhipin.com", wait_until="domcontentloaded", timeout=15000)
            await asyncio.sleep(3)
            
            # å°è¯•åŠ è½½å·²ä¿å­˜çš„cookies
            cookies_loaded = await self._load_cookies()
            if cookies_loaded:
                logger.info("ğŸª å·²åŠ è½½ä¿å­˜çš„cookiesï¼Œåˆ·æ–°é¡µé¢...")
                await self.page.reload()
                await asyncio.sleep(3)
                
                # æ£€æŸ¥æ˜¯å¦ç™»å½•æˆåŠŸ
                if await self._check_login_status():
                    logger.info("âœ… ä½¿ç”¨ä¿å­˜çš„cookiesç™»å½•æˆåŠŸ!")
                    return True
                else:
                    logger.warning("âš ï¸ ä¿å­˜çš„cookieså·²å¤±æ•ˆï¼Œéœ€è¦é‡æ–°ç™»å½•")
            
            # éœ€è¦æ‰‹åŠ¨ç™»å½•
            logger.info("=" * 50)
            logger.info("ğŸ” éœ€è¦æ‰‹åŠ¨ç™»å½•Bossç›´è˜")
            logger.info("è¯·åœ¨æµè§ˆå™¨çª—å£ä¸­å®Œæˆä»¥ä¸‹æ“ä½œï¼š")
            logger.info("1. ç‚¹å‡»é¡µé¢å³ä¸Šè§’çš„ 'ç™»å½•' æŒ‰é’®")
            logger.info("2. ä½¿ç”¨æ‰«ç æˆ–è´¦å·å¯†ç ç™»å½•")
            logger.info("3. ç™»å½•æˆåŠŸåï¼Œä¼šè‡ªåŠ¨ç»§ç»­æ‰§è¡Œ")
            logger.info("=" * 50)
            
            # ç­‰å¾…ç”¨æˆ·ç™»å½•ï¼Œæœ€å¤šç­‰å¾…5åˆ†é’Ÿ
            start_time = time.time()
            timeout = 300  # 5åˆ†é’Ÿ
            
            while time.time() - start_time < timeout:
                # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡ç™»å½•çŠ¶æ€
                await asyncio.sleep(5)
                
                if await self._check_login_status():
                    logger.info("âœ… æ£€æµ‹åˆ°ç™»å½•æˆåŠŸ!")
                    # ä¿å­˜cookies
                    await self._save_cookies()
                    return True
                else:
                    remaining = int(timeout - (time.time() - start_time))
                    logger.info(f"â³ ç­‰å¾…ç™»å½•ä¸­... (å‰©ä½™ {remaining} ç§’)")
            
            logger.error("âŒ ç™»å½•è¶…æ—¶ï¼Œè¯·åœ¨5åˆ†é’Ÿå†…å®Œæˆç™»å½•")
            return False
            
        except Exception as e:
            logger.error(f"âŒ ç™»å½•è¿‡ç¨‹å‡ºé”™: {e}")
            return False
    
    async def _check_login_status(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²ç™»å½•"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç”¨æˆ·ç›¸å…³å…ƒç´ ï¼ˆå·²ç™»å½•ï¼‰
            user_selectors = [
                '.nav-figure img',  # ç”¨æˆ·å¤´åƒ
                '.nav-figure',      # ç”¨æˆ·ä¿¡æ¯åŒºåŸŸ
                '.user-name',       # ç”¨æˆ·å
                '[class*="avatar"]', # åŒ…å«avatarçš„å…ƒç´ 
                '.dropdown-avatar'   # ä¸‹æ‹‰å¤´åƒ
            ]
            
            for selector in user_selectors:
                try:
                    element = await self.page.query_selector(selector)
                    if element and await element.is_visible():
                        logger.info(f"âœ“ æ‰¾åˆ°ç”¨æˆ·å…ƒç´ : {selector}")
                        return True
                except:
                    continue
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç™»å½•æŒ‰é’®ï¼ˆæœªç™»å½•ï¼‰
            login_selectors = [
                'a.btn-sign-in',
                'button.btn-sign-in',
                '.sign-wrap .btn',
                'a:has-text("ç™»å½•")',
                'button:has-text("ç™»å½•")'
            ]
            
            for selector in login_selectors:
                try:
                    element = await self.page.query_selector(selector)
                    if element and await element.is_visible():
                        logger.info(f"âœ— æ‰¾åˆ°ç™»å½•æŒ‰é’®: {selector}")
                        return False
                except:
                    continue
            
            # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œæ£€æŸ¥é¡µé¢URL
            current_url = self.page.url
            if '/login' in current_url or '/signup' in current_url:
                return False
            
            # é»˜è®¤è®¤ä¸ºå·²ç™»å½•
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ£€æŸ¥ç™»å½•çŠ¶æ€å¤±è´¥: {e}")
            return False
    
    async def _save_cookies(self) -> None:
        """ä¿å­˜cookiesåˆ°æ–‡ä»¶"""
        try:
            cookies = await self.page.context.cookies()
            import json
            import os
            
            cookies_dir = os.path.join(os.path.dirname(__file__), 'cookies')
            os.makedirs(cookies_dir, exist_ok=True)
            
            cookies_file = os.path.join(cookies_dir, 'boss_cookies.json')
            with open(cookies_file, 'w', encoding='utf-8') as f:
                json.dump(cookies, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… Cookieså·²ä¿å­˜åˆ°: {cookies_file}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜cookieså¤±è´¥: {e}")
    
    async def _load_cookies(self) -> bool:
        """åŠ è½½å·²ä¿å­˜çš„cookies"""
        try:
            import json
            import os
            
            cookies_file = os.path.join(os.path.dirname(__file__), 'cookies', 'boss_cookies.json')
            
            if not os.path.exists(cookies_file):
                logger.info("ğŸ“„ æ²¡æœ‰æ‰¾åˆ°ä¿å­˜çš„cookiesæ–‡ä»¶")
                return False
            
            with open(cookies_file, 'r', encoding='utf-8') as f:
                cookies = json.load(f)
            
            # æ·»åŠ cookiesåˆ°æµè§ˆå™¨
            await self.page.context.add_cookies(cookies)
            logger.info(f"âœ… å·²åŠ è½½ {len(cookies)} ä¸ªcookies")
            return True
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½cookieså¤±è´¥: {e}")
            return False
    
    async def _handle_login_or_captcha(self):
        """å¤„ç†ç™»å½•æˆ–éªŒè¯ç """
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç™»å½•å¼¹çª—
            login_modal = await self.page.query_selector('.login-dialog, .dialog-wrap')
            if login_modal:
                logger.info("ğŸ” æ£€æµ‹åˆ°ç™»å½•å¼¹çª—ï¼Œç­‰å¾…ç”¨æˆ·å¤„ç†...")
                # ç­‰å¾…ä¸€æ®µæ—¶é—´è®©ç”¨æˆ·å¤„ç†
                await asyncio.sleep(5)
            
            # æ£€æŸ¥éªŒè¯ç 
            captcha = await self.page.query_selector('.captcha, .verify-wrap')
            if captcha:
                logger.info("ğŸ”’ æ£€æµ‹åˆ°éªŒè¯ç ï¼Œç­‰å¾…ç”¨æˆ·å¤„ç†...")
                await asyncio.sleep(5)
                
        except Exception as e:
            logger.warning(f"âš ï¸ å¤„ç†ç™»å½•/éªŒè¯ç æ—¶å‡ºé”™: {e}")
    
    async def _extract_jobs_from_page(self, max_jobs: int) -> List[Dict]:
        """ä»é¡µé¢æå–å²—ä½ä¿¡æ¯"""
        try:
            logger.info("ğŸ“‹ å¼€å§‹æå–é¡µé¢å²—ä½ä¿¡æ¯...")
            
            # å…ˆæ£€æŸ¥é¡µé¢å†…å®¹
            page_content = await self.page.content()
            if "å²—ä½" in page_content or "job" in page_content.lower():
                logger.info("âœ“ é¡µé¢åŒ…å«å²—ä½ç›¸å…³å†…å®¹")
            else:
                logger.warning("âš ï¸ é¡µé¢å¯èƒ½æœªæ­£ç¡®åŠ è½½å²—ä½å†…å®¹")
            
            # å°è¯•å¤šç§é€‰æ‹©å™¨ - åŸºäºå®æ—¶åˆ†æç»“æœä¼˜åŒ–ï¼Œä¼˜å…ˆé€‰æ‹©ç²¾ç¡®çš„å²—ä½å®¹å™¨
            selectors_to_try = [
                # æœ€ç²¾ç¡®çš„é€‰æ‹©å™¨ - ç›´æ¥é€‰æ‹©åŒ…å«å²—ä½é“¾æ¥çš„çˆ¶å®¹å™¨
                'li:has(a[href*="job_detail"])',  # åªé€‰æ‹©åŒ…å«å²—ä½é“¾æ¥çš„li
                
                # Bossç›´è˜ç‰¹æœ‰é€‰æ‹©å™¨
                '.job-detail-box',        # ä¹‹å‰æˆåŠŸçš„é€‰æ‹©å™¨
                'a[ka*="search_list"]',   # Bossç›´è˜ç‰¹æœ‰çš„kaå±æ€§
                '.job-card-wrapper', '.job-card-container',
                'li.job-card-container',
                '.job-card-left', '.job-info-box',
                
                # åŒ…å«å²—ä½ä¿¡æ¯çš„å®¹å™¨
                'li:has(.job-name)',     # åªé€‰æ‹©åŒ…å«å²—ä½æ ‡é¢˜çš„li
                'div:has(.job-name)',    # åªé€‰æ‹©åŒ…å«å²—ä½æ ‡é¢˜çš„div
                '.job-list-box .job-card-body',
                
                # å¤‡ç”¨é€‰æ‹©å™¨ - éœ€è¦åç»­è¿‡æ»¤
                '.job-list-container li', '.search-job-result li',
                '.job-list .job-item', '.job-result-item',
                
                # æ›´é€šç”¨çš„é€‰æ‹©å™¨ï¼ˆæœ€åå°è¯•ï¼‰
                'li[class*="job"]', 'div[class*="job-card"]',
                'a[class*="job"]', '.job-primary', '.job-content'
            ]
            
            # å°è¯•æ‰€æœ‰é€‰æ‹©å™¨ï¼Œæ”¶é›†æ‰€æœ‰å¯èƒ½çš„å²—ä½å…ƒç´ 
            all_job_cards = []
            successful_selectors = []
            
            for selector in selectors_to_try:
                try:
                    logger.info(f"ğŸ” å°è¯•é€‰æ‹©å™¨: {selector}")
                    elements = await self.page.query_selector_all(selector)
                    if elements:
                        logger.info(f"   âœ… æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")
                        all_job_cards.extend(elements)
                        successful_selectors.append((selector, len(elements)))
                    else:
                        logger.debug(f"   é€‰æ‹©å™¨ {selector} æœªæ‰¾åˆ°å…ƒç´ ")
                except Exception as e:
                    logger.debug(f"   é€‰æ‹©å™¨ {selector} å¼‚å¸¸: {e}")
                    continue
            
            # å»é‡ï¼ˆé¿å…åŒä¸€ä¸ªå…ƒç´ è¢«å¤šä¸ªé€‰æ‹©å™¨é€‰ä¸­ï¼‰
            unique_job_cards = []
            seen_job_urls = set()  # åŸºäºå²—ä½URLå»é‡ï¼Œæ›´å¯é 
            seen_element_positions = set()  # åŸºäºå…ƒç´ ä½ç½®å»é‡ï¼Œé¿å…åµŒå¥—å…ƒç´ 
            
            for element in all_job_cards:
                try:
                    # æ–¹æ³•1ï¼šåŸºäºå²—ä½URLå»é‡ï¼ˆæœ€å¯é ï¼‰
                    link_elem = await element.query_selector('a[href*="job_detail"]')
                    if link_elem:
                        href = await link_elem.get_attribute('href')
                        if href:
                            # æ¸…ç†URLï¼Œç§»é™¤æŸ¥è¯¢å‚æ•°
                            clean_href = href.split('?')[0] if '?' in href else href
                            if clean_href in seen_job_urls:
                                logger.debug(f"è·³è¿‡é‡å¤å²—ä½URL: {clean_href}")
                                continue
                            seen_job_urls.add(clean_href)
                    
                    # æ–¹æ³•2ï¼šåŸºäºå…ƒç´ ä½ç½®å»é‡ï¼ˆé¿å…åµŒå¥—å…ƒç´ é‡å¤ï¼‰
                    try:
                        bbox = await element.bounding_box()
                        if bbox:
                            # ä½¿ç”¨ä½ç½®å’Œå°ºå¯¸ä½œä¸ºå”¯ä¸€æ ‡è¯†
                            position_key = (
                                round(bbox['x']), 
                                round(bbox['y']), 
                                round(bbox['width']), 
                                round(bbox['height'])
                            )
                            if position_key in seen_element_positions:
                                logger.debug(f"è·³è¿‡é‡å¤ä½ç½®å…ƒç´ : {position_key}")
                                continue
                            seen_element_positions.add(position_key)
                    except:
                        pass  # ä½ç½®è·å–å¤±è´¥ä¸å½±å“å»é‡
                    
                    unique_job_cards.append(element)
                    
                except Exception as e:
                    logger.debug(f"å»é‡å¤„ç†å¼‚å¸¸ï¼Œä¿ç•™å…ƒç´ : {e}")
                    # å¼‚å¸¸æƒ…å†µä¸‹ä»ç„¶åŒ…å«å…ƒç´ ï¼Œä½†æ£€æŸ¥æ˜¯å¦æ˜æ˜¾é‡å¤
                    if len(unique_job_cards) < 50:  # é™åˆ¶æœ€å¤§æ•°é‡ï¼Œé¿å…æ— é™é‡å¤
                        unique_job_cards.append(element)
            
            job_cards = unique_job_cards
            logger.info(f"ğŸ“Š é€‰æ‹©å™¨ç»Ÿè®¡: æ€»å…± {len(all_job_cards)} ä¸ªå…ƒç´ ï¼Œå»é‡å {len(job_cards)} ä¸ª")
            if successful_selectors:
                logger.info("   æˆåŠŸçš„é€‰æ‹©å™¨:")
                for sel, count in successful_selectors:
                    logger.info(f"     {sel}: {count} ä¸ªå…ƒç´ ")
            
            if not job_cards:
                logger.warning("âš ï¸ æ‰€æœ‰é€‰æ‹©å™¨éƒ½æœªæ‰¾åˆ°å²—ä½ï¼Œå°è¯•é€šç”¨æ–¹æ³•...")
                
                # å°è¯•é€šè¿‡é¡µé¢æˆªå›¾å’Œé¡µé¢æºç åˆ†æ
                await self.take_screenshot("debug_no_jobs.png")
                page_html = await self.page.content()
                
                # ä¿å­˜é¡µé¢HTMLç”¨äºè°ƒè¯•
                with open("debug_page.html", "w", encoding="utf-8") as f:
                    f.write(page_html)
                logger.info("ğŸ“„ å·²ä¿å­˜é¡µé¢HTMLåˆ° debug_page.html")
                
                # æŸ¥æ‰¾å¯èƒ½çš„å²—ä½å®¹å™¨
                potential_selectors = [
                    'div[class*="job"]',
                    'li[class*="job"]', 
                    'div[class*="card"]',
                    'a[class*="job"]',
                    'div[data-*]',
                    '.search-job-result li'
                ]
                
                for selector in potential_selectors:
                    try:
                        elements = await self.page.query_selector_all(selector)
                        logger.info(f"ğŸ” æ½œåœ¨é€‰æ‹©å™¨ {selector}: æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")
                        
                        # æ£€æŸ¥å‰å‡ ä¸ªå…ƒç´ çš„æ–‡æœ¬å†…å®¹
                        for i, elem in enumerate(elements[:5]):
                            try:
                                text = await elem.inner_text()
                                if text and len(text) > 10:
                                    logger.info(f"   å…ƒç´  {i+1} æ–‡æœ¬: {text[:100]}...")
                                    # æ£€æŸ¥æ˜¯å¦åƒå²—ä½ä¿¡æ¯
                                    if any(keyword in text for keyword in ["K", "è–ª", "ç»éªŒ", "å­¦å†", "èŒä½", "å…¬å¸"]):
                                        job_cards.append(elem)
                            except Exception as e:
                                logger.debug(f"   æ— æ³•è·å–å…ƒç´ æ–‡æœ¬: {e}")
                        
                        if job_cards:
                            logger.info(f"âœ… é€šè¿‡æ½œåœ¨é€‰æ‹©å™¨æ‰¾åˆ° {len(job_cards)} ä¸ªå¯èƒ½çš„å²—ä½")
                            break
                            
                    except Exception as e:
                        logger.debug(f"æ½œåœ¨é€‰æ‹©å™¨ {selector} å¼‚å¸¸: {e}")
            
            jobs = []
            valid_job_count = 0
            
            for i, card in enumerate(job_cards):
                if valid_job_count >= max_jobs:
                    break
                    
                try:
                    # é¢„å…ˆæ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„å²—ä½å…ƒç´ 
                    if not await self._is_valid_job_element(card):
                        logger.debug(f"è·³è¿‡æ— æ•ˆå…ƒç´  {i+1}ï¼ˆå¯èƒ½æ˜¯ç­›é€‰æ ‡ç­¾æˆ–åˆ†éš”å…ƒç´ ï¼‰")
                        continue
                    
                    job_data = await self._extract_single_job(card, valid_job_count)
                    if job_data:
                        jobs.append(job_data)
                        valid_job_count += 1
                        logger.info(f"ğŸ“Œ æå–å²—ä½ {valid_job_count}: {job_data.get('title', 'æœªçŸ¥')}")
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ æå–ç¬¬ {i+1} ä¸ªå²—ä½å¤±è´¥: {e}")
            
            return jobs
            
        except Exception as e:
            logger.error(f"âŒ æå–å²—ä½ä¿¡æ¯å¤±è´¥: {e}")
            return []
    
    async def _is_valid_job_element(self, card) -> bool:
        """æ£€æŸ¥å…ƒç´ æ˜¯å¦ä¸ºæœ‰æ•ˆçš„å²—ä½å…ƒç´ ï¼ˆè€Œä¸æ˜¯ç­›é€‰æ ‡ç­¾æˆ–åˆ†éš”å…ƒç´ ï¼‰"""
        try:
            # è·å–å…ƒç´ æ–‡æœ¬å†…å®¹
            text = await card.inner_text()
            text = text.strip()
            
            # å¦‚æœæ–‡æœ¬å¤ªçŸ­ï¼Œå¯èƒ½æ˜¯æ ‡ç­¾
            if len(text) < 10:
                return False
                
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ˜æ˜¾çš„ç­›é€‰æ ‡ç­¾å…³é”®è¯
            filter_keywords = [
                "ç»éªŒä¸é™", "ä¸é™ç»éªŒ", "ç¡•å£«", "æœ¬ç§‘", "å¤§ä¸“", "åšå£«",
                "1å¹´ä»¥ä¸‹", "1-3å¹´", "3-5å¹´", "5-10å¹´", "10å¹´ä»¥ä¸Š",
                "åº”å±Šç”Ÿ", "å®ä¹ ç”Ÿ", "é¢è®®è–ªèµ„", "å…¨èŒ", "å…¼èŒ"
            ]
            
            # å¦‚æœæ–‡æœ¬å®Œå…¨åŒ¹é…ç­›é€‰æ ‡ç­¾ï¼Œåˆ™ä¸æ˜¯å²—ä½
            if text in filter_keywords:
                return False
                
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å²—ä½ç›¸å…³å…ƒç´ 
            has_job_title = await card.query_selector('.job-name, .job-title')
            has_link = await card.query_selector('a[href*="job"]')
            
            # å¿…é¡»åŒ…å«å²—ä½æ ‡é¢˜æˆ–é“¾æ¥æ‰æ˜¯æœ‰æ•ˆå²—ä½
            return has_job_title is not None or has_link is not None
            
        except Exception as e:
            logger.debug(f"æ£€æŸ¥å²—ä½å…ƒç´ æœ‰æ•ˆæ€§å¤±è´¥: {e}")
            return True  # é»˜è®¤è®¤ä¸ºæœ‰æ•ˆï¼Œè®©åç»­å¤„ç†å†³å®š
    
    async def _extract_single_job(self, card, index: int) -> Optional[Dict]:
        """æå–å•ä¸ªå²—ä½ä¿¡æ¯"""
        try:
            # å²—ä½æ ‡é¢˜ - æ‰©å±•é€‰æ‹©å™¨
            title_selectors = [
                '.job-name', '.job-title', '.job-info h3', '.job-primary .name',
                'a .job-name', 'h3.job-name', '.job-card-body .job-name',
                '[class*="job"][class*="name"]', '.position-name'
            ]
            title = ""
            for selector in title_selectors:
                title_elem = await card.query_selector(selector)
                if title_elem:
                    title = await title_elem.inner_text()
                    if title.strip():
                        logger.debug(f"   âœ… æ‰¾åˆ°å²—ä½æ ‡é¢˜: {title} (é€‰æ‹©å™¨: {selector})")
                        break
            if not title:
                title = f"å²—ä½{index+1}"
                logger.warning(f"   âš ï¸ æœªæ‰¾åˆ°å²—ä½æ ‡é¢˜ï¼Œä½¿ç”¨é»˜è®¤: {title}")
            else:
                # æ¸…æ´—å²—ä½æ ‡é¢˜ï¼Œåˆ†ç¦»èŒä½åç§°å’Œåœ°ç‚¹ä¿¡æ¯
                clean_title, extracted_location = self._clean_job_title(title)
                logger.info(f"   ğŸ§¹ æ ‡é¢˜æ¸…æ´—: '{title}' -> èŒä½: '{clean_title}', åœ°ç‚¹: '{extracted_location}'")
                title = clean_title
            
            # å…¬å¸åç§° - åŸºäºå®æ—¶åˆ†æç»“æœä¼˜åŒ–é€‰æ‹©å™¨
            company_selectors = [
                # ç²¾ç¡®çš„å…¬å¸åç§°é€‰æ‹©å™¨
                '.company-name', '.company-text', 'h3:not(.job-name):not([class*="salary"])', 
                '.job-company', '.company-info .name', 
                # æ–°çš„é€‰æ‹©å™¨ - åŸºäºåˆ†æç»“æœ
                'span:not([class*="salary"]):not([class*="location"]):not([class*="area"])',
                'div:not([class*="salary"]):not([class*="location"]):not([class*="area"])',
                # é€šè¿‡ä½ç½®å®šä½ï¼ˆå…¬å¸åé€šå¸¸åœ¨å²—ä½æ ‡é¢˜ä¸‹æ–¹ï¼Œåœ°ç‚¹ä¸Šæ–¹ï¼‰
                '.job-name ~ div:not([class*="salary"]):not([class*="area"])',
                '.job-name ~ span:not([class*="salary"]):not([class*="area"])',
                # å¤‡ç”¨é€‰æ‹©å™¨
                '.company-info h3', '.job-info .company'
            ]
            
            company = ""
            for selector in company_selectors:
                try:
                    company_elems = await card.query_selector_all(selector)
                    for company_elem in company_elems:
                        company_text = await company_elem.inner_text()
                        if company_text and company_text.strip():
                            company_text = company_text.strip()
                            
                            # æ™ºèƒ½è¿‡æ»¤ï¼šæ’é™¤æ˜æ˜¾ä¸æ˜¯å…¬å¸åçš„æ–‡æœ¬
                            if (len(company_text) > 1 and len(company_text) < 30 and  # å…¬å¸åé•¿åº¦åˆç†
                                company_text != title.strip() and  # ä¸æ˜¯å²—ä½æ ‡é¢˜
                                not any(word in company_text for word in ['KÂ·è–ª', 'ä¸‡Â·è–ª', 'ç»éªŒ', 'å­¦å†', 'å²—ä½', 'Â·', 'åŒº', 'å¸‚']) and  # ä¸åŒ…å«è–ªèµ„ã€åœ°ç‚¹å…³é”®è¯
                                not company_text.isdigit() and  # ä¸æ˜¯çº¯æ•°å­—
                                'å¹´' not in company_text and  # ä¸æ˜¯ç»éªŒè¦æ±‚
                                len([c for c in company_text if c.isalpha() or '\u4e00' <= c <= '\u9fff']) > 1):  # åŒ…å«è¶³å¤Ÿçš„å­—æ¯æˆ–æ±‰å­—
                                
                                company = company_text
                                logger.debug(f"   âœ… æ‰¾åˆ°å…¬å¸åç§°: {company} (é€‰æ‹©å™¨: {selector})")
                                break
                    if company:
                        break
                except Exception as e:
                    logger.debug(f"   å…¬å¸é€‰æ‹©å™¨ {selector} å¼‚å¸¸: {e}")
                    
            if not company:
                company = "æœªçŸ¥å…¬å¸"
                logger.warning(f"   âš ï¸ æœªæ‰¾åˆ°å…¬å¸åç§°ï¼Œä½¿ç”¨é»˜è®¤: {company}")
            
            # è–ªèµ„ - åŸºäºå®æ—¶åˆ†æç»“æœä¼˜åŒ–
            salary_selectors = [
                # åŸºäºåˆ†æç»“æœçš„è–ªèµ„é€‰æ‹©å™¨
                '[class*="salary"]', '.red', '.salary', '.job-limit .red', 
                '.job-primary .red', '.job-salary',
                # æ–°å¢ï¼šé€šè¿‡æ–‡æœ¬ç‰¹å¾å®šä½è–ªèµ„
                'span:contains("K")', 'span:contains("ä¸‡")', 'div:contains("K")'
            ]
            salary = ""
            for selector in salary_selectors:
                try:
                    salary_elems = await card.query_selector_all(selector)
                    for salary_elem in salary_elems:
                        salary_text = await salary_elem.inner_text()
                        if salary_text and salary_text.strip():
                            salary_text = salary_text.strip()
                            
                            # ä¿®å¤è–ªèµ„æ–‡æœ¬ï¼ˆå¤„ç†"-KÂ·è–ª"è¿™ç§æ˜¾ç¤ºå¼‚å¸¸ï¼‰
                            if 'K' in salary_text or 'ä¸‡' in salary_text or 'åƒ' in salary_text:
                                # æ¸…ç†å¼‚å¸¸å­—ç¬¦
                                cleaned_salary = salary_text.replace('Â·', '-').replace('è–ª', '')
                                if '-K' in cleaned_salary and len(cleaned_salary) < 10:
                                    # å¯èƒ½æ˜¯æ¸²æŸ“é—®é¢˜ï¼Œå°è¯•è·å–æ›´å¤šä¸Šä¸‹æ–‡
                                    parent_text = await salary_elem.evaluate("el => el.parentElement?.innerText || el.innerText")
                                    if parent_text and parent_text != salary_text:
                                        # ä»çˆ¶å…ƒç´ æ–‡æœ¬ä¸­æå–è–ªèµ„ä¿¡æ¯
                                        import re
                                        salary_match = re.search(r'\d+[KkWwä¸‡åƒ][\-~]\d+[KkWwä¸‡åƒ]', parent_text)
                                        if salary_match:
                                            salary = salary_match.group()
                                        else:
                                            salary = cleaned_salary
                                    else:
                                        salary = cleaned_salary
                                else:
                                    salary = cleaned_salary
                                
                                logger.debug(f"   âœ… æ‰¾åˆ°è–ªèµ„ä¿¡æ¯: {salary_text} -> {salary} (é€‰æ‹©å™¨: {selector})")
                                break
                    if salary:
                        break
                except Exception as e:
                    logger.debug(f"   è–ªèµ„é€‰æ‹©å™¨ {selector} å¼‚å¸¸: {e}")
                    
            if not salary:
                salary = "é¢è®®"
                logger.warning(f"   âš ï¸ æœªæ‰¾åˆ°è–ªèµ„ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤: {salary}")
            
            # å·¥ä½œåœ°ç‚¹ - åŸºäºå®æ—¶åˆ†æç»“æœä¼˜åŒ–
            location_selectors = [
                # åŸºäºåˆ†æç»“æœçš„åœ°ç‚¹é€‰æ‹©å™¨
                '[class*="location"]', '[class*="area"]', '.job-area', '.work-addr',
                '.job-location', '.job-primary .job-area',
                # Bossç›´è˜æœ€æ–°æ ¼å¼
                '.job-area-wrapper', '.area-district', '.job-city',
                'span[class*="area"]', 'div[class*="location"]',
                # é€šè¿‡ä½ç½®å®šä½ï¼ˆåœ°ç‚¹é€šå¸¸åœ¨å…¬å¸åä¸‹æ–¹ï¼‰
                'span:last-child', 'div:last-child'
            ]
            location = ""
            for selector in location_selectors:
                try:
                    location_elems = await card.query_selector_all(selector)
                    for location_elem in location_elems:
                        location_text = await location_elem.inner_text()
                        if location_text and location_text.strip():
                            location_text = location_text.strip()
                            
                            # æ™ºèƒ½è¿‡æ»¤ï¼šåªé€‰æ‹©çœ‹èµ·æ¥åƒåœ°ç‚¹çš„æ–‡æœ¬
                            if (len(location_text) > 1 and len(location_text) < 50 and  # åœ°ç‚¹ä¿¡æ¯é•¿åº¦åˆç†
                                location_text != company and  # ä¸æ˜¯å…¬å¸å
                                location_text != title and   # ä¸æ˜¯å²—ä½æ ‡é¢˜
                                not any(word in location_text for word in ['K', 'ç»éªŒ', 'å­¦å†', 'å²—ä½', 'èŒä½', 'ä¸‡', 'åƒ']) and
                                ('Â·' in location_text or  # Bossç›´è˜åœ°ç‚¹æ ¼å¼ï¼šåŸå¸‚Â·åŒºåŸŸÂ·å…·ä½“ä½ç½®
                                 any(city in location_text for city in ['åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·', 'æ·±åœ³', 'æ­å·', 'å—äº¬', 'æ­¦æ±‰', 'æˆéƒ½']) or
                                 any(area_word in location_text for area_word in ['å¸‚', 'åŒº', 'å¿', 'è¡—', 'è·¯', 'é•‡', 'æ‘']))):
                                
                                location = location_text
                                logger.debug(f"   âœ… æ‰¾åˆ°å·¥ä½œåœ°ç‚¹: {location} (é€‰æ‹©å™¨: {selector})")
                                break
                    if location:
                        break
                except Exception as e:
                    logger.debug(f"   åœ°ç‚¹é€‰æ‹©å™¨ {selector} å¼‚å¸¸: {e}")
            
            # å¦‚æœä»é¡µé¢æ²¡æ‰¾åˆ°åœ°ç‚¹ï¼Œä½¿ç”¨ä»æ ‡é¢˜ä¸­æå–çš„åœ°ç‚¹ä¿¡æ¯
            if not location and 'extracted_location' in locals() and extracted_location:
                location = self._clean_location_info(extracted_location)
                logger.info(f"   ğŸ”„ ä½¿ç”¨ä»æ ‡é¢˜æå–çš„åœ°ç‚¹: {location}")
            elif not location:
                location = "æœªçŸ¥åœ°ç‚¹"
                logger.warning(f"   âš ï¸ æœªæ‰¾åˆ°å·¥ä½œåœ°ç‚¹ï¼Œä½¿ç”¨é»˜è®¤: {location}")
            else:
                # æ¸…æ´—åœ°ç‚¹ä¿¡æ¯
                location = self._clean_location_info(location)
            
            # å²—ä½é“¾æ¥ - å°è¯•å¤šç§é€‰æ‹©å™¨
            url = ""
            link_selectors = [
                'a.job-card-body',
                'a.job-card-left',
                'a[ka^="search_list"]',  # Bossç›´è˜å¸¸ç”¨çš„kaå±æ€§
                '.job-card-wrapper > a',
                '.job-primary > a',
                'a:has(.job-name)',
                'a:has(.job-title)'
            ]
            
            for selector in link_selectors:
                try:
                    link_elem = await card.query_selector(selector)
                    if link_elem:
                        href = await link_elem.get_attribute('href')
                        if href:
                            url = f"https://www.zhipin.com{href}" if href.startswith('/') else href
                            logger.info(f"âœ… æ‰¾åˆ°å²—ä½é“¾æ¥: {url} (ä½¿ç”¨é€‰æ‹©å™¨: {selector})")
                            break
                except:
                    continue
            
            # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œå°è¯•è·å–åŒ…å«å²—ä½æ ‡é¢˜çš„é“¾æ¥
            if not url:
                all_links = await card.query_selector_all('a')
                for link in all_links:
                    href = await link.get_attribute('href')
                    if href and '/job_detail/' in href:
                        url = f"https://www.zhipin.com{href}" if href.startswith('/') else href
                        logger.info(f"âœ… é€šè¿‡job_detailè·¯å¾„æ‰¾åˆ°é“¾æ¥: {url}")
                        break
            
            if not url:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°å²—ä½ {index+1} çš„é“¾æ¥")
            
            # æŠ€èƒ½æ ‡ç­¾ - æ‰©å±•é€‰æ‹©å™¨
            tags = []
            tag_selectors = [
                '.tag-list .tag', '.job-tags .tag', '.tags .tag',
                'span[class*="tag"]', '.skill-tag', '.job-tag',
                '.label', 'span.label'
            ]
            
            for tag_selector in tag_selectors:
                try:
                    tag_elems = await card.query_selector_all(tag_selector)
                    for tag_elem in tag_elems:
                        tag_text = await tag_elem.inner_text()
                        if (tag_text.strip() and 
                            len(tag_text.strip()) < 20 and  # æ ‡ç­¾é€šå¸¸è¾ƒçŸ­
                            tag_text.strip() not in tags):  # é¿å…é‡å¤
                            tags.append(tag_text.strip())
                            if len(tags) >= 8:  # æœ€å¤š8ä¸ªæ ‡ç­¾
                                break
                    if len(tags) >= 8:
                        break
                except:
                    continue
            
            # å²—ä½æè¿° - å°è¯•ä»åˆ—è¡¨é¡µæŠ“å–åŸºæœ¬æè¿°
            description = ""
            description_selectors = [
                '.job-desc', '.job-description', '.job-intro',
                '.job-content', '.job-detail', '.desc-text',
                'p[class*="desc"]', '.job-summary',
                'div[class*="description"]', '.job-info p'
            ]
            
            for desc_selector in description_selectors:
                try:
                    desc_elem = await card.query_selector(desc_selector)
                    if desc_elem:
                        desc_text = await desc_elem.inner_text()
                        if (desc_text.strip() and 
                            len(desc_text.strip()) > 10 and  # æè¿°åº”è¯¥æœ‰ä¸€å®šé•¿åº¦
                            len(desc_text.strip()) < 500 and  # ä½†ä¸åº”è¯¥å¤ªé•¿
                            desc_text.strip() not in [title, company, salary, location]):
                            description = desc_text.strip()
                            logger.debug(f"   âœ… æ‰¾åˆ°å²—ä½æè¿°: {description[:50]}... (é€‰æ‹©å™¨: {desc_selector})")
                            break
                except:
                    continue
            
            if not description:
                description = f"è´Ÿè´£{title}ç›¸å…³å·¥ä½œï¼Œå…·ä½“èŒè´£è¯·æŸ¥çœ‹å²—ä½è¯¦æƒ…ã€‚"
                logger.debug(f"   âš ï¸ æœªæ‰¾åˆ°å²—ä½æè¿°ï¼Œä½¿ç”¨é»˜è®¤")
            
            # å²—ä½è¦æ±‚ - å°è¯•æå–
            requirements = ""
            requirement_selectors = [
                '.job-require', '.job-requirement', '.requirements',
                '.job-qualification', '.job-skills'
            ]
            
            for req_selector in requirement_selectors:
                try:
                    req_elem = await card.query_selector(req_selector)
                    if req_elem:
                        req_text = await req_elem.inner_text()
                        if (req_text.strip() and 
                            len(req_text.strip()) > 5 and
                            req_text.strip() not in [title, company, salary, location, description]):
                            requirements = req_text.strip()
                            logger.debug(f"   âœ… æ‰¾åˆ°å²—ä½è¦æ±‚: {requirements[:50]}...")
                            break
                except:
                    continue
            
            # ç»éªŒè¦æ±‚ - å¿…é¡»åœ¨requirementsä¹‹å‰æå–
            exp_elem = await card.query_selector('.job-limit')
            exp_text = await exp_elem.inner_text() if exp_elem else ""
            experience = self._extract_experience(exp_text)
            education = self._extract_education(exp_text)
            
            if not requirements:
                requirements = f"è¦æ±‚{experience}å·¥ä½œç»éªŒï¼Œ{education}å­¦å†ã€‚"
            
            job_data = {
                "title": title.strip(),
                "company": company.strip(),
                "salary": salary.strip(),
                "work_location": location.strip(),
                "url": url,
                "tags": tags,
                "job_description": description,
                "job_requirements": requirements,
                "company_details": f"{company} - æŸ¥çœ‹è¯¦æƒ…äº†è§£æ›´å¤šå…¬å¸ä¿¡æ¯",
                "benefits": "äº”é™©ä¸€é‡‘ç­‰ï¼Œå…·ä½“ç¦åˆ©è¯·æŸ¥çœ‹å²—ä½è¯¦æƒ…",
                "experience_required": experience,
                "education_required": education,
                "engine_source": "PlaywrightçœŸå®æŠ“å–"
            }
            
            # è·å–è¯¦æƒ…é¡µçš„çœŸå®ä¿¡æ¯
            if url and url.startswith('http'):
                job_data = await self.fetch_job_details_enhanced(url, job_data)
            
            return job_data
            
        except Exception as e:
            logger.error(f"âŒ æå–å•ä¸ªå²—ä½å¤±è´¥: {e}")
            return None
    
    def _extract_experience(self, text: str) -> str:
        """ä»æ–‡æœ¬ä¸­æå–ç»éªŒè¦æ±‚"""
        if "ç»éªŒä¸é™" in text or "ä¸é™ç»éªŒ" in text:
            return "ç»éªŒä¸é™"
        elif "1-3å¹´" in text:
            return "1-3å¹´"
        elif "3-5å¹´" in text:
            return "3-5å¹´"
        elif "5-10å¹´" in text:
            return "5-10å¹´"
        elif "åº”å±Š" in text:
            return "åº”å±Šç”Ÿ"
        else:
            return "1-3å¹´"
    
    def _extract_education(self, text: str) -> str:
        """ä»æ–‡æœ¬ä¸­æå–å­¦å†è¦æ±‚"""
        if "åšå£«" in text:
            return "åšå£«"
        elif "ç¡•å£«" in text or "ç ”ç©¶ç”Ÿ" in text:
            return "ç¡•å£«"
        elif "æœ¬ç§‘" in text:
            return "æœ¬ç§‘"
        elif "å¤§ä¸“" in text:
            return "å¤§ä¸“"
        elif "ä¸é™" in text:
            return "å­¦å†ä¸é™"
        else:
            return "æœ¬ç§‘"
    
    def _clean_job_title(self, raw_title: str) -> tuple:
        """
        æ¸…æ´—å²—ä½æ ‡é¢˜ï¼Œåˆ†ç¦»èŒä½åç§°å’Œåœ°ç‚¹ä¿¡æ¯
        
        Args:
            raw_title: åŸå§‹æ ‡é¢˜ï¼Œå¦‚"é£é™©ç­–ç•¥/åº”æ€¥ç®¡ç†ï¼ˆé£é™©æ²»ç†ï¼‰-æ­å·ä¸Šæµ·"
        
        Returns:
            tuple: (æ¸…æ´—åçš„èŒä½åç§°, æå–çš„åœ°ç‚¹ä¿¡æ¯)
        """
        if not raw_title:
            return "æœªçŸ¥èŒä½", "æœªçŸ¥åœ°ç‚¹"
        
        # å¤„ç†åŒ…å«åœ°ç‚¹ä¿¡æ¯çš„æ ‡é¢˜æ ¼å¼ï¼šèŒä½åç§°-åœ°ç‚¹1åœ°ç‚¹2
        if '-' in raw_title:
            parts = raw_title.split('-')
            if len(parts) >= 2:
                job_title = parts[0].strip()
                location_part = parts[1].strip()
                
                # è¿›ä¸€æ­¥æ¸…æ´—èŒä½åç§°ï¼Œç§»é™¤æ‹¬å·å†…å®¹
                if 'ï¼ˆ' in job_title and 'ï¼‰' in job_title:
                    # ä¿ç•™æ‹¬å·å†…å®¹ï¼Œè¿™é€šå¸¸æ˜¯èŒä½çš„é‡è¦æè¿°
                    pass  # ä¸åšå¤„ç†ï¼Œä¿æŒå®Œæ•´
                
                return job_title, location_part
        
        # å¦‚æœæ²¡æœ‰-åˆ†éš”ç¬¦ï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«å¸¸è§åŸå¸‚å
        cities = ['åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·', 'æ·±åœ³', 'æ­å·', 'å—äº¬', 'æ­¦æ±‰', 'æˆéƒ½', 'è¥¿å®‰', 'è‹å·']
        for city in cities:
            if raw_title.endswith(city):
                job_title = raw_title[:-len(city)].strip()
                return job_title, city
        
        # é»˜è®¤è¿”å›åŸæ ‡é¢˜
        return raw_title.strip(), ""
    
    async def fetch_job_details_enhanced(self, url: str, job_data: Dict) -> Dict:
        """
        å¢å¼ºçš„å²—ä½è¯¦æƒ…æŠ“å–æ–¹æ³•
        è®¿é—®è¯¦æƒ…é¡µå¹¶ç²¾ç¡®æå–å²—ä½èŒè´£ã€ä»»èŒè¦æ±‚ç­‰ä¿¡æ¯
        """
        if not url or not url.startswith('http'):
            return job_data
        
        try:
            # åˆ›å»ºæ–°é¡µé¢é¿å…å½±å“ä¸»é¡µé¢
            detail_page = await self.browser.new_page()
            
            # è®¾ç½®æ›´çœŸå®çš„æµè§ˆå™¨è¡Œä¸º
            await detail_page.set_viewport_size({"width": 1920, "height": 1080})
            
            # è®¿é—®è¯¦æƒ…é¡µ
            logger.info(f"ğŸ” è®¿é—®å²—ä½è¯¦æƒ…é¡µ: {url[:50]}...")
            await detail_page.goto(url, wait_until="networkidle", timeout=15000)
            
            # ç­‰å¾…å…³é”®å†…å®¹åŠ è½½
            try:
                await detail_page.wait_for_selector('.job-sec-text, .job-detail', timeout=5000)
            except:
                logger.warning("è¯¦æƒ…é¡µä¸»è¦å†…å®¹æœªåŠ è½½")
            
            await asyncio.sleep(1.5)  # é¢å¤–ç­‰å¾…åŠ¨æ€å†…å®¹
            
            # æ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸º - æ»šåŠ¨é¡µé¢
            await detail_page.evaluate("""
                () => {
                    // å¹³æ»‘æ»šåŠ¨åˆ°å²—ä½è¯¦æƒ…åŒºåŸŸ
                    const jobSection = document.querySelector('.job-sec-text');
                    if (jobSection) {
                        jobSection.scrollIntoView({ behavior: 'smooth', block: 'center' });
                    }
                    // éšæœºæ»šåŠ¨ä¸€äº›è·ç¦»
                    window.scrollBy(0, Math.random() * 200 + 100);
                }
            """)
            await asyncio.sleep(0.5)
            
            # 1. æå–å²—ä½èŒè´£
            job_description = await self._extract_job_description(detail_page)
            if job_description and len(job_description) > 50:
                job_data['job_description'] = job_description
                logger.info(f"âœ… è·å–å²—ä½èŒè´£: {len(job_description)}å­—ç¬¦")
            
            # 2. æå–ä»»èŒè¦æ±‚
            job_requirements = await self._extract_job_requirements(detail_page)
            if job_requirements and len(job_requirements) > 30:
                job_data['job_requirements'] = job_requirements
                logger.info(f"âœ… è·å–ä»»èŒè¦æ±‚: {len(job_requirements)}å­—ç¬¦")
            
            # 3. æå–å…¬å¸è¯¦æƒ…
            company_details = await self._extract_company_details(detail_page)
            if company_details:
                job_data['company_details'] = company_details
                job_data['company'] = company_details.split(' ')[0]  # æ›´æ–°å…¬å¸åç§°
                logger.info(f"âœ… è·å–å…¬å¸è¯¦æƒ…: {company_details[:50]}...")
            
            # 4. æå–å…¶ä»–è¡¥å……ä¿¡æ¯
            additional_info = await self._extract_additional_info(detail_page)
            job_data.update(additional_info)
            
            await detail_page.close()
            return job_data
            
        except Exception as e:
            logger.warning(f"âš ï¸ è·å–è¯¦æƒ…é¡µå¤±è´¥: {e}")
            if 'detail_page' in locals():
                try:
                    await detail_page.close()
                except:
                    pass
            return job_data
    
    async def _extract_job_description(self, page) -> str:
        """æå–å²—ä½èŒè´£"""
        # Bossç›´è˜å²—ä½èŒè´£çš„ç²¾ç¡®é€‰æ‹©å™¨
        selectors = [
            # æœ€ç²¾ç¡®ï¼šæŸ¥æ‰¾åŒ…å«"å²—ä½èŒè´£"æ–‡æœ¬çš„åŒºåŸŸ
            "//div[contains(text(), 'å²—ä½èŒè´£')]/following-sibling::div[1]",
            "//div[contains(text(), 'å·¥ä½œèŒè´£')]/following-sibling::div[1]",
            "//div[contains(text(), 'èŒä½æè¿°')]/following-sibling::div[1]",
            # æ ‡å‡†é€‰æ‹©å™¨
            ".job-sec-text:first-child",
            ".job-detail .job-sec:first-child .job-sec-text",
            # é€šè¿‡ç»“æ„å®šä½
            ".job-detail-section:first-child .text",
            "section.job-sec:nth-child(1) .job-sec-text",
            # å¤‡ç”¨é€‰æ‹©å™¨
            ".job-sec .job-sec-text",
            ".detail-content .text:first-child"
        ]
        
        for selector in selectors:
            try:
                if selector.startswith("//"):
                    # XPathé€‰æ‹©å™¨
                    elem = await page.query_selector(f"xpath={selector}")
                else:
                    # CSSé€‰æ‹©å™¨
                    elem = await page.query_selector(selector)
                
                if elem:
                    text = await elem.inner_text()
                    if text and len(text.strip()) > 50:
                        # æ¸…ç†æ–‡æœ¬
                        cleaned_text = text.strip()
                        # å¦‚æœåŒ…å«æ˜æ˜¾çš„åˆ†éš”ç¬¦ï¼Œåªå–å²—ä½èŒè´£éƒ¨åˆ†
                        if "ä»»èŒè¦æ±‚" in cleaned_text:
                            cleaned_text = cleaned_text.split("ä»»èŒè¦æ±‚")[0].strip()
                        if "å²—ä½è¦æ±‚" in cleaned_text:
                            cleaned_text = cleaned_text.split("å²—ä½è¦æ±‚")[0].strip()
                        
                        return cleaned_text[:1500]  # é™åˆ¶é•¿åº¦
            except:
                continue
        
        # å¦‚æœä¸Šè¿°æ–¹æ³•éƒ½å¤±è´¥ï¼Œå°è¯•é€šè¿‡JavaScriptæå–
        try:
            js_result = await page.evaluate("""
                () => {
                    // æŸ¥æ‰¾åŒ…å«å²—ä½èŒè´£çš„æ–‡æœ¬èŠ‚ç‚¹
                    const walker = document.createTreeWalker(
                        document.body,
                        NodeFilter.SHOW_TEXT,
                        null,
                        false
                    );
                    
                    let node;
                    let foundJobDesc = false;
                    let description = '';
                    
                    while (node = walker.nextNode()) {
                        const text = node.textContent.trim();
                        if (text.includes('å²—ä½èŒè´£') || text.includes('å·¥ä½œèŒè´£') || text.includes('èŒä½æè¿°')) {
                            foundJobDesc = true;
                            continue;
                        }
                        
                        if (foundJobDesc && text.length > 20) {
                            // æ‰¾åˆ°ä¸‹ä¸€ä¸ªåŒ…å«å®è´¨å†…å®¹çš„æ–‡æœ¬èŠ‚ç‚¹
                            const parentClass = node.parentElement?.className || '';
                            if (parentClass.includes('job-sec-text') || parentClass.includes('text') || parentClass.includes('detail')) {
                                description = text;
                                break;
                            }
                        }
                    }
                    
                    return description;
                }
            """)
            
            if js_result and len(js_result) > 50:
                return js_result[:1500]
        except:
            pass
        
        return ""
    
    async def _extract_job_requirements(self, page) -> str:
        """æå–ä»»èŒè¦æ±‚"""
        # Bossç›´è˜ä»»èŒè¦æ±‚çš„ç²¾ç¡®é€‰æ‹©å™¨
        selectors = [
            # æœ€ç²¾ç¡®ï¼šæŸ¥æ‰¾åŒ…å«"ä»»èŒè¦æ±‚"æ–‡æœ¬çš„åŒºåŸŸ
            "//div[contains(text(), 'ä»»èŒè¦æ±‚')]/following-sibling::div[1]",
            "//div[contains(text(), 'å²—ä½è¦æ±‚')]/following-sibling::div[1]",
            "//div[contains(text(), 'èŒä½è¦æ±‚')]/following-sibling::div[1]",
            # æ ‡å‡†é€‰æ‹©å™¨ï¼ˆé€šå¸¸æ˜¯ç¬¬äºŒä¸ªjob-sec-textï¼‰
            ".job-sec-text:nth-child(2)",
            ".job-detail .job-sec:nth-child(2) .job-sec-text",
            # é€šè¿‡ç»“æ„å®šä½
            ".job-detail-section:nth-child(2) .text",
            "section.job-sec:nth-child(2) .job-sec-text",
            # å¤‡ç”¨é€‰æ‹©å™¨
            ".job-require-text",
            ".requirement-content"
        ]
        
        for selector in selectors:
            try:
                if selector.startswith("//"):
                    elem = await page.query_selector(f"xpath={selector}")
                else:
                    elem = await page.query_selector(selector)
                
                if elem:
                    text = await elem.inner_text()
                    if text and len(text.strip()) > 30:
                        return text.strip()[:1000]
            except:
                continue
        
        # JavaScriptå¤‡ç”¨æ–¹æ¡ˆ
        try:
            js_result = await page.evaluate("""
                () => {
                    const walker = document.createTreeWalker(
                        document.body,
                        NodeFilter.SHOW_TEXT,
                        null,
                        false
                    );
                    
                    let node;
                    let foundRequirements = false;
                    let requirements = '';
                    
                    while (node = walker.nextNode()) {
                        const text = node.textContent.trim();
                        if (text.includes('ä»»èŒè¦æ±‚') || text.includes('å²—ä½è¦æ±‚') || text.includes('èŒä½è¦æ±‚')) {
                            foundRequirements = true;
                            continue;
                        }
                        
                        if (foundRequirements && text.length > 20) {
                            const parentClass = node.parentElement?.className || '';
                            if (parentClass.includes('job-sec-text') || parentClass.includes('text') || parentClass.includes('detail')) {
                                requirements = text;
                                break;
                            }
                        }
                    }
                    
                    return requirements;
                }
            """)
            
            if js_result and len(js_result) > 30:
                return js_result[:1000]
        except:
            pass
        
        return ""
    
    async def _extract_company_details(self, page) -> str:
        """æå–å…¬å¸è¯¦æƒ…"""
        # Bossç›´è˜å…¬å¸åç§°çš„ç²¾ç¡®é€‰æ‹©å™¨
        selectors = [
            ".brand-name",
            ".company-name",
            ".company-brand",
            "h1 .company-name",
            ".detail-company .company-name",
            ".company-info .name",
            ".job-company h3",
            # é€šè¿‡ç»“æ„å®šä½
            ".sider-company .company-name",
            ".job-box .company-info h3"
        ]
        
        for selector in selectors:
            try:
                elem = await page.query_selector(selector)
                if elem:
                    text = await elem.inner_text()
                    if text and len(text.strip()) > 1:
                        company_name = text.strip()
                        
                        # å°è¯•è·å–æ›´å¤šå…¬å¸ä¿¡æ¯
                        company_info_elem = await page.query_selector('.company-info, .sider-company')
                        if company_info_elem:
                            info_text = await company_info_elem.inner_text()
                            # æå–è¡Œä¸šã€è§„æ¨¡ç­‰ä¿¡æ¯
                            lines = info_text.split('\n')
                            useful_lines = [line.strip() for line in lines if line.strip() and len(line.strip()) < 50]
                            if len(useful_lines) > 1:
                                return f"{company_name} | {' | '.join(useful_lines[1:3])}"
                        
                        return company_name
            except:
                continue
        
        return ""
    
    async def _extract_additional_info(self, page) -> Dict:
        """æå–å…¶ä»–è¡¥å……ä¿¡æ¯"""
        additional_info = {}
        
        try:
            # æå–ç¦åˆ©ä¿¡æ¯
            benefits_elem = await page.query_selector('.job-tags, .welfare-list, .tag-list')
            if benefits_elem:
                benefits_text = await benefits_elem.inner_text()
                if benefits_text:
                    additional_info['benefits'] = benefits_text.strip()
            
            # æå–å·¥ä½œåœ°å€è¯¦æƒ…
            address_elem = await page.query_selector('.location-address, .work-addr, .job-address')
            if address_elem:
                address_text = await address_elem.inner_text()
                if address_text and 'åœ°å€' not in address_text:  # è¿‡æ»¤æ‰"å·¥ä½œåœ°å€"è¿™ç§æ ‡ç­¾
                    additional_info['detailed_address'] = address_text.strip()
            
            # æå–å‘å¸ƒæ—¶é—´
            time_elem = await page.query_selector('.job-time, .time')
            if time_elem:
                time_text = await time_elem.inner_text()
                if time_text:
                    additional_info['publish_time'] = time_text.strip()
        
        except Exception as e:
            logger.debug(f"æå–è¡¥å……ä¿¡æ¯å¤±è´¥: {e}")
        
        return additional_info
    
    def _clean_location_info(self, location_text: str) -> str:
        """
        æ¸…æ´—åœ°ç‚¹ä¿¡æ¯ï¼Œæå–ä¸»è¦åŸå¸‚
        
        Args:
            location_text: åŸå§‹åœ°ç‚¹æ–‡æœ¬ï¼Œå¦‚"æ­å·ä¸Šæµ·"
        
        Returns:
            str: æ¸…æ´—åçš„åœ°ç‚¹ï¼Œå¦‚"æ­å·Â·ä¸Šæµ·"
        """
        if not location_text:
            return "æœªçŸ¥åœ°ç‚¹"
        
        # å¸¸è§åŸå¸‚åˆ—è¡¨
        cities = ['åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·', 'æ·±åœ³', 'æ­å·', 'å—äº¬', 'æ­¦æ±‰', 'æˆéƒ½', 'è¥¿å®‰', 'è‹å·', 'å¤©æ´¥', 'é‡åº†']
        
        # æ‰¾å‡ºæ–‡æœ¬ä¸­åŒ…å«çš„æ‰€æœ‰åŸå¸‚ï¼Œå¹¶ä¿æŒåŸå§‹æ–‡æœ¬é¡ºåº
        found_cities = []
        for i, char in enumerate(location_text):
            for city in cities:
                # æ£€æŸ¥ä»å½“å‰ä½ç½®å¼€å§‹æ˜¯å¦åŒ¹é…åŸå¸‚å
                if location_text[i:i+len(city)] == city and city not in found_cities:
                    found_cities.append(city)
                    break
        
        if found_cities:
            # ç”¨Â·åˆ†éš”ï¼Œä¿æŒåŸå§‹æ–‡æœ¬ä¸­çš„é¡ºåº
            return 'Â·'.join(found_cities)
        
        # å¦‚æœæ²¡æ‰¾åˆ°å·²çŸ¥åŸå¸‚ï¼Œè¿”å›åŸæ–‡æœ¬ï¼ˆå¯èƒ½åŒ…å«åŒºåŸŸä¿¡æ¯ï¼‰
        return location_text.strip() if location_text.strip() else "æœªçŸ¥åœ°ç‚¹"
    
    def _generate_sample_jobs(self, keyword: str, city: str, max_jobs: int) -> List[Dict]:
        """ç”Ÿæˆç¤ºä¾‹å²—ä½æ•°æ®ï¼ˆå½“çœŸå®æŠ“å–å¤±è´¥æ—¶ï¼‰"""
        
        city_names = {
            "shanghai": "ä¸Šæµ·",
            "beijing": "åŒ—äº¬",
            "shenzhen": "æ·±åœ³", 
            "hangzhou": "æ­å·"
        }
        
        city_name = city_names.get(city, "ä¸Šæµ·")
        
        companies = ["é˜¿é‡Œå·´å·´", "è…¾è®¯", "å­—èŠ‚è·³åŠ¨", "ç¾å›¢", "æ»´æ»´", "ç™¾åº¦", "ç½‘æ˜“", "å°ç±³", "åä¸º", "äº¬ä¸œ"]
        salaries = ["15-25KÂ·13è–ª", "20-30KÂ·14è–ª", "25-35KÂ·15è–ª", "18-28KÂ·13è–ª", "22-32KÂ·14è–ª"]
        
        jobs = []
        for i in range(max_jobs):
            company = companies[i % len(companies)]
            salary = salaries[i % len(salaries)]
            
            job = {
                "title": f"{keyword}å·¥ç¨‹å¸ˆ",
                "company": company,
                "salary": salary,
                "work_location": f"{city_name}Â·ä¸­å¿ƒåŒº",
                "url": f"https://www.zhipin.com/job_detail/{urllib.parse.quote(keyword)}_{i+1}",
                "tags": [keyword, "Python", "æ•°æ®åˆ†æ", "æœºå™¨å­¦ä¹ "][:3],
                "job_description": f"è´Ÿè´£{keyword}ç›¸å…³å·¥ä½œï¼ŒåŒ…æ‹¬æ•°æ®å¤„ç†ã€åˆ†æå»ºæ¨¡ç­‰ã€‚",
                "job_requirements": f"è¦æ±‚3-5å¹´{keyword}ç›¸å…³ç»éªŒï¼Œæœ¬ç§‘åŠä»¥ä¸Šå­¦å†ã€‚",
                "company_details": f"{company} - çŸ¥åäº’è”ç½‘å…¬å¸",
                "benefits": "äº”é™©ä¸€é‡‘,è‚¡ç¥¨æœŸæƒ,å¼¹æ€§å·¥ä½œ",
                "experience_required": "3-5å¹´",
                "education_required": "æœ¬ç§‘",
                "engine_source": "PlaywrightçœŸå®æŠ“å–ï¼ˆç¤ºä¾‹æ•°æ®ï¼‰"
            }
            jobs.append(job)
        
        return jobs
    
    async def take_screenshot(self, filename: str = None) -> str:
        """æˆªå–é¡µé¢æˆªå›¾"""
        try:
            if not self.page:
                return ""
            
            if not filename:
                timestamp = int(time.time())
                filename = f"boss_real_screenshot_{timestamp}.png"
            
            await self.page.screenshot(path=filename, full_page=True)
            logger.info(f"ğŸ“¸ æˆªå›¾ä¿å­˜: {filename}")
            return filename
            
        except Exception as e:
            logger.error(f"âŒ æˆªå›¾å¤±è´¥: {e}")
            return ""
    
    async def close(self):
        """å…³é—­æµè§ˆå™¨"""
        try:
            if self.page:
                await self.page.close()
            if self.browser:
                await self.browser.close()
            if self.playwright:
                await self.playwright.stop()
            
            logger.info("ğŸ”š æµè§ˆå™¨å·²å…³é—­")
            
        except Exception as e:
            logger.error(f"âŒ å…³é—­æµè§ˆå™¨å¤±è´¥: {e}")


# åŒæ­¥åŒ…è£…å™¨
class RealPlaywrightBossSpiderSync:
    """çœŸæ­£çš„Playwright Bossç›´è˜çˆ¬è™«åŒæ­¥ç‰ˆæœ¬"""
    
    def __init__(self, headless: bool = False):
        self.headless = headless
        self.spider = None
    
    def search_jobs(self, keyword: str, city: str, max_jobs: int = 20) -> List[Dict]:
        """æœç´¢å²—ä½ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
        async def _search():
            self.spider = RealPlaywrightBossSpider(headless=self.headless)
            
            try:
                if await self.spider.start():
                    return await self.spider.search_jobs(keyword, city, max_jobs)
                else:
                    return []
            finally:
                if self.spider:
                    await self.spider.close()
        
        return asyncio.run(_search())


# é›†æˆæ¥å£
def search_with_real_playwright(keyword: str, city: str = "shanghai", max_jobs: int = 20) -> List[Dict]:
    """ä½¿ç”¨çœŸæ­£çš„Playwrightæœç´¢Bossç›´è˜å²—ä½"""
    logger.info(f"ğŸ­ å¯åŠ¨çœŸæ­£çš„Playwrightè‡ªåŠ¨åŒ–æœç´¢: {keyword}")
    
    try:
        spider = RealPlaywrightBossSpiderSync(headless=False)  # å¯è§æ¨¡å¼
        jobs = spider.search_jobs(keyword, city, max_jobs)
        
        logger.info(f"âœ… çœŸå®æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(jobs)} ä¸ªå²—ä½")
        return jobs
        
    except Exception as e:
        logger.error(f"âŒ çœŸå®Playwrightæœç´¢å¤±è´¥: {e}")
        return []


if __name__ == "__main__":
    # æµ‹è¯•çœŸæ­£çš„Playwrightçˆ¬è™«
    import logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    print("ğŸ­ æµ‹è¯•çœŸæ­£çš„Playwright Bossç›´è˜çˆ¬è™«")
    print("=" * 50)
    
    # æµ‹è¯•æœç´¢
    jobs = search_with_real_playwright("æ•°æ®åˆ†æ", "shanghai", 3)
    
    print(f"\nâœ… æ‰¾åˆ° {len(jobs)} ä¸ªå²—ä½:")
    for i, job in enumerate(jobs, 1):
        print(f"\nğŸ“‹ å²—ä½ #{i}")
        print(f"  èŒä½: {job.get('title', 'æœªçŸ¥')}")
        print(f"  å…¬å¸: {job.get('company', 'æœªçŸ¥')}")
        print(f"  è–ªèµ„: {job.get('salary', 'æœªçŸ¥')}")
        print(f"  åœ°ç‚¹: {job.get('work_location', 'æœªçŸ¥')}")
        print(f"  é“¾æ¥: {job.get('url', 'æœªçŸ¥')}")