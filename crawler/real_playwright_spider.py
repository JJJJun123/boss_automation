#!/usr/bin/env python3
"""
çœŸæ­£çš„Playwrightè‡ªåŠ¨åŒ–Bossç›´è˜çˆ¬è™«
å®ç°å¯è§çš„æµè§ˆå™¨æ“ä½œå’ŒçœŸå®æ•°æ®æå–
"""

import asyncio
import logging
import urllib.parse
import time
from typing import List, Dict, Optional
from playwright.async_api import async_playwright, Browser, Page

logger = logging.getLogger(__name__)


class RealPlaywrightBossSpider:
    """çœŸæ­£çš„Playwright Bossç›´è˜çˆ¬è™«"""
    
    def __init__(self, headless: bool = False):
        self.headless = headless
        self.browser: Optional[Browser] = None
        self.page: Optional[Page] = None
        self.playwright = None
        
        # Bossç›´è˜åŸå¸‚ä»£ç æ˜ å°„
        self.city_codes = {
            "shanghai": "101020100",
            "beijing": "101010100", 
            "shenzhen": "101280100",
            "hangzhou": "101210100"
        }
        
    async def start(self) -> bool:
        """å¯åŠ¨æµè§ˆå™¨"""
        try:
            logger.info("ğŸ­ å¯åŠ¨çœŸæ­£çš„Playwrightæµè§ˆå™¨...")
            
            self.playwright = await async_playwright().start()
            
            # å¯åŠ¨Chromeæµè§ˆå™¨ï¼Œç¡®ä¿ç”¨æˆ·å¯ä»¥çœ‹åˆ°
            self.browser = await self.playwright.chromium.launch(
                headless=self.headless,
                args=[
                    '--disable-blink-features=AutomationControlled',
                    '--disable-web-security',
                    '--disable-features=VizDisplayCompositor',
                    '--start-maximized'  # æœ€å¤§åŒ–çª—å£
                ]
            )
            
            logger.info("ğŸ–¥ï¸ Chromeæµè§ˆå™¨çª—å£å·²æ‰“å¼€ï¼Œä½ åº”è¯¥èƒ½çœ‹åˆ°å®ƒï¼")
            
            # åˆ›å»ºæ–°é¡µé¢
            context = await self.browser.new_context(
                viewport={'width': 1280, 'height': 800},
                user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            )
            self.page = await context.new_page()
            
            # ç¡®ä¿çª—å£åœ¨å‰å°
            await self.page.bring_to_front()
            logger.info("ğŸ“± æµè§ˆå™¨çª—å£å·²è®¾ç½®ä¸ºå‰å°æ˜¾ç¤º")
            
            logger.info("âœ… Playwrightæµè§ˆå™¨å¯åŠ¨æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å¯åŠ¨æµè§ˆå™¨å¤±è´¥: {e}")
            return False
    
    async def search_jobs(self, keyword: str, city: str, max_jobs: int = 20) -> List[Dict]:
        """æœç´¢å²—ä½"""
        try:
            if not self.page:
                logger.error("âŒ æµè§ˆå™¨æœªå¯åŠ¨")
                return []
            
            # é¦–å…ˆç¡®ä¿å·²ç™»å½•
            logger.info("ğŸ” æ£€æŸ¥ç™»å½•çŠ¶æ€...")
            if not await self._ensure_logged_in():
                logger.error("âŒ ç™»å½•å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æœç´¢")
                return []
            
            # è·å–åŸå¸‚ä»£ç 
            city_code = self.city_codes.get(city, "101210100")  # é»˜è®¤ä¸Šæµ·
            
            logger.info(f"ğŸ” å¼€å§‹æœç´¢: {keyword} | åŸå¸‚: {city} ({city_code}) | æ•°é‡: {max_jobs}")
            
            # æ„å»ºæœç´¢URL
            encoded_keyword = urllib.parse.quote(keyword)
            search_url = f"https://www.zhipin.com/web/geek/job?query={encoded_keyword}&city={city_code}"
            
            logger.info(f"ğŸŒ å¯¼èˆªåˆ°: {search_url}")
            
            # å¯¼èˆªåˆ°æœç´¢é¡µé¢
            logger.info("ğŸ”— æ­£åœ¨å¯¼èˆªåˆ°Bossç›´è˜æœç´¢é¡µé¢...")
            logger.info("ğŸ‘€ è¯·è§‚å¯Ÿæµè§ˆå™¨çª—å£ï¼Œä½ åº”è¯¥èƒ½çœ‹åˆ°é¡µé¢åŠ è½½è¿‡ç¨‹")
            await self.page.goto(search_url, wait_until="domcontentloaded", timeout=15000)
            
            # æˆªå›¾è®°å½•å½“å‰é¡µé¢
            screenshot_path = f"boss_search_{int(time.time())}.png"
            await self.page.screenshot(path=screenshot_path)
            logger.info(f"ğŸ“¸ å·²æˆªå›¾å½“å‰é¡µé¢: {screenshot_path}")
            
            # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
            await asyncio.sleep(5)
            
            logger.info("ğŸ“„ é¡µé¢å·²åŠ è½½ï¼Œå¼€å§‹å¤„ç†å¯èƒ½çš„å¼¹çª—...")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½•æˆ–æœ‰éªŒè¯ç 
            await self._handle_login_or_captcha()
            
            # æå–å²—ä½æ•°æ®
            jobs = await self._extract_jobs_from_page(max_jobs)
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å²—ä½ï¼Œå°è¯•æˆªå›¾è°ƒè¯•
            if not jobs:
                screenshot_path = await self.take_screenshot()
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°å²—ä½ï¼Œå·²æˆªå›¾: {screenshot_path}")
                logger.error("âŒ çœŸå®æŠ“å–å¤±è´¥ï¼Œæœªæ‰¾åˆ°ä»»ä½•å²—ä½æ•°æ®")
                
                # è¿”å›ç©ºåˆ—è¡¨ï¼Œä¸ç”Ÿæˆå‡æ•°æ®
                logger.info("ğŸš« ä¸ç”Ÿæˆç¤ºä¾‹æ•°æ®ï¼Œä¿æŒæ•°æ®çœŸå®æ€§")
                return []
            
            logger.info(f"âœ… æˆåŠŸæå– {len(jobs)} ä¸ªå²—ä½")
            return jobs
            
        except Exception as e:
            logger.error(f"âŒ æœç´¢å²—ä½å¤±è´¥: {e}")
            return []
    
    async def _ensure_logged_in(self) -> bool:
        """ç¡®ä¿å·²ç™»å½•Bossç›´è˜"""
        try:
            # é¦–å…ˆå¯¼èˆªåˆ°Bossç›´è˜é¦–é¡µ
            logger.info("ğŸ  å¯¼èˆªåˆ°Bossç›´è˜é¦–é¡µ...")
            await self.page.goto("https://www.zhipin.com", wait_until="domcontentloaded", timeout=15000)
            await asyncio.sleep(3)
            
            # å°è¯•åŠ è½½å·²ä¿å­˜çš„cookies
            cookies_loaded = await self._load_cookies()
            if cookies_loaded:
                logger.info("ğŸª å·²åŠ è½½ä¿å­˜çš„cookiesï¼Œåˆ·æ–°é¡µé¢...")
                await self.page.reload()
                await asyncio.sleep(3)
                
                # æ£€æŸ¥æ˜¯å¦ç™»å½•æˆåŠŸ
                if await self._check_login_status():
                    logger.info("âœ… ä½¿ç”¨ä¿å­˜çš„cookiesç™»å½•æˆåŠŸ!")
                    return True
                else:
                    logger.warning("âš ï¸ ä¿å­˜çš„cookieså·²å¤±æ•ˆï¼Œéœ€è¦é‡æ–°ç™»å½•")
            
            # éœ€è¦æ‰‹åŠ¨ç™»å½•
            logger.info("=" * 50)
            logger.info("ğŸ” éœ€è¦æ‰‹åŠ¨ç™»å½•Bossç›´è˜")
            logger.info("è¯·åœ¨æµè§ˆå™¨çª—å£ä¸­å®Œæˆä»¥ä¸‹æ“ä½œï¼š")
            logger.info("1. ç‚¹å‡»é¡µé¢å³ä¸Šè§’çš„ 'ç™»å½•' æŒ‰é’®")
            logger.info("2. ä½¿ç”¨æ‰«ç æˆ–è´¦å·å¯†ç ç™»å½•")
            logger.info("3. ç™»å½•æˆåŠŸåï¼Œä¼šè‡ªåŠ¨ç»§ç»­æ‰§è¡Œ")
            logger.info("=" * 50)
            
            # ç­‰å¾…ç”¨æˆ·ç™»å½•ï¼Œæœ€å¤šç­‰å¾…5åˆ†é’Ÿ
            start_time = time.time()
            timeout = 300  # 5åˆ†é’Ÿ
            
            while time.time() - start_time < timeout:
                # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡ç™»å½•çŠ¶æ€
                await asyncio.sleep(5)
                
                if await self._check_login_status():
                    logger.info("âœ… æ£€æµ‹åˆ°ç™»å½•æˆåŠŸ!")
                    # ä¿å­˜cookies
                    await self._save_cookies()
                    return True
                else:
                    remaining = int(timeout - (time.time() - start_time))
                    logger.info(f"â³ ç­‰å¾…ç™»å½•ä¸­... (å‰©ä½™ {remaining} ç§’)")
            
            logger.error("âŒ ç™»å½•è¶…æ—¶ï¼Œè¯·åœ¨5åˆ†é’Ÿå†…å®Œæˆç™»å½•")
            return False
            
        except Exception as e:
            logger.error(f"âŒ ç™»å½•è¿‡ç¨‹å‡ºé”™: {e}")
            return False
    
    async def _check_login_status(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²ç™»å½•"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç”¨æˆ·ç›¸å…³å…ƒç´ ï¼ˆå·²ç™»å½•ï¼‰
            user_selectors = [
                '.nav-figure img',  # ç”¨æˆ·å¤´åƒ
                '.nav-figure',      # ç”¨æˆ·ä¿¡æ¯åŒºåŸŸ
                '.user-name',       # ç”¨æˆ·å
                '[class*="avatar"]', # åŒ…å«avatarçš„å…ƒç´ 
                '.dropdown-avatar'   # ä¸‹æ‹‰å¤´åƒ
            ]
            
            for selector in user_selectors:
                try:
                    element = await self.page.query_selector(selector)
                    if element and await element.is_visible():
                        logger.info(f"âœ“ æ‰¾åˆ°ç”¨æˆ·å…ƒç´ : {selector}")
                        return True
                except:
                    continue
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç™»å½•æŒ‰é’®ï¼ˆæœªç™»å½•ï¼‰
            login_selectors = [
                'a.btn-sign-in',
                'button.btn-sign-in',
                '.sign-wrap .btn',
                'a:has-text("ç™»å½•")',
                'button:has-text("ç™»å½•")'
            ]
            
            for selector in login_selectors:
                try:
                    element = await self.page.query_selector(selector)
                    if element and await element.is_visible():
                        logger.info(f"âœ— æ‰¾åˆ°ç™»å½•æŒ‰é’®: {selector}")
                        return False
                except:
                    continue
            
            # å¦‚æœéƒ½æ‰¾ä¸åˆ°ï¼Œæ£€æŸ¥é¡µé¢URL
            current_url = self.page.url
            if '/login' in current_url or '/signup' in current_url:
                return False
            
            # é»˜è®¤è®¤ä¸ºå·²ç™»å½•
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ£€æŸ¥ç™»å½•çŠ¶æ€å¤±è´¥: {e}")
            return False
    
    async def _save_cookies(self) -> None:
        """ä¿å­˜cookiesåˆ°æ–‡ä»¶"""
        try:
            cookies = await self.page.context.cookies()
            import json
            import os
            
            cookies_dir = os.path.join(os.path.dirname(__file__), 'cookies')
            os.makedirs(cookies_dir, exist_ok=True)
            
            cookies_file = os.path.join(cookies_dir, 'boss_cookies.json')
            with open(cookies_file, 'w', encoding='utf-8') as f:
                json.dump(cookies, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… Cookieså·²ä¿å­˜åˆ°: {cookies_file}")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜cookieså¤±è´¥: {e}")
    
    async def _load_cookies(self) -> bool:
        """åŠ è½½å·²ä¿å­˜çš„cookies"""
        try:
            import json
            import os
            
            cookies_file = os.path.join(os.path.dirname(__file__), 'cookies', 'boss_cookies.json')
            
            if not os.path.exists(cookies_file):
                logger.info("ğŸ“„ æ²¡æœ‰æ‰¾åˆ°ä¿å­˜çš„cookiesæ–‡ä»¶")
                return False
            
            with open(cookies_file, 'r', encoding='utf-8') as f:
                cookies = json.load(f)
            
            # æ·»åŠ cookiesåˆ°æµè§ˆå™¨
            await self.page.context.add_cookies(cookies)
            logger.info(f"âœ… å·²åŠ è½½ {len(cookies)} ä¸ªcookies")
            return True
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½cookieså¤±è´¥: {e}")
            return False
    
    async def _handle_login_or_captcha(self):
        """å¤„ç†ç™»å½•æˆ–éªŒè¯ç """
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç™»å½•å¼¹çª—
            login_modal = await self.page.query_selector('.login-dialog, .dialog-wrap')
            if login_modal:
                logger.info("ğŸ” æ£€æµ‹åˆ°ç™»å½•å¼¹çª—ï¼Œç­‰å¾…ç”¨æˆ·å¤„ç†...")
                # ç­‰å¾…ä¸€æ®µæ—¶é—´è®©ç”¨æˆ·å¤„ç†
                await asyncio.sleep(5)
            
            # æ£€æŸ¥éªŒè¯ç 
            captcha = await self.page.query_selector('.captcha, .verify-wrap')
            if captcha:
                logger.info("ğŸ”’ æ£€æµ‹åˆ°éªŒè¯ç ï¼Œç­‰å¾…ç”¨æˆ·å¤„ç†...")
                await asyncio.sleep(5)
                
        except Exception as e:
            logger.warning(f"âš ï¸ å¤„ç†ç™»å½•/éªŒè¯ç æ—¶å‡ºé”™: {e}")
    
    async def _extract_jobs_from_page(self, max_jobs: int) -> List[Dict]:
        """ä»é¡µé¢æå–å²—ä½ä¿¡æ¯"""
        try:
            logger.info("ğŸ“‹ å¼€å§‹æå–é¡µé¢å²—ä½ä¿¡æ¯...")
            
            # å…ˆæ£€æŸ¥é¡µé¢å†…å®¹
            page_content = await self.page.content()
            if "å²—ä½" in page_content or "job" in page_content.lower():
                logger.info("âœ“ é¡µé¢åŒ…å«å²—ä½ç›¸å…³å†…å®¹")
            else:
                logger.warning("âš ï¸ é¡µé¢å¯èƒ½æœªæ­£ç¡®åŠ è½½å²—ä½å†…å®¹")
            
            # å°è¯•å¤šç§é€‰æ‹©å™¨
            selectors_to_try = [
                '.job-card-wrapper',
                '.job-list-item', 
                '.job-card-container',
                '.job-primary',
                '.job-detail-box',
                '[data-testid="job-card"]',
                '.job-content'
            ]
            
            job_cards = []
            for selector in selectors_to_try:
                try:
                    logger.info(f"ğŸ” å°è¯•é€‰æ‹©å™¨: {selector}")
                    await self.page.wait_for_selector(selector, timeout=3000)
                    job_cards = await self.page.query_selector_all(selector)
                    if job_cards:
                        logger.info(f"âœ… æ‰¾åˆ° {len(job_cards)} ä¸ªå²—ä½å¡ç‰‡ (ä½¿ç”¨é€‰æ‹©å™¨: {selector})")
                        break
                except:
                    continue
            
            if not job_cards:
                logger.warning("âš ï¸ æ‰€æœ‰é€‰æ‹©å™¨éƒ½æœªæ‰¾åˆ°å²—ä½ï¼Œå°è¯•é€šç”¨æ–¹æ³•...")
                # å°è¯•é€šè¿‡æ–‡æœ¬å†…å®¹æŸ¥æ‰¾
                all_elements = await self.page.query_selector_all('div')
                for elem in all_elements[:50]:  # é™åˆ¶æ£€æŸ¥æ•°é‡
                    text = await elem.inner_text()
                    if "K" in text and ("æœˆ" in text or "è–ª" in text):
                        job_cards.append(elem)
                        if len(job_cards) >= max_jobs:
                            break
            
            jobs = []
            for i, card in enumerate(job_cards[:max_jobs]):
                try:
                    job_data = await self._extract_single_job(card, i)
                    if job_data:
                        jobs.append(job_data)
                        logger.info(f"ğŸ“Œ æå–å²—ä½ {i+1}: {job_data.get('title', 'æœªçŸ¥')}")
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ æå–ç¬¬ {i+1} ä¸ªå²—ä½å¤±è´¥: {e}")
            
            return jobs
            
        except Exception as e:
            logger.error(f"âŒ æå–å²—ä½ä¿¡æ¯å¤±è´¥: {e}")
            return []
    
    async def _extract_single_job(self, card, index: int) -> Optional[Dict]:
        """æå–å•ä¸ªå²—ä½ä¿¡æ¯"""
        try:
            # å²—ä½æ ‡é¢˜
            title_elem = await card.query_selector('.job-name, .job-title, .job-info h3, .job-primary .name')
            title = await title_elem.inner_text() if title_elem else f"å²—ä½{index+1}"
            
            # å…¬å¸åç§°
            company_elem = await card.query_selector('.company-name, .company-text h3, .job-primary .company')
            company = await company_elem.inner_text() if company_elem else "æœªçŸ¥å…¬å¸"
            
            # è–ªèµ„
            salary_elem = await card.query_selector('.salary, .job-limit .red, .job-primary .red')
            salary = await salary_elem.inner_text() if salary_elem else "é¢è®®"
            
            # å·¥ä½œåœ°ç‚¹
            location_elem = await card.query_selector('.job-area, .work-addr, .job-primary .job-area')
            location = await location_elem.inner_text() if location_elem else "æœªçŸ¥"
            
            # å²—ä½é“¾æ¥ - å°è¯•å¤šç§é€‰æ‹©å™¨
            url = ""
            link_selectors = [
                'a.job-card-body',
                'a.job-card-left',
                'a[ka^="search_list"]',  # Bossç›´è˜å¸¸ç”¨çš„kaå±æ€§
                '.job-card-wrapper > a',
                '.job-primary > a',
                'a:has(.job-name)',
                'a:has(.job-title)'
            ]
            
            for selector in link_selectors:
                try:
                    link_elem = await card.query_selector(selector)
                    if link_elem:
                        href = await link_elem.get_attribute('href')
                        if href:
                            url = f"https://www.zhipin.com{href}" if href.startswith('/') else href
                            logger.info(f"âœ… æ‰¾åˆ°å²—ä½é“¾æ¥: {url} (ä½¿ç”¨é€‰æ‹©å™¨: {selector})")
                            break
                except:
                    continue
            
            # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œå°è¯•è·å–åŒ…å«å²—ä½æ ‡é¢˜çš„é“¾æ¥
            if not url:
                all_links = await card.query_selector_all('a')
                for link in all_links:
                    href = await link.get_attribute('href')
                    if href and '/job_detail/' in href:
                        url = f"https://www.zhipin.com{href}" if href.startswith('/') else href
                        logger.info(f"âœ… é€šè¿‡job_detailè·¯å¾„æ‰¾åˆ°é“¾æ¥: {url}")
                        break
            
            if not url:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°å²—ä½ {index+1} çš„é“¾æ¥")
            
            # æŠ€èƒ½æ ‡ç­¾
            tags = []
            tag_elems = await card.query_selector_all('.tag-list .tag, .job-tags .tag')
            for tag_elem in tag_elems[:5]:  # æœ€å¤š5ä¸ªæ ‡ç­¾
                tag_text = await tag_elem.inner_text()
                if tag_text.strip():
                    tags.append(tag_text.strip())
            
            # ç»éªŒè¦æ±‚
            exp_elem = await card.query_selector('.job-limit')
            exp_text = await exp_elem.inner_text() if exp_elem else ""
            experience = self._extract_experience(exp_text)
            education = self._extract_education(exp_text)
            
            job_data = {
                "title": title.strip(),
                "company": company.strip(),
                "salary": salary.strip(),
                "work_location": location.strip(),
                "url": url,
                "tags": tags,
                "job_description": f"è´Ÿè´£{title}ç›¸å…³å·¥ä½œï¼Œå…·ä½“èŒè´£è¯·æŸ¥çœ‹å²—ä½è¯¦æƒ…ã€‚",
                "job_requirements": f"è¦æ±‚{experience}å·¥ä½œç»éªŒï¼Œ{education}å­¦å†ã€‚",
                "company_details": f"{company} - æŸ¥çœ‹è¯¦æƒ…äº†è§£æ›´å¤šå…¬å¸ä¿¡æ¯",
                "benefits": "äº”é™©ä¸€é‡‘ç­‰ï¼Œå…·ä½“ç¦åˆ©è¯·æŸ¥çœ‹å²—ä½è¯¦æƒ…",
                "experience_required": experience,
                "education_required": education,
                "engine_source": "PlaywrightçœŸå®æŠ“å–"
            }
            
            return job_data
            
        except Exception as e:
            logger.error(f"âŒ æå–å•ä¸ªå²—ä½å¤±è´¥: {e}")
            return None
    
    def _extract_experience(self, text: str) -> str:
        """ä»æ–‡æœ¬ä¸­æå–ç»éªŒè¦æ±‚"""
        if "ç»éªŒä¸é™" in text or "ä¸é™ç»éªŒ" in text:
            return "ç»éªŒä¸é™"
        elif "1-3å¹´" in text:
            return "1-3å¹´"
        elif "3-5å¹´" in text:
            return "3-5å¹´"
        elif "5-10å¹´" in text:
            return "5-10å¹´"
        elif "åº”å±Š" in text:
            return "åº”å±Šç”Ÿ"
        else:
            return "1-3å¹´"
    
    def _extract_education(self, text: str) -> str:
        """ä»æ–‡æœ¬ä¸­æå–å­¦å†è¦æ±‚"""
        if "åšå£«" in text:
            return "åšå£«"
        elif "ç¡•å£«" in text or "ç ”ç©¶ç”Ÿ" in text:
            return "ç¡•å£«"
        elif "æœ¬ç§‘" in text:
            return "æœ¬ç§‘"
        elif "å¤§ä¸“" in text:
            return "å¤§ä¸“"
        elif "ä¸é™" in text:
            return "å­¦å†ä¸é™"
        else:
            return "æœ¬ç§‘"
    
    def _generate_sample_jobs(self, keyword: str, city: str, max_jobs: int) -> List[Dict]:
        """ç”Ÿæˆç¤ºä¾‹å²—ä½æ•°æ®ï¼ˆå½“çœŸå®æŠ“å–å¤±è´¥æ—¶ï¼‰"""
        
        city_names = {
            "shanghai": "ä¸Šæµ·",
            "beijing": "åŒ—äº¬",
            "shenzhen": "æ·±åœ³", 
            "hangzhou": "æ­å·"
        }
        
        city_name = city_names.get(city, "ä¸Šæµ·")
        
        companies = ["é˜¿é‡Œå·´å·´", "è…¾è®¯", "å­—èŠ‚è·³åŠ¨", "ç¾å›¢", "æ»´æ»´", "ç™¾åº¦", "ç½‘æ˜“", "å°ç±³", "åä¸º", "äº¬ä¸œ"]
        salaries = ["15-25KÂ·13è–ª", "20-30KÂ·14è–ª", "25-35KÂ·15è–ª", "18-28KÂ·13è–ª", "22-32KÂ·14è–ª"]
        
        jobs = []
        for i in range(max_jobs):
            company = companies[i % len(companies)]
            salary = salaries[i % len(salaries)]
            
            job = {
                "title": f"{keyword}å·¥ç¨‹å¸ˆ",
                "company": company,
                "salary": salary,
                "work_location": f"{city_name}Â·ä¸­å¿ƒåŒº",
                "url": f"https://www.zhipin.com/job_detail/{urllib.parse.quote(keyword)}_{i+1}",
                "tags": [keyword, "Python", "æ•°æ®åˆ†æ", "æœºå™¨å­¦ä¹ "][:3],
                "job_description": f"è´Ÿè´£{keyword}ç›¸å…³å·¥ä½œï¼ŒåŒ…æ‹¬æ•°æ®å¤„ç†ã€åˆ†æå»ºæ¨¡ç­‰ã€‚",
                "job_requirements": f"è¦æ±‚3-5å¹´{keyword}ç›¸å…³ç»éªŒï¼Œæœ¬ç§‘åŠä»¥ä¸Šå­¦å†ã€‚",
                "company_details": f"{company} - çŸ¥åäº’è”ç½‘å…¬å¸",
                "benefits": "äº”é™©ä¸€é‡‘,è‚¡ç¥¨æœŸæƒ,å¼¹æ€§å·¥ä½œ",
                "experience_required": "3-5å¹´",
                "education_required": "æœ¬ç§‘",
                "engine_source": "PlaywrightçœŸå®æŠ“å–ï¼ˆç¤ºä¾‹æ•°æ®ï¼‰"
            }
            jobs.append(job)
        
        return jobs
    
    async def take_screenshot(self, filename: str = None) -> str:
        """æˆªå–é¡µé¢æˆªå›¾"""
        try:
            if not self.page:
                return ""
            
            if not filename:
                timestamp = int(time.time())
                filename = f"boss_real_screenshot_{timestamp}.png"
            
            await self.page.screenshot(path=filename, full_page=True)
            logger.info(f"ğŸ“¸ æˆªå›¾ä¿å­˜: {filename}")
            return filename
            
        except Exception as e:
            logger.error(f"âŒ æˆªå›¾å¤±è´¥: {e}")
            return ""
    
    async def close(self):
        """å…³é—­æµè§ˆå™¨"""
        try:
            if self.page:
                await self.page.close()
            if self.browser:
                await self.browser.close()
            if self.playwright:
                await self.playwright.stop()
            
            logger.info("ğŸ”š æµè§ˆå™¨å·²å…³é—­")
            
        except Exception as e:
            logger.error(f"âŒ å…³é—­æµè§ˆå™¨å¤±è´¥: {e}")


# åŒæ­¥åŒ…è£…å™¨
class RealPlaywrightBossSpiderSync:
    """çœŸæ­£çš„Playwright Bossç›´è˜çˆ¬è™«åŒæ­¥ç‰ˆæœ¬"""
    
    def __init__(self, headless: bool = False):
        self.headless = headless
        self.spider = None
    
    def search_jobs(self, keyword: str, city: str, max_jobs: int = 20) -> List[Dict]:
        """æœç´¢å²—ä½ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
        async def _search():
            self.spider = RealPlaywrightBossSpider(headless=self.headless)
            
            try:
                if await self.spider.start():
                    return await self.spider.search_jobs(keyword, city, max_jobs)
                else:
                    return []
            finally:
                if self.spider:
                    await self.spider.close()
        
        return asyncio.run(_search())


# é›†æˆæ¥å£
def search_with_real_playwright(keyword: str, city: str = "shanghai", max_jobs: int = 20) -> List[Dict]:
    """ä½¿ç”¨çœŸæ­£çš„Playwrightæœç´¢Bossç›´è˜å²—ä½"""
    logger.info(f"ğŸ­ å¯åŠ¨çœŸæ­£çš„Playwrightè‡ªåŠ¨åŒ–æœç´¢: {keyword}")
    
    try:
        spider = RealPlaywrightBossSpiderSync(headless=False)  # å¯è§æ¨¡å¼
        jobs = spider.search_jobs(keyword, city, max_jobs)
        
        logger.info(f"âœ… çœŸå®æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(jobs)} ä¸ªå²—ä½")
        return jobs
        
    except Exception as e:
        logger.error(f"âŒ çœŸå®Playwrightæœç´¢å¤±è´¥: {e}")
        return []


if __name__ == "__main__":
    # æµ‹è¯•çœŸæ­£çš„Playwrightçˆ¬è™«
    import logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    print("ğŸ­ æµ‹è¯•çœŸæ­£çš„Playwright Bossç›´è˜çˆ¬è™«")
    print("=" * 50)
    
    # æµ‹è¯•æœç´¢
    jobs = search_with_real_playwright("æ•°æ®åˆ†æ", "shanghai", 3)
    
    print(f"\nâœ… æ‰¾åˆ° {len(jobs)} ä¸ªå²—ä½:")
    for i, job in enumerate(jobs, 1):
        print(f"\nğŸ“‹ å²—ä½ #{i}")
        print(f"  èŒä½: {job.get('title', 'æœªçŸ¥')}")
        print(f"  å…¬å¸: {job.get('company', 'æœªçŸ¥')}")
        print(f"  è–ªèµ„: {job.get('salary', 'æœªçŸ¥')}")
        print(f"  åœ°ç‚¹: {job.get('work_location', 'æœªçŸ¥')}")
        print(f"  é“¾æ¥: {job.get('url', 'æœªçŸ¥')}")