#!/usr/bin/env python3
"""
çœŸæ­£çš„Playwrightè‡ªåŠ¨åŒ–Bossç›´è˜çˆ¬è™«
å®ç°å¯è§çš„æµè§ˆå™¨æ“ä½œå’ŒçœŸå®æ•°æ®æå–
"""

import asyncio
import logging
import urllib.parse
import time
import os
from pathlib import Path
from typing import List, Dict, Optional
from playwright.async_api import async_playwright, Browser, Page, BrowserContext
from .enhanced_extractor import EnhancedDataExtractor
from .session_manager import SessionManager
from .retry_handler import RetryHandler, RetryConfig, ErrorType, RetryStrategy, retry_on_error
from .large_scale_crawler import LargeScaleCrawler, LargeScaleProgressTracker

logger = logging.getLogger(__name__)


class RealPlaywrightBossSpider:
    """çœŸæ­£çš„Playwright Bossç›´è˜çˆ¬è™«"""
    
    def __init__(self, headless: bool = False):
        self.headless = headless
        self.browser: Optional[Browser] = None
        self.context: Optional[BrowserContext] = None
        self.page: Optional[Page] = None
        self.playwright = None
        self.enhanced_extractor = EnhancedDataExtractor()  # é›†æˆå¢å¼ºæå–å™¨
        self.session_manager = SessionManager()  # é›†æˆä¼šè¯ç®¡ç†å™¨
        self.retry_handler = RetryHandler()  # é›†æˆé‡è¯•å¤„ç†å™¨
        
        # åŠ è½½é…ç½®
        try:
            from config.config_manager import ConfigManager
            self.config_manager = ConfigManager()
            self.browser_config = self.config_manager.get_app_config('crawler', {}).get('browser', {})
        except:
            logger.warning("æ— æ³•åŠ è½½é…ç½®ç®¡ç†å™¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            self.config_manager = None
            self.browser_config = {}
        
        # Bossç›´è˜åŸå¸‚ä»£ç æ˜ å°„ (ä¸app_config.yamlä¿æŒä¸€è‡´)
        self.city_codes = {
            "shanghai": "101020100",   # ä¸Šæµ· (ä¿®å¤ï¼šä¹‹å‰é”™è¯¯ä¸º101210100)
            "beijing": "101010100",    # åŒ—äº¬ (æ­£ç¡®)
            "shenzhen": "101280600",   # æ·±åœ³ (æ­£ç¡®)
            "hangzhou": "101210100"    # æ­å· (ä¿®å¤ï¼šä¹‹å‰é”™è¯¯ä¸º101210300->å˜‰å…´)
        }
        
    @retry_on_error(max_attempts=3, base_delay=2.0, strategy=RetryStrategy.EXPONENTIAL_BACKOFF)
    async def start(self) -> bool:
        """å¯åŠ¨æµè§ˆå™¨ - ä½¿ç”¨æŒä¹…åŒ–ä¸Šä¸‹æ–‡ä¿æŒç™»å½•çŠ¶æ€"""
        logger.info("ğŸ­ å¯åŠ¨çœŸæ­£çš„Playwrightæµè§ˆå™¨...")
        
        self.playwright = await async_playwright().start()
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æŒä¹…åŒ–ä¸Šä¸‹æ–‡
        use_persistent = self.browser_config.get('use_persistent_context', True)
        user_data_dir = self.browser_config.get('user_data_dir', './browser_profile/boss_zhipin')
        
        if use_persistent:
            # åˆ›å»ºç”¨æˆ·æ•°æ®ç›®å½•
            user_data_path = Path(user_data_dir).absolute()
            user_data_path.mkdir(parents=True, exist_ok=True)
            
            logger.info(f"ğŸ“ ä½¿ç”¨æŒä¹…åŒ–æµè§ˆå™¨é…ç½®: {user_data_path}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯é¦–æ¬¡ä½¿ç”¨
            is_first_run = not (user_data_path / "Default").exists()
            if is_first_run:
                logger.info("ğŸ†• æ£€æµ‹åˆ°é¦–æ¬¡è¿è¡Œï¼Œå°†å¼•å¯¼æ‚¨è¿›è¡Œç™»å½•...")
                logger.info("ğŸ‘¤ è¯·åœ¨æ‰“å¼€çš„æµè§ˆå™¨çª—å£ä¸­æ‰‹åŠ¨ç™»å½•Bossç›´è˜")
                logger.info("âœ… ç™»å½•æˆåŠŸåï¼Œæ‚¨çš„ç™»å½•çŠ¶æ€å°†è¢«è‡ªåŠ¨ä¿å­˜")
            
            # ä½¿ç”¨æŒä¹…åŒ–ä¸Šä¸‹æ–‡å¯åŠ¨æµè§ˆå™¨
            logger.info(f"ğŸš€ æ­£åœ¨å¯åŠ¨æµè§ˆå™¨ï¼Œheadlessæ¨¡å¼: {self.headless}")
            self.context = await self.playwright.chromium.launch_persistent_context(
                user_data_dir=str(user_data_path),
                headless=self.headless,
                args=[
                    '--disable-blink-features=AutomationControlled',
                    '--disable-web-security',
                    '--disable-features=VizDisplayCompositor',
                    '--start-maximized'
                ],
                viewport={'width': 1280, 'height': 800},
                user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            )
            
            # è·å–æˆ–åˆ›å»ºé¡µé¢
            pages = self.context.pages
            self.page = pages[0] if pages else await self.context.new_page()
            logger.info(f"âœ… æµè§ˆå™¨å¯åŠ¨æˆåŠŸï¼headless={self.headless}, é¡µé¢æ•°: {len(pages)}")
            
        else:
            # ä¼ ç»Ÿæ–¹å¼å¯åŠ¨æµè§ˆå™¨
            self.browser = await self.playwright.chromium.launch(
                headless=self.headless,
                args=[
                    '--disable-blink-features=AutomationControlled',
                    '--disable-web-security',
                    '--disable-features=VizDisplayCompositor',
                    '--start-maximized'
                ]
            )
            
            # åˆ›å»ºæ–°ä¸Šä¸‹æ–‡
            self.context = await self.browser.new_context(
                viewport={'width': 1280, 'height': 800},
                user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            )
            self.page = await self.context.new_page()
        
        logger.info("ğŸ–¥ï¸ Chromeæµè§ˆå™¨çª—å£å·²æ‰“å¼€ï¼Œä½ åº”è¯¥èƒ½çœ‹åˆ°å®ƒï¼")
        
        # ç¡®ä¿çª—å£åœ¨å‰å°
        await self.page.bring_to_front()
        logger.info("ğŸ“± æµè§ˆå™¨çª—å£å·²è®¾ç½®ä¸ºå‰å°æ˜¾ç¤º")
        
        logger.info("âœ… Playwrightæµè§ˆå™¨å¯åŠ¨æˆåŠŸ")
        return True
    
    async def search_jobs(self, keyword: str, city: str, max_jobs: int = 20) -> List[Dict]:
        """æœç´¢å²—ä½ - å¸¦å®Œå–„çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶"""
        
        # ä½¿ç”¨é‡è¯•æœºåˆ¶æ‰§è¡Œæ ¸å¿ƒæœç´¢é€»è¾‘
        search_config = RetryConfig(
            max_attempts=3,
            base_delay=5.0,
            strategy=RetryStrategy.EXPONENTIAL_BACKOFF,
            allowed_error_types=[
                ErrorType.NETWORK_ERROR,
                ErrorType.TIMEOUT_ERROR, 
                ErrorType.PAGE_LOAD_ERROR,
                ErrorType.ELEMENT_NOT_FOUND
            ]
        )
        
        try:
            return await self.retry_handler.execute_with_retry(
                self._search_jobs_core,
                keyword, city, max_jobs,
                config=search_config,
                context={'operation': 'search_jobs', 'keyword': keyword, 'city': city}
            )
        except Exception as e:
            logger.error(f"âŒ æœç´¢å²—ä½æœ€ç»ˆå¤±è´¥: {e}")
            # è®°å½•è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ç”¨äºåˆ†æ
            await self._log_search_failure(keyword, city, e)
            return []
    
    async def _search_jobs_core(self, keyword: str, city: str, max_jobs: int) -> List[Dict]:
        """æ ¸å¿ƒæœç´¢é€»è¾‘ï¼ˆå†…éƒ¨æ–¹æ³•ï¼Œä¾›é‡è¯•ä½¿ç”¨ï¼‰"""
        if not self.page:
            raise RuntimeError("æµè§ˆå™¨æœªå¯åŠ¨")
        
        # é¦–å…ˆç¡®ä¿å·²ç™»å½•
        logger.info("ğŸ” æ£€æŸ¥ç™»å½•çŠ¶æ€...")
        if not await self._ensure_logged_in():
            raise RuntimeError("ç™»å½•å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æœç´¢")
        
        # è·å–åŸå¸‚ä»£ç 
        city_code = self.city_codes.get(city, "101210100")  # é»˜è®¤ä¸Šæµ·
        
        logger.info(f"ğŸ” å¼€å§‹æœç´¢: {keyword} | åŸå¸‚: {city} ({city_code}) | æ•°é‡: {max_jobs}")
        
        # ä½¿ç”¨æ›´è‡ªç„¶çš„æœç´¢æ–¹å¼
        logger.info(f"ğŸ” å‡†å¤‡æœç´¢: {keyword}")
        
        # ç¡®ä¿åœ¨é¦–é¡µï¼ˆç™»å½•åå¯èƒ½è¿˜åœ¨ç™»å½•é¡µæˆ–å…¶ä»–é¡µé¢ï¼‰
        logger.info("ğŸ  å¯¼èˆªåˆ°Bossç›´è˜é¦–é¡µ...")
        try:
            await self.page.goto("https://www.zhipin.com", wait_until="networkidle", timeout=30000)
            await asyncio.sleep(3)
        except Exception as e:
            logger.warning(f"é¦–é¡µåŠ è½½è¶…æ—¶ï¼Œå°è¯•ç»§ç»­: {e}")
        
        # ç›´æ¥ä½¿ç”¨URLå¯¼èˆªï¼ˆæ›´ç¨³å®šé«˜æ•ˆï¼‰
        logger.info("ğŸ” ä½¿ç”¨URLå¯¼èˆªè¿›è¡Œæœç´¢...")
        encoded_keyword = urllib.parse.quote(keyword)
        search_url = f"https://www.zhipin.com/web/geek/job?query={encoded_keyword}&city={city_code}"
        await self._navigate_to_search_page(search_url)
        
        # å¤„ç†é¡µé¢åŠ è½½å’Œé¢„å¤„ç†ï¼ˆä¼ é€’ç›®æ ‡å²—ä½æ•°é‡ï¼‰
        await self._prepare_search_page(max_jobs)
        
        # æ ¹æ®å²—ä½æ•°é‡é€‰æ‹©åˆé€‚çš„æŠ“å–ç­–ç•¥
        if max_jobs <= 30:
            # å°è§„æ¨¡æŠ“å–ï¼šä½¿ç”¨å¢å¼ºæå–å™¨
            logger.info("ğŸš€ å¯ç”¨å¢å¼ºæ•°æ®æå–å¼•æ“ï¼ˆå°è§„æ¨¡æ¨¡å¼ï¼‰...")
            jobs = await self.enhanced_extractor.extract_job_listings_enhanced(self.page, max_jobs)
        else:
            # å¤§è§„æ¨¡æŠ“å–ï¼šä½¿ç”¨å¤§è§„æ¨¡çˆ¬è™«å¼•æ“
            logger.info(f"ğŸ­ å¯ç”¨å¤§è§„æ¨¡æŠ“å–å¼•æ“ï¼ˆç›®æ ‡: {max_jobs} ä¸ªå²—ä½ï¼‰...")
            large_scale_crawler = LargeScaleCrawler(self.page, self.session_manager, self.retry_handler)
            jobs = await large_scale_crawler.extract_large_scale_jobs(max_jobs)
        
        # éªŒè¯ç»“æœ
        if not jobs:
            await self._handle_no_jobs_found()
            return []
        
        logger.info(f"âœ… æˆåŠŸæå– {len(jobs)} ä¸ªå²—ä½åŸºç¡€ä¿¡æ¯")
        
        # è·å–è¯¦æƒ…é¡µä¿¡æ¯
        logger.info("ğŸ“„ å¼€å§‹è·å–å²—ä½è¯¦æƒ…...")
        jobs_with_details = await self._fetch_job_details(jobs)
        
        logger.info(f"âœ… å®Œæˆè¯¦æƒ…è·å–ï¼Œå…± {len(jobs_with_details)} ä¸ªå²—ä½")
        return jobs_with_details
    
    @retry_on_error(max_attempts=3, base_delay=2.0)
    async def _navigate_to_search_page(self, search_url: str) -> None:
        """å¯¼èˆªåˆ°æœç´¢é¡µé¢"""
        logger.info("ğŸ”— æ­£åœ¨å¯¼èˆªåˆ°Bossç›´è˜æœç´¢é¡µé¢...")
        logger.info("ğŸ‘€ è¯·è§‚å¯Ÿæµè§ˆå™¨çª—å£ï¼Œä½ åº”è¯¥èƒ½çœ‹åˆ°é¡µé¢åŠ è½½è¿‡ç¨‹")
        
        await self.page.goto(search_url, wait_until="domcontentloaded", timeout=15000)
    
    async def _prepare_search_page(self, target_jobs: int = 20) -> None:
        """å‡†å¤‡æœç´¢é¡µé¢ï¼ˆé¡µé¢åŠ è½½ã€æ»šåŠ¨ç­‰ï¼‰
        
        Args:
            target_jobs: ç›®æ ‡å²—ä½æ•°é‡
        """
        # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½å®Œæˆ
        logger.info("â³ ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½...")
        
        # æ£€æŸ¥é¡µé¢æ˜¯å¦è¿˜åœ¨åŠ è½½çŠ¶æ€
        max_wait_time = 60  # æœ€å¤§ç­‰å¾…60ç§’
        wait_start = time.time()
        
        while time.time() - wait_start < max_wait_time:
            try:
                # æ£€æŸ¥é¡µé¢æ ‡é¢˜æ˜¯å¦è¿˜æ˜¯"è¯·ç¨å€™"
                title = await self.page.title()
                if title != "è¯·ç¨å€™":
                    logger.info(f"âœ… é¡µé¢åŠ è½½å®Œæˆï¼Œæ ‡é¢˜: {title}")
                    break
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å²—ä½å†…å®¹å‡ºç°
                job_indicators = await self.page.query_selector_all('li, .job-card, [data-jobid], .job-item')
                if job_indicators:
                    logger.info(f"âœ… æ£€æµ‹åˆ° {len(job_indicators)} ä¸ªæ½œåœ¨å²—ä½å…ƒç´ ")
                    break
                
                logger.info("â³ é¡µé¢ä»åœ¨åŠ è½½ä¸­ï¼Œç»§ç»­ç­‰å¾…...")
                await asyncio.sleep(3)
                
            except Exception as e:
                logger.debug(f"æ£€æŸ¥é¡µé¢çŠ¶æ€æ—¶å‡ºé”™: {e}")
                await asyncio.sleep(2)
        
        # é¢å¤–ç­‰å¾…ç¡®ä¿åŠ¨æ€å†…å®¹åŠ è½½
        await asyncio.sleep(5)
        
        # æ™ºèƒ½æ»šåŠ¨é¡µé¢ä»¥åŠ è½½æ›´å¤šå²—ä½
        logger.info(f"ğŸ“œ æ»šåŠ¨é¡µé¢ä»¥è§¦å‘æ›´å¤šå²—ä½åŠ è½½ï¼ˆç›®æ ‡: {target_jobs} ä¸ªï¼‰...")
        await self._smart_scroll_page(target_jobs)
        
        # æ»šåŠ¨å›é¡¶éƒ¨
        await self.page.evaluate("window.scrollTo(0, 0)")
        await asyncio.sleep(3)
        
        logger.info("ğŸ“„ é¡µé¢å·²å‡†å¤‡å®Œæˆï¼Œå¼€å§‹å¤„ç†å¯èƒ½çš„å¼¹çª—...")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç™»å½•æˆ–æœ‰éªŒè¯ç 
        await self._handle_login_or_captcha()
    
    async def _smart_scroll_page(self, target_jobs: int = 20) -> None:
        """æ™ºèƒ½æ»šåŠ¨é¡µé¢ç­–ç•¥ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
        
        Args:
            target_jobs: ç›®æ ‡å²—ä½æ•°é‡
        """
        try:
            # å…ˆæ£€æŸ¥é¡µé¢æ˜¯å¦ç¨³å®š
            await self._wait_for_page_stable()
            
            # è·å–å½“å‰å²—ä½æ•°é‡
            current_job_count = await self._count_current_jobs()
            logger.info(f"ğŸ“Š å½“å‰é¡µé¢å²—ä½æ•°: {current_job_count}ï¼Œç›®æ ‡: {target_jobs}")
            
            # å¦‚æœå·²ç»è¾¾åˆ°ç›®æ ‡æ•°é‡ï¼Œç›´æ¥è¿”å›
            if current_job_count >= target_jobs:
                logger.info(f"âœ… å·²è¾¾åˆ°ç›®æ ‡å²—ä½æ•°é‡")
                return
            
            # å®‰å…¨åœ°è·å–é¡µé¢é«˜åº¦
            initial_height = await self.page.evaluate("""
                () => {
                    return Math.max(
                        document.body?.scrollHeight || 0,
                        document.documentElement?.scrollHeight || 0,
                        window.innerHeight || 0
                    );
                }
            """)
            
            logger.info(f"ğŸ“œ å¼€å§‹æ™ºèƒ½æ»šåŠ¨ï¼Œåˆå§‹é«˜åº¦: {initial_height}")
            
            # æ ¹æ®éœ€è¦çš„å²—ä½æ•°é‡åŠ¨æ€è°ƒæ•´æ»šåŠ¨æ¬¡æ•°
            max_scroll_attempts = max(8, (target_jobs // 8) + 3)  # è‡³å°‘8æ¬¡å°è¯•ï¼Œæ¯8ä¸ªå²—ä½å¢åŠ 3æ¬¡
            no_change_count = 0  # è¿ç»­æ— å˜åŒ–è®¡æ•°
            
            for scroll_attempt in range(max_scroll_attempts):
                # æ£€æŸ¥æ˜¯å¦ä»åœ¨åŒä¸€é¡µé¢
                current_url = self.page.url
                
                # æ¸è¿›å¼æ»šåŠ¨ï¼Œé¿å…è§¦å‘é¡µé¢è·³è½¬
                scroll_steps = 3
                for step in range(scroll_steps):
                    await self.page.evaluate(f"""
                        () => {{
                            const targetY = window.scrollY + (window.innerHeight * 0.8);
                            window.scrollTo({{
                                top: targetY,
                                behavior: 'smooth'
                            }});
                        }}
                    """)
                    await asyncio.sleep(0.5)
                
                # ç¼“æ…¢æ»šåŠ¨åˆ°åº•éƒ¨ä»¥æ›´å¥½åœ°è§¦å‘æ‡’åŠ è½½
                await self.page.evaluate("""
                    () => {
                        const targetY = document.body.scrollHeight;
                        const currentY = window.scrollY;
                        const step = (targetY - currentY) / 3;
                        
                        // åˆ†3æ­¥æ»šåŠ¨åˆ°åº•éƒ¨
                        let steps = 0;
                        function smoothScroll() {
                            if (steps < 3) {
                                steps++;
                                window.scrollTo({
                                    top: currentY + (step * steps),
                                    behavior: 'smooth'
                                });
                                setTimeout(smoothScroll, 800);
                            }
                        }
                        smoothScroll();
                    }
                """)
                await asyncio.sleep(4)  # ç»™æ›´å¤šæ—¶é—´è®©å†…å®¹åŠ è½½
                
                # æ£€æŸ¥æ˜¯å¦å‘ç”Ÿäº†é¡µé¢è·³è½¬
                if self.page.url != current_url:
                    logger.warning("âš ï¸ æ£€æµ‹åˆ°é¡µé¢è·³è½¬ï¼Œåœæ­¢æ»šåŠ¨")
                    break
                
                # å®‰å…¨åœ°æ£€æŸ¥æ˜¯å¦æœ‰æ–°å†…å®¹åŠ è½½
                new_height = await self.page.evaluate("""
                    () => {
                        return Math.max(
                            document.body?.scrollHeight || 0,
                            document.documentElement?.scrollHeight || 0,
                            window.innerHeight || 0
                        );
                    }
                """)
                
                logger.info(f"   æ»šåŠ¨ {scroll_attempt + 1}/{max_scroll_attempts}ï¼Œé¡µé¢é«˜åº¦: {initial_height} -> {new_height}")
                
                # å¦‚æœé¡µé¢é«˜åº¦æ²¡æœ‰æ˜¾è‘—å˜åŒ–
                if abs(new_height - initial_height) < 100:
                    no_change_count += 1
                    logger.info(f"   é¡µé¢é«˜åº¦å˜åŒ–ä¸å¤§ (è¿ç»­{no_change_count}æ¬¡)")
                    
                    # æ£€æŸ¥å½“å‰å²—ä½æ•°é‡
                    current_job_count = await self._count_current_jobs()
                    logger.info(f"   å½“å‰å²—ä½æ•°: {current_job_count}/{target_jobs}")
                    
                    if current_job_count >= target_jobs:
                        logger.info(f"âœ… å·²è¾¾åˆ°ç›®æ ‡å²—ä½æ•°é‡")
                        break
                    
                    # å¦‚æœè¿ç»­3æ¬¡æ²¡æœ‰å˜åŒ–ä¸”å²—ä½æ•°é‡è¿˜ä¸å¤Ÿï¼Œå°è¯•å…¶ä»–ç­–ç•¥
                    if no_change_count >= 3:
                        if current_job_count < target_jobs:
                            logger.info("   å°è¯•æŸ¥æ‰¾åŠ è½½æ›´å¤šæŒ‰é’®æˆ–ç¿»é¡µ...")
                            # å°è¯•æŸ¥æ‰¾åŠ è½½æ›´å¤šæŒ‰é’®
                            try:
                                load_more_buttons = await self.page.query_selector_all(
                                    'button:has-text("åŠ è½½æ›´å¤š"), a:has-text("æŸ¥çœ‹æ›´å¤š"), '
                                    '.load-more, .more-btn, [class*="more"], [class*="load"]'
                                )
                                if load_more_buttons:
                                    for btn in load_more_buttons[:1]:  # åªç‚¹å‡»ç¬¬ä¸€ä¸ª
                                        if await btn.is_visible():
                                            await btn.click()
                                            logger.info("   ç‚¹å‡»äº†åŠ è½½æ›´å¤šæŒ‰é’®")
                                            await asyncio.sleep(3)
                                            no_change_count = 0
                                            break
                                else:
                                    # å°è¯•æŸ¥æ‰¾ä¸‹ä¸€é¡µæŒ‰é’®
                                    next_page = await self.page.query_selector(
                                        'a:has-text("ä¸‹ä¸€é¡µ"), .next-page, [class*="next"]'
                                    )
                                    if next_page and await next_page.is_visible():
                                        await next_page.click()
                                        logger.info("   ç‚¹å‡»äº†ä¸‹ä¸€é¡µæŒ‰é’®")
                                        await asyncio.sleep(5)
                                        no_change_count = 0
                                    else:
                                        logger.info("   æœªæ‰¾åˆ°åŠ è½½æ›´å¤šæˆ–ç¿»é¡µæŒ‰é’®ï¼Œå·²åˆ°è¾¾æœ€åä¸€é¡µ")
                                        break
                            except Exception as e:
                                logger.debug(f"å°è¯•åŠ è½½æ›´å¤šæ—¶å‡ºé”™: {e}")
                                break
                        else:
                            logger.info("   å·²åˆ°è¾¾é¡µé¢åº•éƒ¨")
                            break
                else:
                    no_change_count = 0  # é‡ç½®è®¡æ•°
                    initial_height = new_height
                    
                    # ç­‰å¾…æ–°å†…å®¹åŠ è½½
                    await asyncio.sleep(3)
                
        except Exception as e:
            if "Execution context was destroyed" in str(e):
                logger.info("âš ï¸ é¡µé¢å¯¼èˆªå¯¼è‡´æ»šåŠ¨ä¸­æ–­ï¼ˆæ­£å¸¸ç°è±¡ï¼‰")
            else:
                logger.warning(f"âš ï¸ æ™ºèƒ½æ»šåŠ¨å‡ºç°å¼‚å¸¸: {str(e)}")
            # ä¸å†å°è¯•é™çº§æ»šåŠ¨ï¼Œé¿å…è§¦å‘æ›´å¤šé”™è¯¯
    
    async def _count_current_jobs(self) -> int:
        """ç»Ÿè®¡å½“å‰é¡µé¢çš„å²—ä½æ•°é‡"""
        try:
            # ä½¿ç”¨å¤šä¸ªé€‰æ‹©å™¨æŸ¥æ‰¾å²—ä½å…ƒç´ ï¼Œå–æœ€å¤§å€¼
            selectors = [
                'li.job-card-wrapper',
                'li[data-jid]', 
                '.job-card-left',
                'li:has(a[href*="job_detail"])',
                'li[class*="job"]',
                'div[class*="job-card"]',
                '.job-list-item',  # æ·»åŠ æ›´å¤šå¯èƒ½çš„é€‰æ‹©å™¨
                '[data-jobid]',
                'a[ka*="search_list"]'
            ]
            
            max_count = 0
            counts = {}
            
            for selector in selectors:
                try:
                    jobs = await self.page.query_selector_all(selector)
                    count = len(jobs) if jobs else 0
                    counts[selector] = count
                    max_count = max(max_count, count)
                except Exception as e:
                    logger.debug(f"é€‰æ‹©å™¨ {selector} æŸ¥è¯¢å¤±è´¥: {e}")
                    continue
            
            # è®°å½•è¯¦ç»†çš„è®¡æ•°ä¿¡æ¯ç”¨äºè°ƒè¯•
            if max_count > 0:
                best_selector = max(counts, key=counts.get)
                logger.debug(f"å²—ä½è®¡æ•°è¯¦æƒ…: {counts}, æœ€ä½³é€‰æ‹©å™¨: {best_selector}")
            
            return max_count
        except Exception as e:
            logger.debug(f"ç»Ÿè®¡å²—ä½æ•°é‡å¤±è´¥: {e}")
            return 0
    
    async def _wait_for_page_stable(self) -> None:
        """ç­‰å¾…é¡µé¢ç¨³å®š"""
        try:
            # ç­‰å¾…ç½‘ç»œç©ºé—²
            await self.page.wait_for_load_state("networkidle", timeout=5000)
        except:
            # å¦‚æœç½‘ç»œä¸€ç›´ä¸ç©ºé—²ï¼Œè‡³å°‘ç­‰å¾…DOMåŠ è½½å®Œæˆ
            await self.page.wait_for_load_state("domcontentloaded", timeout=3000)
    
    async def _handle_no_jobs_found(self) -> None:
        """å¤„ç†æœªæ‰¾åˆ°å²—ä½çš„æƒ…å†µ"""
        screenshot_path = await self.take_screenshot()
        logger.warning(f"âš ï¸ æœªæ‰¾åˆ°å²—ä½ï¼Œå·²æˆªå›¾: {screenshot_path}")
        
        # æ£€æŸ¥é¡µé¢æ˜¯å¦æœ‰é”™è¯¯ä¿¡æ¯
        error_indicators = [
            '.empty-result', '.no-result', '.error-page', 
            ':has-text("æ²¡æœ‰æ‰¾åˆ°")', ':has-text("æš‚æ— æ•°æ®")'
        ]
        
        for selector in error_indicators:
            try:
                element = await self.page.query_selector(selector)
                if element and await element.is_visible():
                    error_text = await element.inner_text()
                    logger.warning(f"é¡µé¢æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯: {error_text}")
                    break
            except:
                continue
        
        logger.error("âŒ çœŸå®æŠ“å–å¤±è´¥ï¼Œæœªæ‰¾åˆ°ä»»ä½•å²—ä½æ•°æ®")
        logger.info("ğŸš« ä¸ç”Ÿæˆç¤ºä¾‹æ•°æ®ï¼Œä¿æŒæ•°æ®çœŸå®æ€§")
    
    async def _log_search_failure(self, keyword: str, city: str, exception: Exception) -> None:
        """è®°å½•æœç´¢å¤±è´¥çš„è¯¦ç»†ä¿¡æ¯"""
        try:
            failure_info = {
                'timestamp': time.time(),
                'keyword': keyword,
                'city': city,
                'error_type': type(exception).__name__,
                'error_message': str(exception),
                'page_url': self.page.url if self.page else 'unknown',
                'retry_stats': self.retry_handler.get_retry_stats()
            }
            
            # ä¿å­˜å¤±è´¥ä¿¡æ¯åˆ°æ–‡ä»¶
            import json
            failure_file = f"search_failure_{int(time.time())}.json"
            with open(failure_file, 'w', encoding='utf-8') as f:
                json.dump(failure_info, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ” æœç´¢å¤±è´¥è¯¦æƒ…å·²ä¿å­˜: {failure_file}")
            
        except Exception as e:
            logger.debug(f"è®°å½•æœç´¢å¤±è´¥ä¿¡æ¯æ—¶å‡ºé”™: {e}")
    
    async def _ensure_logged_in(self) -> bool:
        """ç¡®ä¿å·²ç™»å½•Bossç›´è˜ - æ”¯æŒæŒä¹…åŒ–ç™»å½•çŠ¶æ€"""
        try:
            # é¦–å…ˆå¯¼èˆªåˆ°Bossç›´è˜é¦–é¡µ
            logger.info("ğŸ  å¯¼èˆªåˆ°Bossç›´è˜é¦–é¡µ...")
            try:
                await self.page.goto("https://www.zhipin.com", wait_until="domcontentloaded", timeout=30000)
            except Exception as e:
                logger.warning(f"é¦–é¡µåŠ è½½è¶…æ—¶ï¼Œå°è¯•ç»§ç»­: {e}")
                # å³ä½¿è¶…æ—¶ä¹Ÿå°è¯•ç»§ç»­ï¼Œå› ä¸ºé¡µé¢å¯èƒ½å·²ç»éƒ¨åˆ†åŠ è½½
            await asyncio.sleep(3)
            
            # å¦‚æœä½¿ç”¨æŒä¹…åŒ–ä¸Šä¸‹æ–‡ï¼Œå…ˆæ£€æŸ¥æ˜¯å¦å·²ç»ç™»å½•
            use_persistent = self.browser_config.get('use_persistent_context', True)
            if use_persistent:
                # å®šä¹‰ç™»å½•çŠ¶æ€æ£€æŸ¥çš„é€‰æ‹©å™¨
                login_indicators = [
                    'a[href*="/web/geek/chat"]',  # èŠå¤©å…¥å£
                    '.nav-figure img',  # ç”¨æˆ·å¤´åƒ
                    'a[ka="header-username"]',  # ç”¨æˆ·åé“¾æ¥
                    '.header-login-name'  # ç™»å½•å
                ]
                
                # æ›´ä¸¥æ ¼çš„ç™»å½•çŠ¶æ€æ£€æŸ¥
                # å…ˆæ£€æŸ¥æ˜¯å¦æœ‰ç™»å½•æŒ‰é’®ï¼ˆå¦‚æœæœ‰è¯´æ˜æœªç™»å½•ï¼‰
                login_button = await self.page.query_selector('a[ka="header-login"], .btn-sign, .sign-in')
                if login_button:
                    logger.info("âŒ æ£€æµ‹åˆ°ç™»å½•æŒ‰é’®ï¼Œç”¨æˆ·æœªç™»å½•")
                else:
                    # æ£€æŸ¥ç™»å½•çŠ¶æ€çš„å¤šç§æ–¹å¼ï¼ˆæ›´ä¸¥æ ¼ï¼‰
                    for indicator in login_indicators:
                        try:
                            element = await self.page.query_selector(indicator)
                            if element:
                                logger.info(f"âœ… æ£€æµ‹åˆ°ç™»å½•æ ‡è¯†: {indicator}")
                                logger.info("âœ… ä½¿ç”¨æŒä¹…åŒ–ç™»å½•çŠ¶æ€ï¼Œæ— éœ€é‡æ–°ç™»å½•")
                                return True
                        except:
                            continue
                
                # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°ç™»å½•çŠ¶æ€ï¼Œå¼•å¯¼ç”¨æˆ·ç™»å½•
                logger.info("âŒ æœªæ£€æµ‹åˆ°ç™»å½•çŠ¶æ€")
                logger.info("ğŸ” è¯·æ‰‹åŠ¨ç™»å½•Bossç›´è˜...")
                logger.info("ğŸ‘‰ ç™»å½•æ­¥éª¤ï¼š")
                logger.info("   1. ç‚¹å‡»é¡µé¢å³ä¸Šè§’çš„'ç™»å½•'æŒ‰é’®")
                logger.info("   2. ä½¿ç”¨æ‰‹æœºå·éªŒè¯ç æˆ–æ‰«ç ç™»å½•")
                logger.info("   3. ç™»å½•æˆåŠŸåï¼Œåœ¨æ§åˆ¶å°æŒ‰Enterç»§ç»­")
                
                # ç­‰å¾…ç”¨æˆ·ç™»å½• - ä½¿ç”¨å¼‚æ­¥ç­‰å¾…è€Œéé˜»å¡è¾“å…¥
                logger.info("\nâ¸ï¸  è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆç™»å½•...")
                logger.info("ğŸ’¡ æç¤ºï¼šç™»å½•æˆåŠŸåï¼Œç¨‹åºä¼šè‡ªåŠ¨æ£€æµ‹å¹¶ç»§ç»­")
                
                # å¾ªç¯æ£€æµ‹ç™»å½•çŠ¶æ€ï¼Œæ¯5ç§’æ£€æŸ¥ä¸€æ¬¡
                max_wait_time = 300  # æœ€å¤šç­‰å¾…5åˆ†é’Ÿ
                check_interval = 5   # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡
                waited_time = 0
                
                while waited_time < max_wait_time:
                    await asyncio.sleep(check_interval)
                    waited_time += check_interval
                    
                    # æ£€æŸ¥æ˜¯å¦å·²ç™»å½•
                    for indicator in login_indicators:
                        try:
                            element = await self.page.query_selector(indicator)
                            if element:
                                logger.info(f"âœ… æ£€æµ‹åˆ°ç™»å½•æˆåŠŸï¼")
                                await asyncio.sleep(2)  # ç­‰å¾…é¡µé¢ç¨³å®š
                                return True
                        except:
                            continue
                    
                    # æ˜¾ç¤ºç­‰å¾…è¿›åº¦
                    remaining_time = max_wait_time - waited_time
                    logger.info(f"â³ ç­‰å¾…ç™»å½•ä¸­... (å‰©ä½™ {remaining_time} ç§’)")
                
                logger.error("âŒ ç™»å½•è¶…æ—¶ï¼Œè¯·é‡è¯•")
                return False
                
            else:
                # ä½¿ç”¨ä¼ ç»Ÿçš„ä¼šè¯ç®¡ç†æ–¹å¼
                # å°è¯•åŠ è½½å·²ä¿å­˜çš„ä¼šè¯
                if await self.session_manager.load_session(self.page.context, "zhipin.com"):
                    logger.info("ğŸª å·²åŠ è½½ä¿å­˜çš„ä¼šè¯ï¼Œåˆ·æ–°é¡µé¢...")
                    await self.page.reload()
                    await asyncio.sleep(3)
                    
                    # æ£€æŸ¥æ˜¯å¦ç™»å½•æˆåŠŸ
                    if await self.session_manager.check_login_status(self.page, "zhipin.com"):
                        logger.info("âœ… ä½¿ç”¨ä¿å­˜çš„ä¼šè¯ç™»å½•æˆåŠŸ!")
                        return True
                    else:
                        logger.warning("âš ï¸ ä¿å­˜çš„ä¼šè¯å·²å¤±æ•ˆï¼Œéœ€è¦é‡æ–°ç™»å½•")
                
                # ç­‰å¾…ç”¨æˆ·æ‰‹åŠ¨ç™»å½•
                if await self.session_manager.wait_for_login(self.page, timeout=300, domain="zhipin.com"):
                    # ä¿å­˜æ–°çš„ä¼šè¯
                    await self.session_manager.save_session(self.page.context, self.page, "zhipin.com")
                    return True
                else:
                    logger.error("âŒ ç™»å½•å¤±è´¥")
                    return False
            
        except Exception as e:
            logger.error(f"âŒ ç™»å½•è¿‡ç¨‹å‡ºé”™: {e}")
            return False
    
    def get_session_info(self) -> Dict:
        """è·å–å½“å‰ä¼šè¯ä¿¡æ¯"""
        return self.session_manager.get_session_info()
    
    async def _fetch_job_details(self, jobs: List[Dict]) -> List[Dict]:
        """è·å–å²—ä½è¯¦ç»†ä¿¡æ¯"""
        jobs_with_details = []
        
        for i, job in enumerate(jobs):
            try:
                logger.info(f"ğŸ“‹ è·å–ç¬¬ {i+1}/{len(jobs)} ä¸ªå²—ä½è¯¦æƒ…: {job.get('title', 'æœªçŸ¥å²—ä½')}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„URL
                job_url = job.get('url', '')
                if not job_url or not job_url.startswith('http'):
                    logger.warning(f"âš ï¸ å²—ä½ {i+1} æ²¡æœ‰æœ‰æ•ˆURLï¼Œè·³è¿‡è¯¦æƒ…è·å–")
                    jobs_with_details.append(job)
                    continue
                
                # è·å–è¯¦æƒ…é¡µæ•°æ®
                details = await self._extract_job_detail_page(job_url)
                
                # åˆå¹¶åŸºç¡€ä¿¡æ¯å’Œè¯¦æƒ…ä¿¡æ¯
                enhanced_job = {**job, **details}
                jobs_with_details.append(enhanced_job)
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                await asyncio.sleep(1)
                
            except Exception as e:
                logger.error(f"âŒ è·å–å²—ä½ {i+1} è¯¦æƒ…å¤±è´¥: {e}")
                # ä¿ç•™åŸå§‹æ•°æ®
                jobs_with_details.append(job)
                continue
        
        return jobs_with_details
    
    async def _extract_job_detail_page(self, job_url: str) -> Dict:
        """æå–å²—ä½è¯¦æƒ…é¡µä¿¡æ¯"""
        try:
            logger.debug(f"ğŸ”— è®¿é—®è¯¦æƒ…é¡µ: {job_url}")
            
            # å¯¼èˆªåˆ°è¯¦æƒ…é¡µ
            await self.page.goto(job_url, wait_until="domcontentloaded", timeout=30000)  # å¢åŠ åˆ°30ç§’
            await asyncio.sleep(2)
            
            # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
            await self._wait_for_detail_page_load()
            
            # æå–å·¥ä½œèŒè´£
            job_description = await self._extract_job_description()
            
            # æå–ä»»èŒèµ„æ ¼  
            job_requirements = await self._extract_job_requirements()
            
            # æå–å…¬å¸ä¿¡æ¯
            company_details = await self._extract_company_details()
            
            # æå–ç¦åˆ©å¾…é‡
            benefits = await self._extract_benefits()
            
            # æå–å®Œæ•´è–ªèµ„ä¿¡æ¯
            salary_info = await self._extract_salary_info()
            
            result = {
                'job_description': job_description,
                'job_requirements': job_requirements, 
                'company_details': company_details,
                'benefits': benefits,
                'detail_extraction_success': True
            }
            
            # å¦‚æœæå–åˆ°äº†æ›´å®Œæ•´çš„è–ªèµ„ä¿¡æ¯ï¼Œæ›´æ–°å®ƒ
            if salary_info and salary_info != "è–ªèµ„é¢è®®":
                result['salary'] = salary_info
                
            return result
            
        except Exception as e:
            logger.error(f"âŒ æå–è¯¦æƒ…é¡µå¤±è´¥: {e}")
            return {
                'job_description': 'è¯¦æƒ…é¡µåŠ è½½å¤±è´¥ï¼Œè¯·ç›´æ¥è®¿é—®å²—ä½é“¾æ¥æŸ¥çœ‹',
                'job_requirements': 'è¯¦æƒ…é¡µåŠ è½½å¤±è´¥ï¼Œè¯·ç›´æ¥è®¿é—®å²—ä½é“¾æ¥æŸ¥çœ‹',
                'company_details': 'è¯¦æƒ…é¡µåŠ è½½å¤±è´¥',
                'benefits': 'è¯¦æƒ…é¡µåŠ è½½å¤±è´¥',
                'detail_extraction_success': False
            }
    
    async def _wait_for_detail_page_load(self) -> None:
        """ç­‰å¾…è¯¦æƒ…é¡µåŠ è½½å®Œæˆ"""
        try:
            # ç­‰å¾…å…³é”®å…ƒç´ å‡ºç°
            key_selectors = [
                '.job-sec-text',  # å²—ä½æè¿°åŒºåŸŸ
                '.job-detail-section',  # è¯¦æƒ…åŒºåŸŸ
                '.job-primary',  # ä¸»è¦ä¿¡æ¯åŒºåŸŸ
                '.job-banner'  # æ¨ªå¹…åŒºåŸŸ
            ]
            
            # å°è¯•ç­‰å¾…ä»»æ„ä¸€ä¸ªå…³é”®é€‰æ‹©å™¨å‡ºç°
            for selector in key_selectors:
                try:
                    await self.page.wait_for_selector(selector, timeout=3000)
                    logger.debug(f"âœ… è¯¦æƒ…é¡µå…³é”®å…ƒç´ å·²åŠ è½½: {selector}")
                    break
                except:
                    continue
            
            # é¢å¤–ç­‰å¾…ç¡®ä¿åŠ¨æ€å†…å®¹åŠ è½½
            await asyncio.sleep(2)
            
        except Exception as e:
            logger.debug(f"ç­‰å¾…è¯¦æƒ…é¡µåŠ è½½æ—¶å‡ºé”™: {e}")
    
    async def _extract_job_description(self) -> str:
        """æå–å·¥ä½œèŒè´£"""
        selectors = [
            '.job-sec-text',  # Bossç›´è˜å¸¸ç”¨çš„èŒè´£æè¿°é€‰æ‹©å™¨
            '.job-detail-text .text',
            '.job-description .text-desc',
            '.job-detail .job-sec .text-desc',
            '[class*="job-sec"] .text',
            '.text-desc',
            '.job-content .text'
        ]
        
        for selector in selectors:
            try:
                elements = await self.page.query_selector_all(selector)
                if elements:
                    # è·å–æ‰€æœ‰åŒ¹é…å…ƒç´ çš„æ–‡æœ¬
                    texts = []
                    for element in elements:
                        text = await element.inner_text()
                        if text and text.strip():
                            texts.append(text.strip())
                    
                    if texts:
                        # æŸ¥æ‰¾åŒ…å«"èŒè´£"ã€"å·¥ä½œå†…å®¹"ç­‰å…³é”®è¯çš„éƒ¨åˆ†
                        for text in texts:
                            if any(keyword in text for keyword in ['èŒè´£', 'å·¥ä½œå†…å®¹', 'å²—ä½èŒè´£', 'ä¸»è¦å·¥ä½œ']):
                                logger.debug(f"âœ… æ‰¾åˆ°å·¥ä½œèŒè´£: {selector}")
                                return text
                        
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šå…³é”®è¯ï¼Œè¿”å›ç¬¬ä¸€ä¸ªè¾ƒé•¿çš„æ–‡æœ¬
                        for text in texts:
                            if len(text) > 50:  # èŒè´£æè¿°é€šå¸¸è¾ƒé•¿
                                logger.debug(f"âœ… æ‰¾åˆ°å·¥ä½œæè¿°: {selector}")
                                return text
                                
            except Exception as e:
                logger.debug(f"æå–å·¥ä½œèŒè´£å¤±è´¥ {selector}: {e}")
                continue
        
        return "å·¥ä½œèŒè´£ä¿¡æ¯æœªæ‰¾åˆ°ï¼Œè¯·æŸ¥çœ‹å²—ä½è¯¦æƒ…é¡µ"
    
    async def _extract_job_requirements(self) -> str:
        """æå–ä»»èŒèµ„æ ¼"""
        selectors = [
            '.job-sec-text',
            '.job-detail-text .text', 
            '.job-requirements .text-desc',
            '.job-detail .job-sec .text-desc',
            '[class*="job-sec"] .text',
            '.text-desc',
            '.job-content .text'
        ]
        
        for selector in selectors:
            try:
                elements = await self.page.query_selector_all(selector)
                if elements:
                    texts = []
                    for element in elements:
                        text = await element.inner_text()
                        if text and text.strip():
                            texts.append(text.strip())
                    
                    if texts:
                        # æŸ¥æ‰¾åŒ…å«"è¦æ±‚"ã€"èµ„æ ¼"ã€"æ¡ä»¶"ç­‰å…³é”®è¯çš„éƒ¨åˆ†
                        for text in texts:
                            if any(keyword in text for keyword in ['ä»»èŒ', 'è¦æ±‚', 'èµ„æ ¼', 'æ¡ä»¶', 'æŠ€èƒ½', 'ç»éªŒ']):
                                logger.debug(f"âœ… æ‰¾åˆ°ä»»èŒè¦æ±‚: {selector}")
                                return text
                        
                        # å¦‚æœæœ‰å¤šä¸ªæ–‡æœ¬å—ï¼Œå–ç¬¬äºŒä¸ªï¼ˆç¬¬ä¸€ä¸ªé€šå¸¸æ˜¯èŒè´£ï¼‰
                        if len(texts) >= 2:
                            logger.debug(f"âœ… æ‰¾åˆ°ä»»èŒè¦æ±‚ï¼ˆç¬¬äºŒæ®µï¼‰: {selector}")
                            return texts[1]
                            
            except Exception as e:
                logger.debug(f"æå–ä»»èŒè¦æ±‚å¤±è´¥ {selector}: {e}")
                continue
        
        return "ä»»èŒè¦æ±‚ä¿¡æ¯æœªæ‰¾åˆ°ï¼Œè¯·æŸ¥çœ‹å²—ä½è¯¦æƒ…é¡µ"
    
    async def _extract_company_details(self) -> str:
        """æå–å…¬å¸è¯¦æƒ…"""
        selectors = [
            '.company-info .company-text',
            '.company-description',
            '.company-detail-text',
            '.company-info .text'
        ]
        
        for selector in selectors:
            try:
                element = await self.page.query_selector(selector)
                if element:
                    text = await element.inner_text()
                    if text and text.strip():
                        logger.debug(f"âœ… æ‰¾åˆ°å…¬å¸è¯¦æƒ…: {selector}")
                        return text.strip()
            except Exception as e:
                logger.debug(f"æå–å…¬å¸è¯¦æƒ…å¤±è´¥ {selector}: {e}")
                continue
        
        return "å…¬å¸è¯¦æƒ…ä¿¡æ¯æœªæ‰¾åˆ°"
    
    async def _extract_salary_info(self) -> str:
        """æå–è–ªèµ„ä¿¡æ¯"""
        # Bossç›´è˜è¯¦æƒ…é¡µçš„è–ªèµ„é€‰æ‹©å™¨
        selectors = [
            '.salary',
            '.job-primary .info-primary .salary',
            '.info-primary h1 + .salary',
            '.job-detail .salary',
            '[class*="salary"]',
            '.job-primary .name + .salary',
            'span.salary'
        ]
        
        for selector in selectors:
            try:
                element = await self.page.query_selector(selector)
                if element:
                    text = await element.inner_text()
                    if text and text.strip():
                        salary = text.strip()
                        # æ¸…ç†è–ªèµ„æ–‡æœ¬
                        salary = salary.replace('Â·', '-').replace('è–ª', '')
                        # éªŒè¯æ˜¯å¦æ˜¯æœ‰æ•ˆçš„è–ªèµ„æ ¼å¼
                        if any(k in salary for k in ['K', 'ä¸‡', 'åƒ']) and len(salary) > 2:
                            logger.debug(f"âœ… æ‰¾åˆ°è–ªèµ„ä¿¡æ¯: {selector} â†’ {salary}")
                            return salary
            except Exception as e:
                logger.debug(f"æå–è–ªèµ„å¤±è´¥ {selector}: {e}")
                continue
        
        # å°è¯•ä»é¡µé¢æ–‡æœ¬ä¸­æŸ¥æ‰¾è–ªèµ„
        try:
            page_text = await self.page.content()
            import re
            # åŒ¹é…è–ªèµ„æ¨¡å¼: 15K-25K, 15-25K, 1.5ä¸‡-2.5ä¸‡ç­‰
            salary_pattern = r'\b(\d+(?:\.\d+)?)\s*[-~]\s*(\d+(?:\.\d+)?)\s*([Kkåƒä¸‡])\b'
            match = re.search(salary_pattern, page_text)
            if match:
                salary = match.group(0)
                logger.debug(f"âœ… ä»é¡µé¢æ–‡æœ¬ä¸­æ‰¾åˆ°è–ªèµ„: {salary}")
                return salary
        except:
            pass
        
        return ""
    
    async def _extract_benefits(self) -> str:
        """æå–ç¦åˆ©å¾…é‡"""
        selectors = [
            '.job-tags .tag',
            '.welfare-list .welfare-item',
            '.job-welfare .tag-item',
            '.benefits .benefit-item'
        ]
        
        benefits = []
        for selector in selectors:
            try:
                elements = await self.page.query_selector_all(selector)
                for element in elements:
                    text = await element.inner_text()
                    if text and text.strip():
                        benefits.append(text.strip())
            except Exception as e:
                logger.debug(f"æå–ç¦åˆ©å¾…é‡å¤±è´¥ {selector}: {e}")
                continue
        
        if benefits:
            logger.debug(f"âœ… æ‰¾åˆ°ç¦åˆ©å¾…é‡: {len(benefits)} é¡¹")
            return " | ".join(benefits[:10])  # é™åˆ¶æ•°é‡é¿å…è¿‡é•¿
        
        return "ç¦åˆ©å¾…é‡ä¿¡æ¯æœªæ‰¾åˆ°"
    
    async def _handle_login_or_captcha(self):
        """å¤„ç†ç™»å½•æˆ–éªŒè¯ç """
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç™»å½•å¼¹çª—
            login_modal = await self.page.query_selector('.login-dialog, .dialog-wrap')
            if login_modal:
                logger.info("ğŸ” æ£€æµ‹åˆ°ç™»å½•å¼¹çª—ï¼Œç­‰å¾…ç”¨æˆ·å¤„ç†...")
                # ç­‰å¾…ä¸€æ®µæ—¶é—´è®©ç”¨æˆ·å¤„ç†
                await asyncio.sleep(5)
            
            # æ£€æŸ¥éªŒè¯ç 
            captcha = await self.page.query_selector('.captcha, .verify-wrap')
            if captcha:
                logger.info("ğŸ”’ æ£€æµ‹åˆ°éªŒè¯ç ï¼Œç­‰å¾…ç”¨æˆ·å¤„ç†...")
                await asyncio.sleep(5)
                
        except Exception as e:
            if "Execution context was destroyed" in str(e):
                logger.debug("âš ï¸ é¡µé¢å¯¼èˆªå¯¼è‡´ç™»å½•æ£€æŸ¥ä¸­æ–­ï¼ˆæ­£å¸¸ç°è±¡ï¼‰")
            else:
                logger.warning(f"âš ï¸ å¤„ç†ç™»å½•/éªŒè¯ç æ—¶å‡ºé”™: {e}")
    
    async def get_performance_report(self) -> Dict:
        """è·å–çˆ¬è™«æ€§èƒ½æŠ¥å‘Š"""
        return self.enhanced_extractor.get_performance_report()
    
    def get_system_status(self) -> Dict:
        """è·å–ç³»ç»Ÿæ•´ä½“çŠ¶æ€æŠ¥å‘Š"""
        return {
            'crawler_status': {
                'browser_active': self.browser is not None,
                'page_active': self.page is not None,
                'current_url': self.page.url if self.page else None
            },
            'session_info': self.session_manager.get_session_info(),
            'retry_stats': self.retry_handler.get_retry_stats(),
            'extractor_performance': self.enhanced_extractor.get_performance_report(),
            'city_codes': self.city_codes,
            'enhancement_status': {
                'smart_selector_enabled': True,
                'enhanced_extractor_enabled': True,
                'session_manager_enabled': True,
                'retry_handler_enabled': True,
                'version': 'v2.0-enhanced'
            }
        }
    
    async def take_screenshot(self, filename: str = None) -> str:
        """æˆªå–é¡µé¢æˆªå›¾"""
        try:
            if not self.page:
                return ""
            
            if not filename:
                timestamp = int(time.time())
                filename = f"boss_real_screenshot_{timestamp}.png"
            
            await self.page.screenshot(path=filename, full_page=True)
            logger.info(f"ğŸ“¸ æˆªå›¾ä¿å­˜: {filename}")
            return filename
            
        except Exception as e:
            logger.error(f"âŒ æˆªå›¾å¤±è´¥: {e}")
            return ""
    
    async def close(self):
        """å…³é—­æµè§ˆå™¨"""
        try:
            # å¯¹äºæŒä¹…åŒ–ä¸Šä¸‹æ–‡ï¼Œåªéœ€è¦å…³é—­ä¸Šä¸‹æ–‡
            if self.context:
                await self.context.close()
            # å¯¹äºéæŒä¹…åŒ–æ¨¡å¼ï¼Œéœ€è¦å…³é—­æµè§ˆå™¨
            elif self.browser:
                if self.page:
                    await self.page.close()
                await self.browser.close()
            
            if self.playwright:
                await self.playwright.stop()
            
            logger.info("ğŸ”š æµè§ˆå™¨å·²å…³é—­")
            
        except Exception as e:
            logger.error(f"âŒ å…³é—­æµè§ˆå™¨å¤±è´¥: {e}")


# åŒæ­¥åŒ…è£…å™¨
class RealPlaywrightBossSpiderSync:
    """çœŸæ­£çš„Playwright Bossç›´è˜çˆ¬è™«åŒæ­¥ç‰ˆæœ¬"""
    
    def __init__(self, headless: bool = False):
        self.headless = headless
        self.spider = None
    
    def search_jobs(self, keyword: str, city: str, max_jobs: int = 20) -> List[Dict]:
        """æœç´¢å²—ä½ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
        async def _search():
            self.spider = RealPlaywrightBossSpider(headless=self.headless)
            
            try:
                if await self.spider.start():
                    return await self.spider.search_jobs(keyword, city, max_jobs)
                else:
                    return []
            finally:
                if self.spider:
                    await self.spider.close()
        
        return asyncio.run(_search())


# é›†æˆæ¥å£
def search_with_real_playwright(keyword: str, city: str = "shanghai", max_jobs: int = 20) -> List[Dict]:
    """ä½¿ç”¨çœŸæ­£çš„Playwrightæœç´¢Bossç›´è˜å²—ä½"""
    logger.info(f"ğŸ­ å¯åŠ¨çœŸæ­£çš„Playwrightè‡ªåŠ¨åŒ–æœç´¢: {keyword}")
    
    try:
        spider = RealPlaywrightBossSpiderSync(headless=False)  # å¯è§æ¨¡å¼
        jobs = spider.search_jobs(keyword, city, max_jobs)
        
        logger.info(f"âœ… çœŸå®æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(jobs)} ä¸ªå²—ä½")
        return jobs
        
    except Exception as e:
        logger.error(f"âŒ çœŸå®Playwrightæœç´¢å¤±è´¥: {e}")
        return []


if __name__ == "__main__":
    # æµ‹è¯•çœŸæ­£çš„Playwrightçˆ¬è™«
    import logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    print("ğŸ­ æµ‹è¯•çœŸæ­£çš„Playwright Bossç›´è˜çˆ¬è™«")
    print("=" * 50)
    
    # æµ‹è¯•æœç´¢
    jobs = search_with_real_playwright("æ•°æ®åˆ†æ", "shanghai", 3)
    
    print(f"\nâœ… æ‰¾åˆ° {len(jobs)} ä¸ªå²—ä½:")
    for i, job in enumerate(jobs, 1):
        print(f"\nğŸ“‹ å²—ä½ #{i}")
        print(f"  èŒä½: {job.get('title', 'æœªçŸ¥')}")
        print(f"  å…¬å¸: {job.get('company', 'æœªçŸ¥')}")
        print(f"  è–ªèµ„: {job.get('salary', 'æœªçŸ¥')}")
        print(f"  åœ°ç‚¹: {job.get('work_location', 'æœªçŸ¥')}")
        print(f"  é“¾æ¥: {job.get('url', 'æœªçŸ¥')}")