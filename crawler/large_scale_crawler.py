#!/usr/bin/env python3
"""
å¤§è§„æ¨¡å²—ä½æŠ“å–ä¼˜åŒ–å¼•æ“
æ”¯æŒ50-100+å²—ä½é«˜æ•ˆæŠ“å–ï¼ŒåŒ…å«æ™ºèƒ½åˆ†é¡µã€æ‰¹é‡å¤„ç†å’Œæ€§èƒ½ä¼˜åŒ–
"""

import asyncio
import logging
import time
import math
from typing import List, Dict, Optional, Tuple
from playwright.async_api import Page, Browser
from .enhanced_extractor import EnhancedDataExtractor
from .session_manager import SessionManager
from .retry_handler import RetryHandler, retry_on_error

logger = logging.getLogger(__name__)


class LargeScaleCrawler:
    """å¤§è§„æ¨¡å²—ä½æŠ“å–å¼•æ“"""
    
    def __init__(self, page: Page, session_manager: SessionManager, retry_handler: RetryHandler):
        self.page = page
        self.session_manager = session_manager
        self.retry_handler = retry_handler
        self.enhanced_extractor = EnhancedDataExtractor()
        
        # å¤§è§„æ¨¡æŠ“å–é…ç½®
        self.batch_size = 20  # æ¯æ‰¹å¤„ç†çš„å²—ä½æ•°é‡
        self.max_scroll_attempts = 15  # æœ€å¤§æ»šåŠ¨å°è¯•æ¬¡æ•°
        self.scroll_delay = 2.0  # æ»šåŠ¨å»¶è¿Ÿ
        self.page_load_timeout = 30  # é¡µé¢åŠ è½½è¶…æ—¶
        
        # æ€§èƒ½ç›‘æ§
        self.performance_stats = {
            "total_jobs_found": 0,
            "successful_extractions": 0,
            "total_pages_processed": 0,
            "total_scroll_attempts": 0,
            "avg_page_processing_time": 0.0,
            "start_time": time.time()
        }
    
    async def extract_large_scale_jobs(self, max_jobs: int = 80) -> List[Dict]:
        """
        å¤§è§„æ¨¡å²—ä½æŠ“å–ä¸»å…¥å£
        
        Args:
            max_jobs: ç›®æ ‡å²—ä½æ•°é‡ï¼ˆ50-100+ï¼‰
            
        Returns:
            æå–çš„å²—ä½æ•°æ®åˆ—è¡¨
        """
        logger.info(f"ğŸš€ å¯åŠ¨å¤§è§„æ¨¡å²—ä½æŠ“å–å¼•æ“ï¼Œç›®æ ‡: {max_jobs} ä¸ªå²—ä½")
        start_time = time.time()
        
        try:
            # ç¬¬ä¸€é˜¶æ®µï¼šæ™ºèƒ½é¡µé¢å‡†å¤‡å’Œå†…å®¹å‘ç°
            await self._prepare_large_scale_extraction()
            
            # ç¬¬äºŒé˜¶æ®µï¼šæ™ºèƒ½æ»šåŠ¨å’Œå†…å®¹åŠ è½½
            total_jobs_loaded = await self._intelligent_scroll_and_load(max_jobs)
            
            # ç¬¬ä¸‰é˜¶æ®µï¼šæ‰¹é‡æ•°æ®æå–
            all_jobs = await self._batch_extract_jobs(max_jobs)
            
            # ç¬¬å››é˜¶æ®µï¼šæ•°æ®è´¨é‡éªŒè¯å’Œå»é‡
            validated_jobs = await self._validate_and_deduplicate(all_jobs)
            
            extraction_time = time.time() - start_time
            self._update_performance_stats(len(validated_jobs), extraction_time)
            
            logger.info(f"âœ… å¤§è§„æ¨¡æŠ“å–å®Œæˆ: {len(validated_jobs)} ä¸ªå²—ä½ï¼Œè€—æ—¶ {extraction_time:.2f}s")
            logger.info(f"ğŸ“Š æŠ“å–æ•ˆç‡: {len(validated_jobs)/extraction_time:.1f} å²—ä½/ç§’")
            
            return validated_jobs[:max_jobs]  # ç¡®ä¿ä¸è¶…è¿‡ç›®æ ‡æ•°é‡
            
        except Exception as e:
            logger.error(f"âŒ å¤§è§„æ¨¡æŠ“å–å¤±è´¥: {e}")
            return []
    
    async def _prepare_large_scale_extraction(self) -> None:
        """å¤§è§„æ¨¡æŠ“å–é¡µé¢å‡†å¤‡"""
        logger.info("ğŸ”§ å‡†å¤‡å¤§è§„æ¨¡æŠ“å–ç¯å¢ƒ...")
        
        # ä¼˜åŒ–é¡µé¢æ€§èƒ½è®¾ç½®
        await self.page.evaluate("""
            () => {
                // ç¦ç”¨ä¸å¿…è¦çš„åŠ¨ç”»ä»¥æå‡æ€§èƒ½
                const style = document.createElement('style');
                style.textContent = `
                    *, *::before, *::after {
                        animation-duration: 0.01s !important;
                        animation-delay: 0.01s !important;
                        transition-duration: 0.01s !important;
                        transition-delay: 0.01s !important;
                    }
                `;
                document.head.appendChild(style);
                
                // ä¼˜åŒ–æ»šåŠ¨æ€§èƒ½
                document.documentElement.style.scrollBehavior = 'auto';
            }
        """)
        
        # ç­‰å¾…åˆå§‹å†…å®¹åŠ è½½
        await asyncio.sleep(3)
        
        # æ£€æŸ¥å¹¶å¤„ç†å¯èƒ½çš„åçˆ¬è™«æœºåˆ¶
        await self._handle_anti_crawling_measures()
        
        logger.info("âœ… å¤§è§„æ¨¡æŠ“å–ç¯å¢ƒå‡†å¤‡å®Œæˆ")
    
    async def _handle_anti_crawling_measures(self) -> None:
        """å¤„ç†åçˆ¬è™«æªæ–½"""
        try:
            # æ£€æŸ¥éªŒè¯ç 
            captcha_selectors = ['.captcha', '.verify-wrap', '[class*="captcha"]', '.geetest']
            for selector in captcha_selectors:
                element = await self.page.query_selector(selector)
                if element and await element.is_visible():
                    logger.warning("ğŸ”’ æ£€æµ‹åˆ°éªŒè¯ç ï¼Œæš‚åœç­‰å¾…å¤„ç†...")
                    await asyncio.sleep(10)
                    break
            
            # æ£€æŸ¥ç™»å½•è¦æ±‚
            login_selectors = ['.login-dialog', '.dialog-wrap', '.modal[class*="login"]']
            for selector in login_selectors:
                element = await self.page.query_selector(selector)
                if element and await element.is_visible():
                    logger.info("ğŸ” æ£€æµ‹åˆ°ç™»å½•è¦æ±‚ï¼Œä½¿ç”¨ä¼šè¯ç®¡ç†...")
                    if not await self.session_manager.check_login_status(self.page, "zhipin.com"):
                        logger.warning("âš ï¸ éœ€è¦ç™»å½•æ‰èƒ½ç»§ç»­å¤§è§„æ¨¡æŠ“å–")
                    break
                    
        except Exception as e:
            logger.debug(f"å¤„ç†åçˆ¬è™«æªæ–½æ—¶å‡ºé”™: {e}")
    
    async def _intelligent_scroll_and_load(self, target_jobs: int) -> int:
        """
        æ™ºèƒ½æ»šåŠ¨å’Œå†…å®¹åŠ è½½ç­–ç•¥
        
        Returns:
            å·²åŠ è½½çš„å²—ä½æ•°é‡
        """
        logger.info(f"ğŸ“œ å¼€å§‹æ™ºèƒ½æ»šåŠ¨åŠ è½½ï¼Œç›®æ ‡: {target_jobs} ä¸ªå²—ä½")
        
        jobs_loaded = 0
        scroll_attempts = 0
        consecutive_no_new_content = 0
        
        while jobs_loaded < target_jobs and scroll_attempts < self.max_scroll_attempts:
            scroll_attempts += 1
            self.performance_stats["total_scroll_attempts"] += 1
            
            # è·å–å½“å‰å²—ä½æ•°é‡
            current_jobs = await self._count_current_jobs()
            
            logger.info(f"   æ»šåŠ¨ {scroll_attempts}/{self.max_scroll_attempts}: å½“å‰å‘ç° {current_jobs} ä¸ªå²—ä½")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–°å†…å®¹åŠ è½½
            if current_jobs > jobs_loaded:
                jobs_loaded = current_jobs
                consecutive_no_new_content = 0
            else:
                consecutive_no_new_content += 1
                
                # å¦‚æœè¿ç»­å¤šæ¬¡æ²¡æœ‰æ–°å†…å®¹ï¼Œå°è¯•ä¸åŒçš„æ»šåŠ¨ç­–ç•¥
                if consecutive_no_new_content >= 3:
                    logger.info("ğŸ”„ å°è¯•æ›¿ä»£æ»šåŠ¨ç­–ç•¥...")
                    await self._alternative_scroll_strategy()
                    consecutive_no_new_content = 0
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»æ‰¾åˆ°è¶³å¤Ÿçš„å²—ä½
            if jobs_loaded >= target_jobs:
                logger.info(f"âœ… å·²æ‰¾åˆ°è¶³å¤Ÿå²—ä½: {jobs_loaded}")
                break
            
            # ç»§ç»­æ»šåŠ¨åŠ è½½æ›´å¤šå†…å®¹
            await self._smart_scroll_step()
            
            # å¦‚æœè¿ç»­å¤šæ¬¡æ²¡æœ‰æ–°å†…å®¹ï¼Œå¯èƒ½å·²ç»åˆ°åº•äº†
            if consecutive_no_new_content >= 5:
                logger.info("ğŸ“„ å¯èƒ½å·²ç»åˆ°è¾¾é¡µé¢åº•éƒ¨ï¼Œç»“æŸæ»šåŠ¨")
                break
        
        logger.info(f"ğŸ“œ æ»šåŠ¨å®Œæˆ: æ€»å…±å‘ç° {jobs_loaded} ä¸ªå²—ä½")
        return jobs_loaded
    
    async def _count_current_jobs(self) -> int:
        """ç»Ÿè®¡å½“å‰é¡µé¢çš„å²—ä½æ•°é‡"""
        job_selectors = [
            'li[data-jobid]',
            '.job-card',
            '.job-item',
            '[class*="job-primary"]',
            '.job-list li',
            'li:has(.job-title)',
            'li:has(.job-name)'
        ]
        
        max_count = 0
        for selector in job_selectors:
            try:
                elements = await self.page.query_selector_all(selector)
                count = len(elements)
                if count > max_count:
                    max_count = count
            except:
                continue
        
        return max_count
    
    async def _smart_scroll_step(self) -> None:
        """æ™ºèƒ½æ»šåŠ¨æ­¥éª¤"""
        try:
            # è·å–å½“å‰é¡µé¢é«˜åº¦
            current_height = await self.page.evaluate("document.body.scrollHeight")
            
            # æ»šåŠ¨åˆ°é¡µé¢åº•éƒ¨
            await self.page.evaluate("""
                () => {
                    window.scrollTo(0, document.body.scrollHeight);
                }
            """)
            
            # ç­‰å¾…å†…å®¹åŠ è½½
            await asyncio.sleep(self.scroll_delay)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ–°å†…å®¹åŠ è½½
            new_height = await self.page.evaluate("document.body.scrollHeight")
            
            # å¦‚æœé¡µé¢é«˜åº¦å¢åŠ ï¼Œè¯´æ˜æœ‰æ–°å†…å®¹åŠ è½½
            if new_height > current_height:
                logger.debug(f"   é¡µé¢é«˜åº¦å¢åŠ : {current_height} -> {new_height}")
                # é¢å¤–ç­‰å¾…ç¡®ä¿å†…å®¹å®Œå…¨åŠ è½½
                await asyncio.sleep(1)
            
        except Exception as e:
            logger.debug(f"æ»šåŠ¨æ­¥éª¤å¤±è´¥: {e}")
    
    async def _alternative_scroll_strategy(self) -> None:
        """æ›¿ä»£æ»šåŠ¨ç­–ç•¥ï¼ˆå½“æ ‡å‡†æ»šåŠ¨æ— æ•ˆæ—¶ï¼‰"""
        try:
            # ç­–ç•¥1: åˆ†æ®µæ»šåŠ¨
            viewport_height = await self.page.evaluate("window.innerHeight")
            for i in range(3):
                scroll_position = viewport_height * (i + 1)
                await self.page.evaluate(f"window.scrollTo(0, {scroll_position})")
                await asyncio.sleep(1)
            
            # ç­–ç•¥2: å°è¯•ç‚¹å‡»"åŠ è½½æ›´å¤š"æŒ‰é’®
            load_more_selectors = [
                '.load-more',
                '.more-btn',
                '[class*="load-more"]',
                'button:has-text("æ›´å¤š")',
                'button:has-text("åŠ è½½")'
            ]
            
            for selector in load_more_selectors:
                try:
                    button = await self.page.query_selector(selector)
                    if button and await button.is_visible():
                        logger.info(f"ğŸ”˜ æ‰¾åˆ°åŠ è½½æ›´å¤šæŒ‰é’®: {selector}")
                        await button.click()
                        await asyncio.sleep(3)
                        return
                except:
                    continue
            
            # ç­–ç•¥3: é”®ç›˜æ»šåŠ¨
            await self.page.keyboard.press('End')
            await asyncio.sleep(2)
            
        except Exception as e:
            logger.debug(f"æ›¿ä»£æ»šåŠ¨ç­–ç•¥å¤±è´¥: {e}")
    
    async def _batch_extract_jobs(self, max_jobs: int) -> List[Dict]:
        """æ‰¹é‡æå–å²—ä½æ•°æ®"""
        logger.info(f"ğŸ”„ å¼€å§‹æ‰¹é‡æå–å²—ä½æ•°æ®ï¼Œç›®æ ‡: {max_jobs}")
        
        # ä½¿ç”¨å¢å¼ºæå–å™¨è·å–æ‰€æœ‰å²—ä½
        all_jobs = await self.enhanced_extractor.extract_job_listings_enhanced(
            self.page, max_jobs
        )
        
        logger.info(f"ğŸ“Š æ‰¹é‡æå–å®Œæˆ: {len(all_jobs)} ä¸ªå²—ä½")
        return all_jobs
    
    async def _validate_and_deduplicate(self, jobs: List[Dict]) -> List[Dict]:
        """æ•°æ®è´¨é‡éªŒè¯å’Œå»é‡"""
        logger.info(f"ğŸ” å¼€å§‹æ•°æ®éªŒè¯å’Œå»é‡ï¼ŒåŸå§‹æ•°æ®: {len(jobs)} ä¸ªå²—ä½")
        
        validated_jobs = []
        seen_urls = set()
        seen_titles_companies = set()
        
        for job in jobs:
            try:
                # åŸºæœ¬å­—æ®µéªŒè¯
                if not self._validate_job_data(job):
                    continue
                
                # URLå»é‡
                job_url = job.get('url', '')
                if job_url and job_url in seen_urls:
                    continue
                seen_urls.add(job_url)
                
                # æ ‡é¢˜+å…¬å¸å»é‡
                title = job.get('title', '').strip()
                company = job.get('company', '').strip()
                title_company_key = f"{title}_{company}"
                
                if title_company_key in seen_titles_companies:
                    continue
                seen_titles_companies.add(title_company_key)
                
                # æ·»åŠ æå–ç´¢å¼•å’Œæ—¶é—´æˆ³
                job['extraction_index'] = len(validated_jobs)
                job['extraction_timestamp'] = time.time()
                job['engine_source'] = 'Large Scale Crawler'
                job['extraction_method'] = 'batch'
                
                validated_jobs.append(job)
                
            except Exception as e:
                logger.debug(f"éªŒè¯å²—ä½æ•°æ®å¤±è´¥: {e}")
                continue
        
        logger.info(f"âœ… éªŒè¯å®Œæˆ: {len(validated_jobs)} ä¸ªæœ‰æ•ˆå²—ä½")
        return validated_jobs
    
    def _validate_job_data(self, job: Dict) -> bool:
        """éªŒè¯å•ä¸ªå²—ä½æ•°æ®çš„æœ‰æ•ˆæ€§"""
        required_fields = ['title', 'company']
        
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        for field in required_fields:
            if not job.get(field) or not job[field].strip():
                return False
        
        # æ£€æŸ¥æ•°æ®è´¨é‡
        title = job.get('title', '').strip()
        if len(title) < 2 or len(title) > 100:
            return False
        
        company = job.get('company', '').strip()
        if len(company) < 2 or len(company) > 100:
            return False
        
        return True
    
    def _update_performance_stats(self, successful_jobs: int, total_time: float) -> None:
        """æ›´æ–°æ€§èƒ½ç»Ÿè®¡"""
        self.performance_stats.update({
            "successful_extractions": successful_jobs,
            "total_processing_time": total_time,
            "extraction_rate": successful_jobs / total_time if total_time > 0 else 0,
            "efficiency_score": min(successful_jobs / 50, 1.0)  # ä»¥50ä¸ªå²—ä½ä¸ºåŸºå‡†è®¡ç®—æ•ˆç‡
        })
    
    def get_performance_report(self) -> Dict:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        return {
            "large_scale_crawler_stats": self.performance_stats,
            "configuration": {
                "batch_size": self.batch_size,
                "max_scroll_attempts": self.max_scroll_attempts,
                "scroll_delay": self.scroll_delay,
                "page_load_timeout": self.page_load_timeout
            },
            "status": "optimized_for_large_scale"
        }


class LargeScaleProgressTracker:
    """å¤§è§„æ¨¡æŠ“å–è¿›åº¦è·Ÿè¸ªå™¨"""
    
    def __init__(self, total_target: int):
        self.total_target = total_target
        self.current_progress = 0
        self.phase_progress = {}
        self.start_time = time.time()
    
    def update_phase(self, phase_name: str, progress: int, total: int):
        """æ›´æ–°é˜¶æ®µè¿›åº¦"""
        percentage = (progress / total) * 100 if total > 0 else 0
        self.phase_progress[phase_name] = {
            'current': progress,
            'total': total,
            'percentage': percentage
        }
        
        logger.info(f"ğŸ“ˆ {phase_name}: {progress}/{total} ({percentage:.1f}%)")
    
    def get_overall_progress(self) -> Dict:
        """è·å–æ•´ä½“è¿›åº¦"""
        elapsed_time = time.time() - self.start_time
        
        return {
            "target_jobs": self.total_target,
            "current_jobs": self.current_progress,
            "overall_percentage": (self.current_progress / self.total_target) * 100,
            "elapsed_time": elapsed_time,
            "estimated_remaining": self._estimate_remaining_time(),
            "phase_details": self.phase_progress
        }
    
    def _estimate_remaining_time(self) -> float:
        """ä¼°ç®—å‰©ä½™æ—¶é—´"""
        elapsed = time.time() - self.start_time
        if self.current_progress > 0:
            rate = self.current_progress / elapsed
            remaining_jobs = self.total_target - self.current_progress
            return remaining_jobs / rate if rate > 0 else 0
        return 0