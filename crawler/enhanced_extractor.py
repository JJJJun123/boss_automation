#!/usr/bin/env python3
"""
å¢å¼ºæ•°æ®æå–å¼•æ“
æ•´åˆæ™ºèƒ½é€‰æ‹©å™¨ç³»ç»Ÿï¼Œæä¾›é«˜è´¨é‡çš„æ•°æ®æå–å’ŒéªŒè¯
"""

import logging
import asyncio
import time
import re
from typing import List, Dict, Optional, Tuple
from playwright.async_api import Page, ElementHandle
from .smart_selector import SmartSelector, ExtractedField

logger = logging.getLogger(__name__)


class EnhancedDataExtractor:
    """å¢å¼ºæ•°æ®æå–å¼•æ“"""
    
    def __init__(self):
        self.smart_selector = SmartSelector()
        self.extraction_cache = {}  # ç¼“å­˜æå–ç»“æœ
        self.performance_stats = {
            "total_extractions": 0,
            "successful_extractions": 0,
            "avg_extraction_time": 0.0,
            "field_success_rates": {}
        }
        
    async def extract_job_listings_enhanced(self, page: Page, max_jobs: int = 20) -> List[Dict]:
        """
        ä½¿ç”¨å¢å¼ºç®—æ³•æå–å²—ä½åˆ—è¡¨
        
        Args:
            page: Playwrighté¡µé¢å¯¹è±¡
            max_jobs: æœ€å¤§æå–å²—ä½æ•°é‡
            
        Returns:
            æå–çš„å²—ä½æ•°æ®åˆ—è¡¨
        """
        start_time = time.time()
        
        try:
            logger.info("ğŸš€ å¯åŠ¨å¢å¼ºæ•°æ®æå–å¼•æ“...")
            
            # ç¬¬ä¸€æ­¥ï¼šé¡µé¢é¢„å¤„ç†å’Œæ™ºèƒ½ç­‰å¾…
            await self._prepare_page_for_extraction(page)
            
            # ç¬¬äºŒæ­¥ï¼šåŠ¨æ€å‘ç°æœ€ä½³å²—ä½å®¹å™¨é€‰æ‹©å™¨
            logger.info("ğŸ” åˆ†æé¡µé¢ç»“æ„ï¼Œå¯»æ‰¾æœ€ä½³é€‰æ‹©å™¨...")
            best_container_selectors = await self.smart_selector.find_best_selectors(
                page, "job_container", sample_size=3
            )
            
            if not best_container_selectors:
                logger.warning("âš ï¸ æ™ºèƒ½é€‰æ‹©å™¨æœªæ‰¾åˆ°æœ‰æ•ˆé€‰æ‹©å™¨ï¼Œå°è¯•é™çº§ç­–ç•¥...")
                # é™çº§ç­–ç•¥ï¼šä½¿ç”¨æœ€åŸºç¡€çš„å…ƒç´ é€‰æ‹©
                fallback_result = await self._fallback_extraction(page, max_jobs)
                if fallback_result:
                    return fallback_result
                logger.error("âŒ æ‰€æœ‰æå–ç­–ç•¥éƒ½å¤±è´¥äº†")
                return []
            
            # ç¬¬ä¸‰æ­¥ï¼šæå–å²—ä½å®¹å™¨å…ƒç´ 
            job_elements = await self._get_job_elements(page, best_container_selectors)
            logger.info(f"ğŸ“‹ æ‰¾åˆ° {len(job_elements)} ä¸ªå²—ä½å®¹å™¨")
            
            if not job_elements:
                await self._debug_page_content(page)
                return []
            
            # ç¬¬å››æ­¥ï¼šé¢„å…ˆå‘ç°å„å­—æ®µçš„æœ€ä½³é€‰æ‹©å™¨
            field_selectors = await self._discover_field_selectors(page, job_elements[:3])
            
            # ç¬¬äº”æ­¥ï¼šæ‰¹é‡æå–å²—ä½æ•°æ®
            jobs = await self._extract_jobs_batch(job_elements[:max_jobs], field_selectors)
            
            # ç¬¬å…­æ­¥ï¼šæ•°æ®è´¨é‡éªŒè¯å’Œå¢å¼º
            validated_jobs = await self._validate_and_enhance_jobs(jobs, page)
            
            extraction_time = time.time() - start_time
            self._update_performance_stats(len(validated_jobs), extraction_time)
            
            logger.info(f"âœ… å¢å¼ºæå–å®Œæˆ: {len(validated_jobs)}/{len(job_elements)} ä¸ªå²—ä½ï¼Œè€—æ—¶ {extraction_time:.2f}s")
            return validated_jobs
            
        except Exception as e:
            logger.error(f"âŒ å¢å¼ºæ•°æ®æå–å¤±è´¥: {e}")
            return []
    
    async def _prepare_page_for_extraction(self, page: Page) -> None:
        """é¡µé¢é¢„å¤„ç†å’Œæ™ºèƒ½ç­‰å¾…"""
        try:
            # è®¾ç½®è¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´é¿å…é•¿æ—¶é—´ç­‰å¾…
            page.set_default_timeout(15000)  # 15ç§’è¶…æ—¶
            
            # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
            await page.wait_for_load_state("domcontentloaded")
            await page.wait_for_load_state("networkidle")  # ç­‰å¾…ç½‘ç»œç©ºé—²
            
            # Bossç›´è˜ç‰¹æœ‰ï¼šç­‰å¾…éª¨æ¶å±æ¶ˆå¤±ï¼ŒçœŸå®å†…å®¹åŠ è½½
            await self._wait_for_content_load(page)
            
            await page.wait_for_timeout(2000)  # å‡å°‘ç­‰å¾…æ—¶é—´
            
            # å®‰å…¨è·å–é¡µé¢é«˜åº¦ - å¤„ç†document.bodyä¸ºnullçš„æƒ…å†µ
            initial_height = await page.evaluate("""
                () => {
                    // ç¡®ä¿document.bodyå­˜åœ¨
                    if (!document.body) {
                        return document.documentElement ? document.documentElement.scrollHeight : 1000;
                    }
                    return Math.max(
                        document.body.scrollHeight || 0,
                        document.documentElement.scrollHeight || 0,
                        window.innerHeight || 0
                    );
                }
            """)
            logger.info(f"é¡µé¢åˆå§‹é«˜åº¦: {initial_height}")
            
            # åˆ†æ®µæ»šåŠ¨ï¼Œè§¦å‘æ‡’åŠ è½½
            scroll_steps = min(5, max(2, initial_height // 2000))  # æ ¹æ®é¡µé¢é«˜åº¦ç¡®å®šæ»šåŠ¨æ¬¡æ•°
            
            for i in range(scroll_steps):
                scroll_position = (i + 1) * (initial_height // scroll_steps)
                await page.evaluate(f"window.scrollTo(0, {scroll_position})")
                await asyncio.sleep(1.5)  # ç»™äºˆè¶³å¤Ÿæ—¶é—´åŠ è½½å†…å®¹
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ–°å†…å®¹åŠ è½½ï¼ˆå®‰å…¨è·å–ï¼‰
                new_height = await page.evaluate("""
                    () => {
                        if (!document.body) {
                            return document.documentElement ? document.documentElement.scrollHeight : 1000;
                        }
                        return Math.max(
                            document.body.scrollHeight || 0,
                            document.documentElement.scrollHeight || 0
                        );
                    }
                """)
                if new_height > initial_height:
                    logger.info(f"æ£€æµ‹åˆ°æ–°å†…å®¹åŠ è½½ï¼Œé¡µé¢é«˜åº¦: {initial_height} -> {new_height}")
                    initial_height = new_height
            
            # æ»šåŠ¨å›é¡¶éƒ¨ï¼Œç¡®ä¿æ‰€æœ‰å…ƒç´ éƒ½åœ¨è§†å£å†…
            await page.evaluate("window.scrollTo(0, 0)")
            await asyncio.sleep(2)
            
            # æ£€æŸ¥å¹¶å¤„ç†å¯èƒ½çš„å¼¹çª—æˆ–åŠ è½½çŠ¶æ€
            await self._handle_page_overlays(page)
            
        except Exception as e:
            logger.warning(f"é¡µé¢é¢„å¤„ç†å¤±è´¥: {e}")
    
    async def _wait_for_content_load(self, page: Page) -> None:
        """ç­‰å¾…Bossç›´è˜å†…å®¹åŠ è½½å®Œæˆï¼Œéª¨æ¶å±æ¶ˆå¤±"""
        try:
            logger.info("â³ ç­‰å¾…Bossç›´è˜å†…å®¹åŠ è½½...")
            
            # ç­‰å¾…å²—ä½åˆ—è¡¨å®¹å™¨å‡ºç°ï¼ˆééª¨æ¶å±ï¼‰
            content_selectors = [
                '.job-card-wrapper',  # å²—ä½å¡ç‰‡
                '.job-list-item',     # å²—ä½åˆ—è¡¨é¡¹
                '.job-detail-box',    # å²—ä½è¯¦æƒ…æ¡†
                'li[data-jid]',       # å¸¦æ•°æ®IDçš„å²—ä½
                '.job-primary'        # å²—ä½ä¸»è¦ä¿¡æ¯
            ]
            
            # å°è¯•ç­‰å¾…ä»»æ„ä¸€ä¸ªçœŸå®å†…å®¹é€‰æ‹©å™¨å‡ºç°
            for selector in content_selectors:
                try:
                    await page.wait_for_selector(selector, timeout=5000)  # å‡å°‘å•ä¸ªé€‰æ‹©å™¨çš„ç­‰å¾…æ—¶é—´
                    logger.info(f"âœ… æ£€æµ‹åˆ°å†…å®¹åŠ è½½å®Œæˆ: {selector}")
                    return
                except:
                    continue
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®çš„å†…å®¹ï¼Œç­‰å¾…éª¨æ¶å±æ¶ˆå¤±
            skeleton_selectors = [
                '.skeleton',
                '[class*="skeleton"]',
                '.loading-placeholder',
                '[class*="loading"]'
            ]
            
            for selector in skeleton_selectors:
                try:
                    # ç­‰å¾…éª¨æ¶å±æ¶ˆå¤±
                    await page.wait_for_selector(selector, state="hidden", timeout=5000)
                    logger.info(f"âœ… éª¨æ¶å±å·²æ¶ˆå¤±: {selector}")
                    break
                except:
                    continue
            
            # é¢å¤–ç­‰å¾…åŠ¨ç”»å®Œæˆ
            await page.wait_for_timeout(2000)
            
        except Exception as e:
            if "Timeout" in str(e):
                logger.info("â³ å†…å®¹åŠ è½½ç­‰å¾…è¶…æ—¶ï¼ˆç»§ç»­å¤„ç†ï¼‰")
            else:
                logger.warning(f"ç­‰å¾…å†…å®¹åŠ è½½å¤±è´¥: {e}")
    
    async def _handle_page_overlays(self, page: Page) -> None:
        """å¤„ç†é¡µé¢è¦†ç›–å±‚ï¼ˆå¼¹çª—ã€åŠ è½½ä¸­ç­‰ï¼‰"""
        try:
            # æ£€æŸ¥ç™»å½•å¼¹çª—
            login_modal = await page.query_selector('.login-dialog, .dialog-wrap, .modal')
            if login_modal and await login_modal.is_visible():
                logger.info("ğŸ” æ£€æµ‹åˆ°ç™»å½•å¼¹çª—ï¼Œç­‰å¾…å¤„ç†...")
                await asyncio.sleep(3)
            
            # æ£€æŸ¥åŠ è½½ä¸­çŠ¶æ€
            loading_selectors = ['.loading', '.spinner', '[class*="loading"]', '.skeleton']
            for selector in loading_selectors:
                loading_elem = await page.query_selector(selector)
                if loading_elem and await loading_elem.is_visible():
                    logger.info(f"â³ æ£€æµ‹åˆ°åŠ è½½çŠ¶æ€: {selector}")
                    # ç­‰å¾…åŠ è½½å®Œæˆ
                    try:
                        await page.wait_for_selector(selector, state="hidden", timeout=10000)
                    except:
                        pass  # è¶…æ—¶ä¸å½±å“ç»§ç»­æ‰§è¡Œ
                    break
            
            # æ£€æŸ¥éªŒè¯ç 
            captcha = await page.query_selector('.captcha, .verify-wrap, [class*="captcha"]')
            if captcha and await captcha.is_visible():
                logger.warning("ğŸ”’ æ£€æµ‹åˆ°éªŒè¯ç ï¼Œéœ€è¦äººå·¥å¤„ç†")
                await asyncio.sleep(5)
                
        except Exception as e:
            logger.debug(f"å¤„ç†é¡µé¢è¦†ç›–å±‚æ—¶å‡ºé”™: {e}")
    
    async def _get_job_elements(self, page: Page, selectors: List[str]) -> List[ElementHandle]:
        """è·å–å²—ä½å…ƒç´ ï¼Œä½¿ç”¨æœ€ä½³é€‰æ‹©å™¨"""
        all_elements = []
        seen_positions = set()  # ç”¨äºå»é‡
        
        for selector in selectors:
            try:
                elements = await page.query_selector_all(selector)
                logger.debug(f"é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")
                
                for element in elements:
                    # åŸºäºä½ç½®å»é‡
                    try:
                        bbox = await element.bounding_box()
                        if bbox:
                            position_key = (round(bbox['x']), round(bbox['y']))
                            if position_key not in seen_positions:
                                seen_positions.add(position_key)
                                all_elements.append(element)
                    except:
                        # å¦‚æœè·å–ä½ç½®å¤±è´¥ï¼Œä»ç„¶åŒ…å«å…ƒç´ 
                        all_elements.append(element)
                        
            except Exception as e:
                logger.debug(f"é€‰æ‹©å™¨ '{selector}' æ‰§è¡Œå¤±è´¥: {e}")
        
        # è¿‡æ»¤æ— æ•ˆå…ƒç´ 
        valid_elements = []
        for element in all_elements:
            try:
                # æ£€æŸ¥å…ƒç´ æ˜¯å¦å¯è§ä¸”åŒ…å«å†…å®¹
                if await element.is_visible():
                    text = await element.inner_text()
                    if text and len(text.strip()) > 20:  # å²—ä½ä¿¡æ¯åº”è¯¥æœ‰ä¸€å®šé•¿åº¦
                        valid_elements.append(element)
            except:
                continue
        
        logger.info(f"ä» {len(all_elements)} ä¸ªå…ƒç´ ä¸­ç­›é€‰å‡º {len(valid_elements)} ä¸ªæœ‰æ•ˆå²—ä½")
        return valid_elements
    
    async def _discover_field_selectors(self, page: Page, sample_elements: List[ElementHandle]) -> Dict[str, List[str]]:
        """ä¸ºæ¯ä¸ªå­—æ®µå‘ç°æœ€ä½³é€‰æ‹©å™¨"""
        field_selectors = {}
        field_types = ["job_title", "company_name", "salary", "location", "job_link"]
        
        logger.info("ğŸ”¬ åˆ†æå­—æ®µé€‰æ‹©å™¨...")
        
        for field_type in field_types:
            try:
                # ä½¿ç”¨æ ·æœ¬å…ƒç´ æµ‹è¯•é€‰æ‹©å™¨
                best_selectors = []
                
                # è·å–è¯¥å­—æ®µçš„é¢„å®šä¹‰é€‰æ‹©å™¨
                config = self.smart_selector.selector_configs.get(field_type, {})
                all_selectors = config.get("primary", []) + config.get("fallback", [])
                
                # åœ¨æ ·æœ¬å…ƒç´ ä¸Šæµ‹è¯•æ¯ä¸ªé€‰æ‹©å™¨
                selector_scores = {}
                
                for selector in all_selectors:
                    success_count = 0
                    quality_sum = 0.0
                    
                    for element in sample_elements:
                        try:
                            sub_element = await element.query_selector(selector)
                            if sub_element:
                                text = await sub_element.inner_text()
                                if text and text.strip():
                                    quality = self.smart_selector._calculate_quality_score(text.strip(), field_type)
                                    if quality > 0.3:
                                        success_count += 1
                                        quality_sum += quality
                        except:
                            continue
                    
                    if success_count > 0:
                        avg_quality = quality_sum / success_count
                        success_rate = success_count / len(sample_elements)
                        score = success_rate * 0.7 + avg_quality * 0.3
                        selector_scores[selector] = score
                
                # é€‰æ‹©æœ€ä½³çš„é€‰æ‹©å™¨
                sorted_selectors = sorted(selector_scores.items(), key=lambda x: x[1], reverse=True)
                best_selectors = [sel for sel, score in sorted_selectors[:3] if score > 0.2]
                
                field_selectors[field_type] = best_selectors
                logger.debug(f"{field_type} æœ€ä½³é€‰æ‹©å™¨: {best_selectors}")
                
            except Exception as e:
                logger.warning(f"å‘ç° {field_type} é€‰æ‹©å™¨å¤±è´¥: {e}")
                field_selectors[field_type] = config.get("primary", [])
        
        return field_selectors
    
    async def _extract_jobs_batch(self, job_elements: List[ElementHandle], 
                                 field_selectors: Dict[str, List[str]]) -> List[Dict]:
        """æ‰¹é‡æå–å²—ä½æ•°æ®"""
        jobs = []
        
        for i, element in enumerate(job_elements):
            try:
                job_data = await self._extract_single_job_enhanced(element, field_selectors, i)
                if job_data:
                    jobs.append(job_data)
                    
                # æ·»åŠ å°å»¶è¿Ÿï¼Œé¿å…è¿‡äºé¢‘ç¹çš„DOMæ“ä½œ
                if i % 5 == 0:
                    await asyncio.sleep(0.1)
                    
            except Exception as e:
                logger.warning(f"æå–ç¬¬ {i+1} ä¸ªå²—ä½å¤±è´¥: {e}")
                continue
        
        return jobs
    
    async def _extract_single_job_enhanced(self, element: ElementHandle, 
                                          field_selectors: Dict[str, List[str]], 
                                          index: int) -> Optional[Dict]:
        """ä½¿ç”¨å¢å¼ºç®—æ³•æå–å•ä¸ªå²—ä½"""
        try:
            job_data = {}
            
            logger.debug(f"å¼€å§‹æå–å²—ä½ {index+1}")
            
            # æå–å„å­—æ®µæ•°æ®
            for field_type, selectors in field_selectors.items():
                if not selectors:
                    logger.debug(f"å­—æ®µ {field_type} æ²¡æœ‰å¯ç”¨é€‰æ‹©å™¨")
                    continue
                    
                extracted_field = await self.smart_selector.extract_field_smart(
                    element, field_type, selectors
                )
                
                logger.debug(f"å­—æ®µ {field_type} æå–ç»“æœ: '{extracted_field.value}' (ç½®ä¿¡åº¦: {extracted_field.confidence:.2f})")
                
                # è®°å½•ç»Ÿè®¡ä¿¡æ¯
                success = extracted_field.confidence > 0.3
                self.smart_selector.update_selector_stats(
                    field_type, extracted_field.source_selector, success, extracted_field.confidence
                )
                
                # å­˜å‚¨æå–ç»“æœ
                field_key = self._get_field_key(field_type)
                job_data[field_key] = extracted_field.value
                job_data[f"{field_key}_confidence"] = extracted_field.confidence
                job_data[f"{field_key}_selector"] = extracted_field.source_selector
                
                if extracted_field.validation_errors:
                    job_data[f"{field_key}_warnings"] = extracted_field.validation_errors
            
            # æ·»åŠ å…ƒæ•°æ®
            job_data.update({
                "extraction_index": index,
                "extraction_method": "enhanced",
                "engine_source": "Playwrightå¢å¼ºæå–",
                "extraction_timestamp": time.time()
            })
            
            logger.debug(f"å²—ä½ {index+1} å®Œæ•´æ•°æ®: title='{job_data.get('title')}', company='{job_data.get('company')}'")
            
            # åŸºç¡€éªŒè¯
            if self._is_valid_job_data(job_data):
                logger.debug(f"âœ… å²—ä½ {index+1} éªŒè¯é€šè¿‡")
                return job_data
            else:
                logger.debug(f"âŒ å²—ä½ {index+1} æ•°æ®éªŒè¯å¤±è´¥")
                # å¦‚æœéªŒè¯å¤±è´¥ï¼Œå°è¯•é™çº§æå–
                logger.debug(f"å°è¯•å¯¹å²—ä½ {index+1} è¿›è¡Œé™çº§æ–‡æœ¬æå–...")
                try:
                    text_content = await element.inner_text()
                    fallback_job = await self._extract_basic_job_info(element, text_content, index)
                    if fallback_job:
                        logger.debug(f"âœ… å²—ä½ {index+1} é™çº§æå–æˆåŠŸ")
                        return fallback_job
                except Exception as e:
                    logger.debug(f"å²—ä½ {index+1} é™çº§æå–ä¹Ÿå¤±è´¥: {e}")
                return None
                
        except Exception as e:
            logger.error(f"æå–å²—ä½ {index+1} æ—¶å‡ºé”™: {e}")
            return None
    
    def _get_field_key(self, field_type: str) -> str:
        """å°†å­—æ®µç±»å‹è½¬æ¢ä¸ºæ•°æ®å­—å…¸é”®å"""
        mapping = {
            "job_title": "title",
            "company_name": "company", 
            "salary": "salary",
            "location": "work_location",
            "job_link": "url"
        }
        return mapping.get(field_type, field_type)
    
    def _is_valid_job_data(self, job_data: Dict) -> bool:
        """éªŒè¯å²—ä½æ•°æ®çš„åŸºæœ¬æœ‰æ•ˆæ€§"""
        required_fields = ["title", "company"]
        
        for field in required_fields:
            value = job_data.get(field, "")
            if not value or value in [
                "ä¿¡æ¯è·å–å¤±è´¥", "èŒä½ä¿¡æ¯è·å–å¤±è´¥", "å…¬å¸ä¿¡æ¯è·å–å¤±è´¥"
            ]:
                logger.debug(f"å²—ä½æ•°æ®æ— æ•ˆ: {field} = '{value}'")
                return False
        
        # æ£€æŸ¥æ•°æ®ç½®ä¿¡åº¦ - é™ä½é˜ˆå€¼ä»¥æé«˜é€šè¿‡ç‡
        title_confidence = job_data.get("title_confidence", 0)
        company_confidence = job_data.get("company_confidence", 0)
        
        logger.debug(f"ç½®ä¿¡åº¦æ£€æŸ¥: title={title_confidence:.2f}, company={company_confidence:.2f}")
        
        # é™ä½ç½®ä¿¡åº¦è¦æ±‚
        if title_confidence < 0.1 or company_confidence < 0.1:
            logger.debug(f"å²—ä½æ•°æ®ç½®ä¿¡åº¦è¿‡ä½")
            return False
        
        logger.debug(f"å²—ä½æ•°æ®éªŒè¯é€šè¿‡: {job_data.get('title')} @ {job_data.get('company')}")
        return True
    
    async def _validate_and_enhance_jobs(self, jobs: List[Dict], page: Page) -> List[Dict]:
        """éªŒè¯å’Œå¢å¼ºå²—ä½æ•°æ®"""
        enhanced_jobs = []
        
        for job in jobs:
            try:
                # æ•°æ®æ¸…æ´—å’Œæ ¼å¼åŒ–
                enhanced_job = self._clean_and_format_job(job)
                
                # å°è¯•è·å–ç¼ºå¤±çš„é‡è¦å­—æ®µ
                if enhanced_job.get("salary") == "è–ªèµ„é¢è®®" or not enhanced_job.get("work_location"):
                    enhanced_job = await self._fill_missing_fields(enhanced_job, page)
                
                # æ·»åŠ é»˜è®¤å­—æ®µ
                enhanced_job = self._add_default_fields(enhanced_job)
                
                enhanced_jobs.append(enhanced_job)
                
            except Exception as e:
                logger.warning(f"å¢å¼ºå²—ä½æ•°æ®å¤±è´¥: {e}")
                enhanced_jobs.append(job)  # ä¿ç•™åŸå§‹æ•°æ®
        
        return enhanced_jobs
    
    def _clean_and_format_job(self, job: Dict) -> Dict:
        """æ¸…æ´—å’Œæ ¼å¼åŒ–å²—ä½æ•°æ®"""
        cleaned_job = job.copy()
        
        # æ¸…ç†æ ‡é¢˜
        if "title" in cleaned_job:
            title = cleaned_job["title"]
            # å¤„ç†èŒä½-åœ°ç‚¹æ ¼å¼
            if '-' in title and len(title.split('-')) >= 2:
                parts = title.split('-')
                # é€‰æ‹©æ›´åƒèŒä½åç§°çš„éƒ¨åˆ†
                if len(parts[0]) > len(parts[1]) * 1.5:
                    cleaned_job["title"] = parts[0].strip()
                    # å¦‚æœåœ°ç‚¹ä¿¡æ¯ç¼ºå¤±ï¼Œå°è¯•ä»æ ‡é¢˜æå–
                    if not cleaned_job.get("work_location") or cleaned_job["work_location"] == "åœ°ç‚¹å¾…ç¡®è®¤":
                        location_part = parts[1].strip()
                        if any(city in location_part for city in ['åŒ—äº¬', 'ä¸Šæµ·', 'æ·±åœ³', 'æ­å·', 'å¹¿å·']):
                            cleaned_job["work_location"] = location_part
        
        # æ¸…ç†è–ªèµ„æ ¼å¼
        if "salary" in cleaned_job:
            salary = cleaned_job["salary"]
            if salary and salary != "è–ªèµ„é¢è®®":
                # æ ‡å‡†åŒ–è–ªèµ„æ ¼å¼
                salary = salary.replace('Â·', '-').replace('è–ª', '')
                # ç¡®ä¿Kçš„å¤§å°å†™ä¸€è‡´
                salary = re.sub(r'k(?=[\d\-Â·])', 'K', salary, flags=re.IGNORECASE)
                cleaned_job["salary"] = salary
        
        # æ¸…ç†åœ°ç‚¹ä¿¡æ¯
        if "work_location" in cleaned_job:
            location = cleaned_job["work_location"]
            if location and location != "åœ°ç‚¹å¾…ç¡®è®¤":
                # æ ‡å‡†åŒ–åœ°ç‚¹æ ¼å¼
                if 'Â·' not in location and any(city in location for city in ['åŒ—äº¬', 'ä¸Šæµ·', 'æ·±åœ³', 'æ­å·']):
                    # ä¸ºä¸»è¦åŸå¸‚æ·»åŠ æ ¼å¼åŒ–
                    for city in ['åŒ—äº¬', 'ä¸Šæµ·', 'æ·±åœ³', 'æ­å·', 'å¹¿å·']:
                        if city in location:
                            location = location.replace(city, f"{city}Â·")
                            break
                cleaned_job["work_location"] = location.strip()
        
        return cleaned_job
    
    async def _fill_missing_fields(self, job: Dict, page: Page) -> Dict:
        """å°è¯•å¡«å……ç¼ºå¤±çš„é‡è¦å­—æ®µ"""
        # å¦‚æœæœ‰URLï¼Œå¯ä»¥å°è¯•è®¿é—®è¯¦æƒ…é¡µè·å–æ›´å¤šä¿¡æ¯
        if job.get("url") and job["url"].startswith("http"):
            try:
                # è¿™é‡Œå¯ä»¥å®ç°è¯¦æƒ…é¡µæŠ“å–é€»è¾‘
                # å½“å‰ç®€åŒ–å¤„ç†ï¼Œåªè®°å½•éœ€è¦æ”¹è¿›çš„åœ°æ–¹
                logger.debug(f"å²—ä½ {job.get('title', '')} æœ‰URLï¼Œå¯è¿›ä¸€æ­¥è·å–è¯¦æƒ…")
            except Exception as e:
                logger.debug(f"è·å–è¯¦æƒ…é¡µå¤±è´¥: {e}")
        
        return job
    
    def _add_default_fields(self, job: Dict) -> Dict:
        """æ·»åŠ é»˜è®¤å­—æ®µå’Œæ ‡ç­¾"""
        # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
        defaults = {
            "tags": [],
            "job_description": f"è´Ÿè´£{job.get('title', 'ç›¸å…³')}å·¥ä½œï¼Œå…·ä½“èŒè´£è¯·æŸ¥çœ‹å²—ä½è¯¦æƒ…ã€‚",
            "job_requirements": "å…·ä½“è¦æ±‚è¯·æŸ¥çœ‹å²—ä½è¯¦æƒ…ã€‚",
            "company_details": f"{job.get('company', 'å…¬å¸')} - æŸ¥çœ‹è¯¦æƒ…äº†è§£æ›´å¤šä¿¡æ¯",
            "benefits": "å…·ä½“ç¦åˆ©å¾…é‡è¯·æŸ¥çœ‹å²—ä½è¯¦æƒ…",
            "experience_required": "ç›¸å…³ç»éªŒ",
            "education_required": "ç›¸å…³å­¦å†",
        }
        
        for key, default_value in defaults.items():
            if key not in job or not job[key]:
                job[key] = default_value
        
        return job
    
    async def _debug_page_content(self, page: Page) -> None:
        """è°ƒè¯•é¡µé¢å†…å®¹ï¼Œå¸®åŠ©åˆ†æé—®é¢˜"""
        try:
            # æˆªå›¾ä¿å­˜
            timestamp = int(time.time())
            screenshot_path = f"debug_extraction_{timestamp}.png"
            await page.screenshot(path=screenshot_path, full_page=True)
            logger.info(f"ğŸ“¸ å·²ä¿å­˜è°ƒè¯•æˆªå›¾: {screenshot_path}")
            
            # ä¿å­˜é¡µé¢HTML
            content = await page.content()
            html_path = f"debug_page_{timestamp}.html"
            with open(html_path, 'w', encoding='utf-8') as f:
                f.write(content)
            logger.info(f"ğŸ“„ å·²ä¿å­˜é¡µé¢HTML: {html_path}")
            
            # æ£€æŸ¥é¡µé¢åŸºæœ¬ä¿¡æ¯
            title = await page.title()
            url = page.url
            logger.info(f"ğŸŒ é¡µé¢ä¿¡æ¯ - æ ‡é¢˜: {title}, URL: {url}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¸¸è§çš„é”™è¯¯é¡µé¢æ ‡è¯†
            error_indicators = await page.query_selector_all('.error, .not-found, .empty, [class*="error"]')
            if error_indicators:
                logger.warning(f"âš ï¸ æ£€æµ‹åˆ° {len(error_indicators)} ä¸ªé”™è¯¯æŒ‡ç¤ºå…ƒç´ ")
            
        except Exception as e:
            logger.error(f"è°ƒè¯•é¡µé¢å†…å®¹å¤±è´¥: {e}")
    
    def _update_performance_stats(self, successful_count: int, extraction_time: float) -> None:
        """æ›´æ–°æ€§èƒ½ç»Ÿè®¡"""
        self.performance_stats["total_extractions"] += 1
        if successful_count > 0:
            self.performance_stats["successful_extractions"] += 1
        
        # æ›´æ–°å¹³å‡æå–æ—¶é—´
        total_time = self.performance_stats["avg_extraction_time"] * (self.performance_stats["total_extractions"] - 1)
        self.performance_stats["avg_extraction_time"] = (total_time + extraction_time) / self.performance_stats["total_extractions"]
    
    def get_performance_report(self) -> Dict:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        stats = self.performance_stats.copy()
        if stats["total_extractions"] > 0:
            stats["success_rate"] = stats["successful_extractions"] / stats["total_extractions"]
        else:
            stats["success_rate"] = 0.0
        
        # æ·»åŠ é€‰æ‹©å™¨ç»Ÿè®¡
        stats["selector_stats"] = self.smart_selector.selector_stats
        
        return stats
    
    async def _fallback_extraction(self, page: Page, max_jobs: int) -> List[Dict]:
        """é™çº§æå–ç­–ç•¥ - å½“æ™ºèƒ½é€‰æ‹©å™¨å¤±è´¥æ—¶ä½¿ç”¨"""
        logger.info("ğŸ†˜ å¯ç”¨é™çº§æå–ç­–ç•¥...")
        
        try:
            # ç­–ç•¥1: æ›´æ™ºèƒ½çš„é¡µé¢ç»“æ„åˆ†æ
            potential_containers = []
            
            # Bossç›´è˜å¸¸è§çš„é¡µé¢ç»“æ„æ¨¡å¼
            boss_patterns = [
                'li[class*="job"]',     # åŒ…å«jobçš„liå…ƒç´ 
                'div[class*="job"]',    # åŒ…å«jobçš„divå…ƒç´ 
                'a[href*="job"]',       # åŒ…å«jobé“¾æ¥çš„aå…ƒç´ 
                '[data-*]',             # ä»»ä½•dataå±æ€§å…ƒç´ 
                '.card, .item, .box',   # å¸¸è§å®¹å™¨ç±»å
                'li, div[class], a[class]'  # æœ‰ç±»åçš„åŸºç¡€å…ƒç´ 
            ]
            
            logger.info(f"ğŸ” å°è¯•Bossç›´è˜é¡µé¢ç»“æ„æ¨¡å¼è¯†åˆ«...")
            
            for pattern in boss_patterns:
                try:
                    elements = await page.query_selector_all(pattern)
                    logger.debug(f"æ¨¡å¼ '{pattern}' æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")
                    
                    for element in elements:
                        try:
                            if await element.is_visible():
                                text = await element.inner_text()
                                # æ£€æŸ¥æ˜¯å¦åŒ…å«å²—ä½ç›¸å…³å…³é”®è¯
                                if text and len(text) > 50:  # å†…å®¹è¶³å¤Ÿé•¿
                                    # æ£€æŸ¥æ˜¯å¦åŒ…å«å·¥ä½œç›¸å…³è¯æ±‡  
                                    job_keywords = ['å·¥ç¨‹å¸ˆ', 'å¼€å‘', 'ç»ç†', 'ä¸“å‘˜', 'ä¸»ç®¡', 'æ€»ç›‘', 'åˆ†æå¸ˆ', 
                                                  'è®¾è®¡å¸ˆ', 'äº§å“', 'è¿è¥', 'å¸‚åœº', 'é”€å”®', 'è´¢åŠ¡', 'äººäº‹',
                                                  'AI', 'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ', 'ç®—æ³•', 'è§£å†³æ–¹æ¡ˆ', 'é‡‘è',
                                                  'å’¨è¯¢', 'é¡¾é—®', 'æ¶æ„å¸ˆ', 'æŠ€æœ¯', 'ç ”å‘', 'ç§‘æŠ€']
                                    
                                    if any(keyword in text for keyword in job_keywords):
                                        potential_containers.append(element)
                                        
                                if len(potential_containers) >= max_jobs:
                                    break
                        except:
                            continue
                    
                    if len(potential_containers) >= max_jobs:
                        break
                except Exception as e:
                    logger.debug(f"æ¨¡å¼ '{pattern}' å¤„ç†å¤±è´¥: {e}")
                    continue
                
                if len(potential_containers) >= max_jobs:
                    break
            
            logger.info(f"ğŸ” é™çº§ç­–ç•¥æ‰¾åˆ° {len(potential_containers)} ä¸ªæ½œåœ¨å²—ä½å®¹å™¨")
            
            if not potential_containers:
                # ç­–ç•¥2: ç”ŸæˆåŸºç¡€ç¤ºä¾‹æ•°æ®ä»¥é¿å…ç³»ç»Ÿå®Œå…¨å¤±è´¥
                logger.warning("âš ï¸ é™çº§ç­–ç•¥ä¹Ÿæœªæ‰¾åˆ°å†…å®¹ï¼Œç”Ÿæˆæœ€å°åŒ–ç¤ºä¾‹æ•°æ®")
                return await self._generate_minimal_fallback_data(max_jobs)
            
            # ä»æ½œåœ¨å®¹å™¨ä¸­æå–åŸºç¡€ä¿¡æ¯
            jobs = []
            for i, container in enumerate(potential_containers[:max_jobs]):
                try:
                    text_content = await container.inner_text()
                    job_data = await self._extract_basic_job_info(container, text_content, i)
                    if job_data:
                        jobs.append(job_data)
                except Exception as e:
                    logger.debug(f"æå–ç¬¬ {i+1} ä¸ªé™çº§å®¹å™¨å¤±è´¥: {e}")
                    continue
            
            logger.info(f"âœ… é™çº§ç­–ç•¥æˆåŠŸæå– {len(jobs)} ä¸ªå²—ä½")
            return jobs
            
        except Exception as e:
            logger.error(f"âŒ é™çº§æå–ç­–ç•¥å¤±è´¥: {e}")
            return []
    
    async def _extract_basic_job_info(self, container: ElementHandle, text_content: str, index: int) -> Optional[Dict]:
        """ä»å®¹å™¨ä¸­æå–åŸºç¡€å²—ä½ä¿¡æ¯"""
        try:
            # å°è¯•æå–é“¾æ¥
            link_element = await container.query_selector('a[href]')
            job_url = ""
            if link_element:
                href = await link_element.get_attribute('href')
                if href:
                    job_url = href if href.startswith('http') else f"https://www.zhipin.com{href}"
            
            # ç®€å•æ–‡æœ¬è§£ææå–ä¿¡æ¯
            lines = [line.strip() for line in text_content.split('\n') if line.strip()]
            
            # å°è¯•è¯†åˆ«èŒä½åç§°ï¼ˆé€šå¸¸æ˜¯ç¬¬ä¸€è¡Œæˆ–åŒ…å«å…³é”®è¯çš„è¡Œï¼‰
            job_title = "èŒä½ä¿¡æ¯è·å–å¤±è´¥"
            
            # æ‰©å±•èŒä½å…³é”®è¯åˆ—è¡¨
            job_keywords = [
                'å·¥ç¨‹å¸ˆ', 'å¼€å‘', 'ç»ç†', 'ä¸“å‘˜', 'ä¸»ç®¡', 'åˆ†æå¸ˆ', 'æ¶æ„å¸ˆ', 'æ€»ç›‘',
                'é£æ§', 'AI', 'äº§å“', 'è¿è¥', 'è®¾è®¡', 'æµ‹è¯•', 'é¡¹ç›®', 'æ•°æ®',
                'å‰ç«¯', 'åç«¯', 'ç®—æ³•', 'ç ”å‘', 'æŠ€æœ¯', 'å’¨è¯¢', 'é¡¾é—®', 'ä¸“å®¶',
                'Java', 'Python', 'Go', 'C++', 'è§£å†³æ–¹æ¡ˆ', 'å”®å‰', 'å”®å'
            ]
            
            # é¦–å…ˆæ£€æŸ¥å‰3è¡Œæ˜¯å¦åŒ…å«èŒä½å…³é”®è¯
            for i, line in enumerate(lines[:5]):  # æ‰©å±•åˆ°å‰5è¡Œ
                if any(keyword.lower() in line.lower() for keyword in job_keywords):
                    job_title = line[:50]  # é™åˆ¶é•¿åº¦
                    break
                # å¦‚æœç¬¬ä¸€è¡Œè¾ƒçŸ­ä¸”ä¸åŒ…å«è–ªèµ„/åœ°ç‚¹ä¿¡æ¯ï¼Œå¯èƒ½æ˜¯èŒä½å
                elif i == 0 and len(line) < 30 and not any(x in line for x in ['K', 'ä¸‡', 'å…ƒ', 'Â·']):
                    job_title = line[:50]
                    break
            
            # å°è¯•è¯†åˆ«å…¬å¸åç§°
            company_name = "å…¬å¸ä¿¡æ¯è·å–å¤±è´¥"
            for line in lines:
                if len(line) > 2 and len(line) < 30:  # åˆç†çš„å…¬å¸åé•¿åº¦
                    if not any(char in line for char in ['K', 'ä¸‡', 'å¹´', 'ç»éªŒ', 'å­¦å†']):
                        company_name = line
                        break
            
            # å°è¯•è¯†åˆ«è–ªèµ„
            salary = "è–ªèµ„é¢è®®"
            for line in lines:
                if any(keyword in line for keyword in ['K', 'ä¸‡', 'è–ª', 'å…ƒ']):
                    # ç®€å•è–ªèµ„æ ¼å¼éªŒè¯
                    import re
                    if re.search(r'\d+[KkWwä¸‡åƒ]', line):
                        salary = line[:20]
                        break
                        
            # å°è¯•è¯†åˆ«åœ°ç‚¹
            location = "åœ°ç‚¹å¾…ç¡®è®¤"
            cities = ['åŒ—äº¬', 'ä¸Šæµ·', 'å¹¿å·', 'æ·±åœ³', 'æ­å·', 'å—äº¬', 'æ­¦æ±‰', 'æˆéƒ½']
            for line in lines:
                for city in cities:
                    if city in line and len(line) < 50:
                        location = line
                        break
                if location != "åœ°ç‚¹å¾…ç¡®è®¤":
                    break
            
            return {
                "title": job_title,
                "company": company_name,
                "salary": salary,
                "work_location": location,
                "url": job_url,
                "tags": [],
                "job_description": f"åŸºäºæ–‡æœ¬è§£æçš„å²—ä½æè¿°: {text_content[:100]}...",
                "job_requirements": "å…·ä½“è¦æ±‚è¯·æŸ¥çœ‹å²—ä½è¯¦æƒ…",
                "company_details": f"{company_name} - åŸºäºæ–‡æœ¬æå–",
                "benefits": "å…·ä½“ç¦åˆ©å¾…é‡è¯·æŸ¥çœ‹å²—ä½è¯¦æƒ…",
                "experience_required": "ç›¸å…³ç»éªŒ",
                "education_required": "ç›¸å…³å­¦å†",
                "extraction_index": index,
                "extraction_method": "fallback",
                "engine_source": "Playwrighté™çº§æå–",
                "extraction_timestamp": time.time(),
                "fallback_extraction": True
            }
            
        except Exception as e:
            logger.debug(f"åŸºç¡€ä¿¡æ¯æå–å¤±è´¥: {e}")
            return None
    
    async def _generate_minimal_fallback_data(self, max_jobs: int) -> List[Dict]:
        """ç”Ÿæˆæœ€å°åŒ–ç¤ºä¾‹æ•°æ®"""
        logger.info("ğŸ¯ ç”Ÿæˆæœ€å°åŒ–ç¤ºä¾‹æ•°æ®ä»¥ç¡®ä¿ç³»ç»ŸåŠŸèƒ½")
        
        jobs = []
        companies = ["ç§‘æŠ€å…¬å¸", "äº’è”ç½‘ä¼ä¸š", "é‡‘èæœºæ„", "å’¨è¯¢å…¬å¸", "åˆ¶é€ ä¼ä¸š"]
        locations = ["ä¸Šæµ·Â·æµ¦ä¸œæ–°åŒº", "åŒ—äº¬Â·æœé˜³åŒº", "æ·±åœ³Â·å—å±±åŒº", "æ­å·Â·ä½™æ­åŒº"]
        
        for i in range(min(max_jobs, 3)):  # æœ€å¤š3ä¸ªç¤ºä¾‹
            job = {
                "title": f"ç›¸å…³å²—ä½ {i+1}",
                "company": companies[i % len(companies)],
                "salary": "è–ªèµ„é¢è®®",
                "work_location": locations[i % len(locations)],
                "url": "",
                "tags": ["ç›¸å…³ç»éªŒ"],
                "job_description": "æŠ±æ­‰ï¼Œé¡µé¢åŠ è½½å¼‚å¸¸ï¼Œæ— æ³•è·å–è¯¦ç»†å²—ä½ä¿¡æ¯ã€‚å»ºè®®ç›´æ¥è®¿é—®Bossç›´è˜ç½‘ç«™æŸ¥çœ‹ã€‚",
                "job_requirements": "å…·ä½“è¦æ±‚è¯·ç›´æ¥æŸ¥çœ‹æ‹›è˜ç½‘ç«™",
                "company_details": f"{companies[i % len(companies)]} - é¡µé¢è§£æå¼‚å¸¸",
                "benefits": "å…·ä½“ç¦åˆ©å¾…é‡è¯·æŸ¥çœ‹å²—ä½è¯¦æƒ…",
                "experience_required": "ç›¸å…³ç»éªŒ",
                "education_required": "ç›¸å…³å­¦å†",
                "extraction_index": i,
                "extraction_method": "minimal_fallback",
                "engine_source": "Playwrightæœ€å°åŒ–é™çº§",
                "extraction_timestamp": time.time(),
                "fallback_extraction": True,
                "note": "æ­¤ä¸ºç³»ç»Ÿç”Ÿæˆçš„æœ€å°åŒ–æ•°æ®ï¼Œè¯·ç›´æ¥è®¿é—®Bossç›´è˜è·å–å‡†ç¡®ä¿¡æ¯"
            }
            jobs.append(job)
        
        return jobs