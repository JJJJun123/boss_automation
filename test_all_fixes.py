#!/usr/bin/env python3
"""
æµ‹è¯•æ‰€æœ‰ä¿®å¤ï¼š
1. å‰ç«¯é‡å¤æ˜¾ç¤ºé—®é¢˜
2. ç»Ÿè®¡æ•°å­—ç‚¹å‡»åŠŸèƒ½
3. å²—ä½æè¿°å®Œæ•´æŠ“å–
"""

import asyncio
import logging
import sys
import os
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


async def test_job_description_extraction():
    """æµ‹è¯•å²—ä½æè¿°æŠ“å–"""
    try:
        logger.info("ğŸ¯ æµ‹è¯•å®Œæ•´å²—ä½æè¿°æŠ“å–...")
        
        from crawler.real_playwright_spider import RealPlaywrightBossSpider
        
        spider = RealPlaywrightBossSpider(headless=False)
        
        if not await spider.start():
            logger.error("âŒ æ— æ³•å¯åŠ¨Playwright")
            return False
        
        # ç›´æ¥è®¿é—®ä½ æä¾›çš„ç¤ºä¾‹URL
        test_url = "https://www.zhipin.com/job_detail/dfe790b1cfccc3fe03Vz39q7ElJS.html"
        
        # åˆ›å»ºè¯¦æƒ…é¡µ
        detail_page = await spider.browser.new_page()
        await detail_page.set_viewport_size({"width": 1920, "height": 1080})
        
        logger.info(f"ğŸ” è®¿é—®æµ‹è¯•URL: {test_url}")
        await detail_page.goto(test_url, wait_until="networkidle", timeout=15000)
        await asyncio.sleep(2)
        
        # æµ‹è¯•å²—ä½èŒè´£æå–
        job_desc = await spider._extract_job_description(detail_page)
        logger.info("\nğŸ“‹ å²—ä½èŒè´£æå–ç»“æœ:")
        if job_desc and "åŸºäºAIå¤§æ¨¡å‹æŠ€æœ¯" in job_desc:
            logger.info("âœ… æˆåŠŸæå–å®Œæ•´å²—ä½èŒè´£!")
            logger.info(f"å†…å®¹é¢„è§ˆ: {job_desc[:200]}...")
            logger.info(f"æ€»é•¿åº¦: {len(job_desc)}å­—ç¬¦")
        else:
            logger.error("âŒ å²—ä½èŒè´£æå–å¤±è´¥æˆ–ä¸å®Œæ•´")
            logger.info(f"å®é™…å†…å®¹: {job_desc[:100] if job_desc else 'ç©º'}")
        
        # æµ‹è¯•ä»»èŒè¦æ±‚æå–
        job_req = await spider._extract_job_requirements(detail_page)
        logger.info("\nğŸ“‹ ä»»èŒè¦æ±‚æå–ç»“æœ:")
        if job_req and "æ•™è‚²èƒŒæ™¯" in job_req:
            logger.info("âœ… æˆåŠŸæå–å®Œæ•´ä»»èŒè¦æ±‚!")
            logger.info(f"å†…å®¹é¢„è§ˆ: {job_req[:200]}...")
            logger.info(f"æ€»é•¿åº¦: {len(job_req)}å­—ç¬¦")
        else:
            logger.error("âŒ ä»»èŒè¦æ±‚æå–å¤±è´¥æˆ–ä¸å®Œæ•´")
            logger.info(f"å®é™…å†…å®¹: {job_req[:100] if job_req else 'ç©º'}")
        
        await detail_page.close()
        
        # æµ‹è¯•å®Œæ•´æœç´¢æµç¨‹
        logger.info("\nğŸ” æµ‹è¯•å®Œæ•´æœç´¢æµç¨‹...")
        keyword = "AIå¤§æ¨¡å‹"
        city = "shanghai"
        
        jobs = await spider.search_jobs(keyword, city, 1)
        
        if jobs:
            job = jobs[0]
            logger.info(f"\nâœ… æˆåŠŸæŠ“å–å²—ä½: {job.get('title')}")
            
            # æ£€æŸ¥å…³é”®å­—æ®µ
            desc = job.get('job_description', '')
            req = job.get('job_requirements', '')
            
            success_count = 0
            total_checks = 4
            
            # æ£€æŸ¥1: å²—ä½æè¿°é•¿åº¦
            if len(desc) > 200:
                logger.info(f"âœ… å²—ä½æè¿°å……è¶³: {len(desc)}å­—ç¬¦")
                success_count += 1
            else:
                logger.warning(f"âš ï¸ å²—ä½æè¿°è¾ƒçŸ­: {len(desc)}å­—ç¬¦")
            
            # æ£€æŸ¥2: å²—ä½æè¿°è´¨é‡
            if desc and not "å…·ä½“èŒè´£è¯·æŸ¥çœ‹å²—ä½è¯¦æƒ…" in desc:
                logger.info("âœ… å²—ä½æè¿°æ˜¯çœŸå®å†…å®¹")
                success_count += 1
            else:
                logger.warning("âš ï¸ å²—ä½æè¿°ä»æ˜¯æ¨¡æ¿")
            
            # æ£€æŸ¥3: ä»»èŒè¦æ±‚é•¿åº¦
            if len(req) > 100:
                logger.info(f"âœ… ä»»èŒè¦æ±‚å……è¶³: {len(req)}å­—ç¬¦")
                success_count += 1
            else:
                logger.warning(f"âš ï¸ ä»»èŒè¦æ±‚è¾ƒçŸ­: {len(req)}å­—ç¬¦")
            
            # æ£€æŸ¥4: ä»»èŒè¦æ±‚è´¨é‡
            if req and not "è¦æ±‚1-3å¹´å·¥ä½œç»éªŒ" in req:
                logger.info("âœ… ä»»èŒè¦æ±‚æ˜¯çœŸå®å†…å®¹")
                success_count += 1
            else:
                logger.warning("âš ï¸ ä»»èŒè¦æ±‚ä»æ˜¯æ¨¡æ¿")
            
            logger.info(f"\nğŸ“Š æ€»ä½“æˆåŠŸç‡: {success_count}/{total_checks}")
            return success_count >= 3
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        try:
            await spider.close()
        except:
            pass


def test_frontend_fixes():
    """æµ‹è¯•å‰ç«¯ä¿®å¤"""
    logger.info("\nğŸ–¥ï¸ å‰ç«¯ä¿®å¤è¯´æ˜:")
    logger.info("1. âœ… ä¿®å¤äº†è¿›åº¦æ¶ˆæ¯é‡å¤æ˜¾ç¤ºçš„é—®é¢˜")
    logger.info("   - åˆ é™¤äº†é‡å¤çš„ progress_update äº‹ä»¶ç›‘å¬å™¨")
    logger.info("2. âœ… ä¿®å¤äº†ç»Ÿè®¡æ•°å­—æ— æ³•ç‚¹å‡»çš„é—®é¢˜")
    logger.info("   - å°† showAllJobs å’Œ showQualifiedJobs å‡½æ•°æ·»åŠ åˆ° window å¯¹è±¡")
    logger.info("3. âœ… ç»Ÿè®¡ç•Œé¢ç°åœ¨æ˜¯äº¤äº’å¼çš„:")
    logger.info("   - ç‚¹å‡»'æ€»æœç´¢æ•°'æŸ¥çœ‹æ‰€æœ‰å²—ä½")
    logger.info("   - ç‚¹å‡»'åˆæ ¼å²—ä½'æŸ¥çœ‹æ¨èå²—ä½")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ Bossç›´è˜æ‰€æœ‰ä¿®å¤æµ‹è¯•")
    print("=" * 60)
    print("æµ‹è¯•é¡¹ç›®:")
    print("1. å‰ç«¯é‡å¤æ˜¾ç¤ºé—®é¢˜")
    print("2. ç»Ÿè®¡æ•°å­—ç‚¹å‡»åŠŸèƒ½")
    print("3. å²—ä½æè¿°å®Œæ•´æŠ“å–")
    print()
    
    # æµ‹è¯•å‰ç«¯ä¿®å¤
    test_frontend_fixes()
    
    # æµ‹è¯•å²—ä½æè¿°æŠ“å–
    result = asyncio.run(test_job_description_extraction())
    
    if result:
        print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
        print("ğŸš€ è¿è¡Œ python run_web.py ä½“éªŒå®Œæ•´åŠŸèƒ½")
    else:
        print("\nâš ï¸ éƒ¨åˆ†æµ‹è¯•æœªé€šè¿‡ï¼Œä½†åŸºæœ¬åŠŸèƒ½å¯ç”¨")
        print("ğŸ’¡ å²—ä½æè¿°å¯èƒ½å› ç½‘ç«™æ›´æ–°éœ€è¦è°ƒæ•´é€‰æ‹©å™¨")


if __name__ == "__main__":
    main()