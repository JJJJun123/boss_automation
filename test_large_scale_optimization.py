#!/usr/bin/env python3
"""
å¤§è§„æ¨¡æŠ“å–ä¼˜åŒ–æµ‹è¯•è„šæœ¬
éªŒè¯50-100+å²—ä½æŠ“å–èƒ½åŠ›ã€AIå²—ä½è¦æ±‚æ€»ç»“å’Œæˆæœ¬æ§åˆ¶åŠŸèƒ½
"""

import os
import sys
import asyncio
import logging
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config.config_manager import ConfigManager
from crawler.unified_crawler_interface import unified_search_jobs
from analyzer.job_analyzer import JobAnalyzer
from analyzer.job_requirement_summarizer import JobRequirementSummarizer

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('logs/large_scale_test.log')
    ]
)
logger = logging.getLogger(__name__)


class LargeScaleOptimizationTester:
    """å¤§è§„æ¨¡ä¼˜åŒ–åŠŸèƒ½æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.config_manager = ConfigManager()
        self.test_results = {}
        
    async def run_comprehensive_test(self):
        """è¿è¡Œç»¼åˆæµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹å¤§è§„æ¨¡æŠ“å–ä¼˜åŒ–ç»¼åˆæµ‹è¯•")
        logger.info("=" * 60)
        
        # æµ‹è¯•é…ç½®
        test_scenarios = [
            {"keyword": "AIç®—æ³•", "city": "shanghai", "max_jobs": 50, "description": "ä¸­ç­‰è§„æ¨¡æµ‹è¯•"},
            {"keyword": "æ•°æ®åˆ†æ", "city": "beijing", "max_jobs": 80, "description": "å¤§è§„æ¨¡æµ‹è¯•"},
        ]
        
        for i, scenario in enumerate(test_scenarios, 1):
            logger.info(f"\nğŸ“‹ æµ‹è¯•åœºæ™¯ {i}/{len(test_scenarios)}: {scenario['description']}")
            logger.info(f"   å…³é”®è¯: {scenario['keyword']}")
            logger.info(f"   åŸå¸‚: {scenario['city']}")
            logger.info(f"   ç›®æ ‡å²—ä½æ•°: {scenario['max_jobs']}")
            
            try:
                await self._test_scenario(f"scenario_{i}", scenario)
            except Exception as e:
                logger.error(f"âŒ æµ‹è¯•åœºæ™¯ {i} å¤±è´¥: {e}")
                self.test_results[f"scenario_{i}"] = {"status": "failed", "error": str(e)}
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        self._generate_test_report()
    
    async def _test_scenario(self, scenario_id: str, scenario: dict):
        """æµ‹è¯•å•ä¸ªåœºæ™¯"""
        start_time = time.time()
        
        # ç¬¬ä¸€é˜¶æ®µï¼šå¤§è§„æ¨¡çˆ¬è™«æµ‹è¯•
        logger.info("ğŸ” é˜¶æ®µ1: å¤§è§„æ¨¡çˆ¬è™«æµ‹è¯•")
        crawl_start = time.time()
        
        jobs = await unified_search_jobs(
            keyword=scenario["keyword"],
            city=scenario["city"],
            max_jobs=scenario["max_jobs"],
            engine="real_playwright"
        )
        
        crawl_time = time.time() - crawl_start
        
        if not jobs:
            raise Exception("çˆ¬è™«æœªè·å–åˆ°ä»»ä½•å²—ä½")
        
        logger.info(f"âœ… çˆ¬è™«å®Œæˆ: {len(jobs)} ä¸ªå²—ä½ï¼Œè€—æ—¶ {crawl_time:.2f}s")
        logger.info(f"ğŸ“Š çˆ¬è™«æ•ˆç‡: {len(jobs)/crawl_time:.1f} å²—ä½/ç§’")
        
        # éªŒè¯å²—ä½æ•°æ®è´¨é‡
        valid_jobs = self._validate_job_data(jobs)
        logger.info(f"ğŸ” æ•°æ®è´¨é‡: {len(valid_jobs)}/{len(jobs)} ä¸ªæœ‰æ•ˆå²—ä½")
        
        # ç¬¬äºŒé˜¶æ®µï¼šAIå²—ä½è¦æ±‚æ€»ç»“æµ‹è¯•
        logger.info("\nğŸ§  é˜¶æ®µ2: AIå²—ä½è¦æ±‚æ€»ç»“æµ‹è¯•")
        summarizer_start = time.time()
        
        summarizer = JobRequirementSummarizer()
        summaries = await summarizer.summarize_batch_jobs(jobs[:20])  # æµ‹è¯•å‰20ä¸ªå²—ä½
        
        summarizer_time = time.time() - summarizer_start
        
        logger.info(f"âœ… AIæ€»ç»“å®Œæˆ: {len(summaries)} ä¸ªå²—ä½ï¼Œè€—æ—¶ {summarizer_time:.2f}s")
        
        # è·å–æˆæœ¬ä¼˜åŒ–æŠ¥å‘Š
        cost_report = summarizer.get_cost_savings_report()
        logger.info(f"ğŸ’° æˆæœ¬ä¼˜åŒ–æŠ¥å‘Š:")
        logger.info(f"   ç¼“å­˜å‘½ä¸­ç‡: {cost_report.get('cache_statistics', {}).get('cache_hit_rate', '0%')}")
        logger.info(f"   AIè°ƒç”¨æ¬¡æ•°: {cost_report.get('cache_statistics', {}).get('ai_calls_made', 0)}")
        logger.info(f"   èŠ‚çœæˆæœ¬: {cost_report.get('cost_analysis', {}).get('total_savings', 'Â¥0.00')}")
        
        # ç¬¬ä¸‰é˜¶æ®µï¼šç«¯åˆ°ç«¯åˆ†ææµ‹è¯•
        logger.info("\nğŸ¤– é˜¶æ®µ3: ç«¯åˆ°ç«¯AIåˆ†ææµ‹è¯•")
        analysis_start = time.time()
        
        analyzer = JobAnalyzer("deepseek")
        analyzed_jobs = analyzer.analyze_jobs(jobs[:15])  # æµ‹è¯•å‰15ä¸ªå²—ä½çš„å®Œæ•´åˆ†æ
        
        analysis_time = time.time() - analysis_start
        
        logger.info(f"âœ… å®Œæ•´åˆ†æå®Œæˆ: {len(analyzed_jobs)} ä¸ªå²—ä½ï¼Œè€—æ—¶ {analysis_time:.2f}s")
        
        # éªŒè¯åˆ†æç»“æœè´¨é‡
        analysis_quality = self._validate_analysis_quality(analyzed_jobs)
        
        total_time = time.time() - start_time
        
        # è®°å½•æµ‹è¯•ç»“æœ
        self.test_results[scenario_id] = {
            "status": "completed",
            "scenario": scenario,
            "performance": {
                "total_time": total_time,
                "crawl_time": crawl_time,
                "summarizer_time": summarizer_time,
                "analysis_time": analysis_time,
                "jobs_found": len(jobs),
                "valid_jobs": len(valid_jobs),
                "analyzed_jobs": len(analyzed_jobs),
                "crawl_efficiency": len(jobs) / crawl_time,
                "overall_efficiency": len(analyzed_jobs) / total_time
            },
            "quality": {
                "data_validity_rate": len(valid_jobs) / len(jobs) if jobs else 0,
                "analysis_quality_score": analysis_quality
            },
            "ai_optimization": cost_report
        }
        
        logger.info(f"ğŸ“ˆ åœºæ™¯æ€»ç»“:")
        logger.info(f"   æ€»è€—æ—¶: {total_time:.2f}s")
        logger.info(f"   æ•´ä½“æ•ˆç‡: {len(analyzed_jobs)/total_time:.1f} ä¸ªå®Œæ•´åˆ†æ/ç§’")
        logger.info(f"   æ•°æ®æœ‰æ•ˆç‡: {len(valid_jobs)/len(jobs)*100:.1f}%")
        logger.info(f"   åˆ†æè´¨é‡: {analysis_quality:.1f}/10")
    
    def _validate_job_data(self, jobs: list) -> list:
        """éªŒè¯å²—ä½æ•°æ®è´¨é‡"""
        valid_jobs = []
        
        for job in jobs:
            # æ£€æŸ¥å¿…è¦å­—æ®µ
            if not job.get('title') or not job.get('company'):
                continue
            
            # æ£€æŸ¥å­—æ®µè´¨é‡
            title = job.get('title', '').strip()
            company = job.get('company', '').strip()
            
            if len(title) < 2 or len(title) > 100:
                continue
                
            if len(company) < 2 or len(company) > 100:
                continue
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å²—ä½æè¿°æˆ–è¦æ±‚
            has_description = (
                job.get('job_description') and 
                len(job.get('job_description', '').strip()) > 20
            )
            has_requirements = (
                job.get('job_requirements') and 
                len(job.get('job_requirements', '').strip()) > 20
            )
            
            if has_description or has_requirements:
                valid_jobs.append(job)
        
        return valid_jobs
    
    def _validate_analysis_quality(self, analyzed_jobs: list) -> float:
        """éªŒè¯åˆ†æç»“æœè´¨é‡"""
        if not analyzed_jobs:
            return 0.0
        
        quality_scores = []
        
        for job in analyzed_jobs:
            score = 0
            
            # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†æç»“æœ
            analysis = job.get('analysis', {})
            if analysis:
                score += 2
                
                # æ£€æŸ¥è¯„åˆ†æ˜¯å¦åˆç†
                job_score = analysis.get('score', 0)
                if 0 <= job_score <= 10:
                    score += 2
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ¨èç†ç”±
                if analysis.get('reason') and len(analysis.get('reason', '')) > 10:
                    score += 2
            
            # æ£€æŸ¥æ˜¯å¦æœ‰AIè¦æ±‚æ€»ç»“
            ai_summary = job.get('ai_requirement_summary', {})
            if ai_summary:
                score += 2
                
                # æ£€æŸ¥æ€»ç»“è´¨é‡
                if (ai_summary.get('core_responsibilities') and 
                    ai_summary.get('key_requirements')):
                    score += 2
            
            quality_scores.append(score)
        
        return sum(quality_scores) / len(quality_scores) if quality_scores else 0.0
    
    def _generate_test_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“Š å¤§è§„æ¨¡æŠ“å–ä¼˜åŒ–æµ‹è¯•æŠ¥å‘Š")
        logger.info("=" * 60)
        
        if not self.test_results:
            logger.error("âŒ æ²¡æœ‰å®Œæˆçš„æµ‹è¯•ç»“æœ")
            return
        
        # ç»Ÿè®¡æ€»ä½“æ€§èƒ½
        total_jobs = sum(
            result.get('performance', {}).get('jobs_found', 0) 
            for result in self.test_results.values() 
            if result.get('status') == 'completed'
        )
        
        total_analyzed = sum(
            result.get('performance', {}).get('analyzed_jobs', 0) 
            for result in self.test_results.values() 
            if result.get('status') == 'completed'
        )
        
        avg_crawl_efficiency = sum(
            result.get('performance', {}).get('crawl_efficiency', 0) 
            for result in self.test_results.values() 
            if result.get('status') == 'completed'
        ) / len([r for r in self.test_results.values() if r.get('status') == 'completed'])
        
        avg_quality = sum(
            result.get('quality', {}).get('analysis_quality_score', 0) 
            for result in self.test_results.values() 
            if result.get('status') == 'completed'
        ) / len([r for r in self.test_results.values() if r.get('status') == 'completed'])
        
        logger.info(f"ğŸ¯ æ€»ä½“æ€§èƒ½æŒ‡æ ‡:")
        logger.info(f"   æ€»æŠ“å–å²—ä½æ•°: {total_jobs}")
        logger.info(f"   æ€»åˆ†æå²—ä½æ•°: {total_analyzed}")
        logger.info(f"   å¹³å‡æŠ“å–æ•ˆç‡: {avg_crawl_efficiency:.1f} å²—ä½/ç§’")
        logger.info(f"   å¹³å‡åˆ†æè´¨é‡: {avg_quality:.1f}/10")
        
        # è¯¦ç»†åœºæ™¯ç»“æœ
        logger.info(f"\nğŸ“‹ è¯¦ç»†æµ‹è¯•ç»“æœ:")
        for scenario_id, result in self.test_results.items():
            if result.get('status') == 'completed':
                perf = result['performance']
                quality = result['quality']
                logger.info(f"\n   {scenario_id}: âœ… æˆåŠŸ")
                logger.info(f"     å²—ä½æ•°: {perf['jobs_found']} (æœ‰æ•ˆ: {perf['valid_jobs']})")
                logger.info(f"     æ€»è€—æ—¶: {perf['total_time']:.2f}s")
                logger.info(f"     æŠ“å–æ•ˆç‡: {perf['crawl_efficiency']:.1f} å²—ä½/ç§’")
                logger.info(f"     åˆ†æè´¨é‡: {quality['analysis_quality_score']:.1f}/10")
            else:
                logger.info(f"\n   {scenario_id}: âŒ å¤±è´¥ - {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
        
        # AIæˆæœ¬ä¼˜åŒ–æ€»ç»“
        logger.info(f"\nğŸ’° AIæˆæœ¬ä¼˜åŒ–æ•ˆæœ:")
        for scenario_id, result in self.test_results.items():
            if result.get('status') == 'completed':
                ai_opt = result.get('ai_optimization', {})
                cache_stats = ai_opt.get('cache_statistics', {})
                cost_analysis = ai_opt.get('cost_analysis', {})
                
                logger.info(f"   {scenario_id}:")
                logger.info(f"     ç¼“å­˜å‘½ä¸­ç‡: {cache_stats.get('cache_hit_rate', '0%')}")
                logger.info(f"     èŠ‚çœæˆæœ¬: {cost_analysis.get('total_savings', 'Â¥0.00')}")
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Šåˆ°æ–‡ä»¶
        self._save_detailed_report()
        
        logger.info("\nâœ… å¤§è§„æ¨¡æŠ“å–ä¼˜åŒ–æµ‹è¯•å®Œæˆ!")
    
    def _save_detailed_report(self):
        """ä¿å­˜è¯¦ç»†æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        try:
            import json
            report_file = Path("data/large_scale_test_report.json")
            report_file.parent.mkdir(parents=True, exist_ok=True)
            
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "test_timestamp": time.time(),
                    "test_date": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "results": self.test_results
                }, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
    Path("logs").mkdir(exist_ok=True)
    
    logger.info("ğŸ¯ Bossç›´è˜å¤§è§„æ¨¡æŠ“å–ä¼˜åŒ–æµ‹è¯•")
    logger.info("æµ‹è¯•ç›®æ ‡:")
    logger.info("  1. éªŒè¯50-100+å²—ä½å¤§è§„æ¨¡æŠ“å–èƒ½åŠ›")
    logger.info("  2. æµ‹è¯•AIå²—ä½è¦æ±‚æ€»ç»“åŠŸèƒ½")
    logger.info("  3. éªŒè¯AIæˆæœ¬æ§åˆ¶ä¼˜åŒ–æ•ˆæœ")
    logger.info("  4. è¯„ä¼°æ•´ä½“ç³»ç»Ÿæ€§èƒ½")
    
    tester = LargeScaleOptimizationTester()
    await tester.run_comprehensive_test()


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("\nâ¹ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()