#!/usr/bin/env python3
"""
æµ‹è¯•å¢å¼ºçˆ¬è™«ç³»ç»Ÿ
éªŒè¯æ‰€æœ‰ä¼˜åŒ–ç»„ä»¶çš„åŠŸèƒ½
"""

import asyncio
import logging
import time
from crawler.enhanced_crawler_manager import (
    get_enhanced_crawler_manager,
    enhanced_search_jobs,
    batch_enhanced_search,
    get_crawler_status,
    optimize_crawler_system
)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)


async def test_basic_search():
    """æµ‹è¯•åŸºç¡€æœç´¢åŠŸèƒ½"""
    print("\n" + "="*60)
    print("ğŸ§ª æµ‹è¯•1: åŸºç¡€æœç´¢åŠŸèƒ½")
    print("="*60)
    
    try:
        results = await enhanced_search_jobs("æ•°æ®åˆ†æ", "shanghai", 5)
        
        print(f"âœ… æœç´¢ç»“æœ: {len(results)} ä¸ªå²—ä½")
        
        if results:
            print("\nğŸ“‹ å‰3ä¸ªå²—ä½:")
            for i, job in enumerate(results[:3], 1):
                print(f"  {i}. {job.get('title', 'æœªçŸ¥')} - {job.get('company', 'æœªçŸ¥')}")
                print(f"     è–ªèµ„: {job.get('salary', 'æœªçŸ¥')} | åœ°ç‚¹: {job.get('work_location', 'æœªçŸ¥')}")
                print(f"     æ¥æº: {job.get('engine_source', 'æœªçŸ¥')}")
                print()
        
        return True
        
    except Exception as e:
        print(f"âŒ åŸºç¡€æœç´¢æµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_concurrent_search():
    """æµ‹è¯•å¹¶å‘æœç´¢åŠŸèƒ½"""
    print("\n" + "="*60)
    print("ğŸ§ª æµ‹è¯•2: å¹¶å‘æœç´¢åŠŸèƒ½")
    print("="*60)
    
    try:
        search_requests = [
            {"keyword": "Pythonå¼€å‘", "city": "shanghai", "max_jobs": 3},
            {"keyword": "æ•°æ®ç§‘å­¦", "city": "beijing", "max_jobs": 3},
            {"keyword": "æœºå™¨å­¦ä¹ ", "city": "shenzhen", "max_jobs": 3}
        ]
        
        start_time = time.time()
        results = await batch_enhanced_search(search_requests)
        duration = time.time() - start_time
        
        print(f"âœ… å¹¶å‘æœç´¢å®Œæˆï¼Œè€—æ—¶: {duration:.2f}s")
        print(f"ğŸ“Š æœç´¢ç»“æœç»Ÿè®¡:")
        
        total_jobs = 0
        for task_id, job_list in results.items():
            job_count = len(job_list)
            total_jobs += job_count
            print(f"  - {task_id}: {job_count} ä¸ªå²—ä½")
        
        print(f"ğŸ“ˆ æ€»è®¡: {total_jobs} ä¸ªå²—ä½")
        return True
        
    except Exception as e:
        print(f"âŒ å¹¶å‘æœç´¢æµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_caching_mechanism():
    """æµ‹è¯•ç¼“å­˜æœºåˆ¶"""
    print("\n" + "="*60)
    print("ğŸ§ª æµ‹è¯•3: ç¼“å­˜æœºåˆ¶")
    print("="*60)
    
    try:
        keyword = "AIå·¥ç¨‹å¸ˆ"
        city = "hangzhou"
        max_jobs = 3
        
        # ç¬¬ä¸€æ¬¡æœç´¢ï¼ˆåº”è¯¥ä»ç½‘ç»œè·å–ï¼‰
        print("ğŸ” ç¬¬ä¸€æ¬¡æœç´¢ï¼ˆä»ç½‘ç»œè·å–ï¼‰...")
        start_time = time.time()
        results1 = await enhanced_search_jobs(keyword, city, max_jobs)
        duration1 = time.time() - start_time
        
        print(f"  - ç»“æœ: {len(results1)} ä¸ªå²—ä½")
        print(f"  - è€—æ—¶: {duration1:.2f}s")
        
        # ç­‰å¾…ä¸€ç§’é’Ÿ
        await asyncio.sleep(1)
        
        # ç¬¬äºŒæ¬¡æœç´¢ï¼ˆåº”è¯¥ä»ç¼“å­˜è·å–ï¼‰
        print("ğŸ” ç¬¬äºŒæ¬¡æœç´¢ï¼ˆä»ç¼“å­˜è·å–ï¼‰...")
        start_time = time.time()
        results2 = await enhanced_search_jobs(keyword, city, max_jobs)
        duration2 = time.time() - start_time
        
        print(f"  - ç»“æœ: {len(results2)} ä¸ªå²—ä½")
        print(f"  - è€—æ—¶: {duration2:.2f}s")
        
        # åˆ†æç¼“å­˜æ•ˆæœ
        if duration2 < duration1 * 0.5:
            print("âœ… ç¼“å­˜æœºåˆ¶å·¥ä½œæ­£å¸¸ï¼Œç¬¬äºŒæ¬¡æœç´¢æ˜æ˜¾æ›´å¿«")
        else:
            print("âš ï¸ ç¼“å­˜æœºåˆ¶å¯èƒ½æœªç”Ÿæ•ˆ")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç¼“å­˜æœºåˆ¶æµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_performance_monitoring():
    """æµ‹è¯•æ€§èƒ½ç›‘æ§"""
    print("\n" + "="*60)
    print("ğŸ§ª æµ‹è¯•4: æ€§èƒ½ç›‘æ§")
    print("="*60)
    
    try:
        # è·å–ç³»ç»ŸçŠ¶æ€
        status = await get_crawler_status()
        
        print("ğŸ“Š ç³»ç»ŸçŠ¶æ€:")
        print(f"  - åˆå§‹åŒ–çŠ¶æ€: {status.get('initialized', False)}")
        print(f"  - è¿è¡Œæ—¶é•¿: {status.get('uptime', 0):.2f}s")
        
        runtime_stats = status.get('runtime_stats', {})
        print(f"  - æ€»æœç´¢æ¬¡æ•°: {runtime_stats.get('total_searches', 0)}")
        print(f"  - æˆåŠŸæœç´¢æ¬¡æ•°: {runtime_stats.get('successful_searches', 0)}")
        print(f"  - å¹³å‡å“åº”æ—¶é—´: {runtime_stats.get('avg_response_time', 0):.2f}s")
        
        # è·å–æ€§èƒ½ç›‘æ§æ•°æ®
        perf_monitor = status.get('performance_monitor', {})
        if perf_monitor:
            print("ğŸ“ˆ æ€§èƒ½ç›‘æ§æ•°æ®:")
            print(f"  - ç›‘æ§æ¿€æ´»: {perf_monitor.get('monitoring_active', False)}")
            print(f"  - æ€§èƒ½è¯„åˆ†: {perf_monitor.get('performance_score', 0):.1f}/100")
            
            system_info = perf_monitor.get('system', {})
            if system_info:
                print(f"  - CPUä½¿ç”¨ç‡: {system_info.get('cpu_percent', 0):.1f}%")
                print(f"  - å†…å­˜ä½¿ç”¨ç‡: {system_info.get('memory_percent', 0):.1f}%")
        
        # è·å–å¹¶å‘ç®¡ç†å™¨çŠ¶æ€
        concurrent_mgr = status.get('concurrent_manager', {})
        if concurrent_mgr:
            print("ğŸ”„ å¹¶å‘ç®¡ç†å™¨:")
            print(f"  - è¿è¡ŒçŠ¶æ€: {concurrent_mgr.get('is_running', False)}")
            print(f"  - æ´»è·ƒä»»åŠ¡: {concurrent_mgr.get('active_tasks', 0)}")
            print(f"  - é˜Ÿåˆ—å¤§å°: {concurrent_mgr.get('queue_size', 0)}")
            
            cache_stats = concurrent_mgr.get('cache_stats', {})
            if cache_stats:
                print(f"  - ç¼“å­˜å‘½ä¸­ç‡: {cache_stats.get('hit_rate', 0):.2%}")
                print(f"  - å†…å­˜ç¼“å­˜å¤§å°: {cache_stats.get('memory_cache_size', 0)}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ€§èƒ½ç›‘æ§æµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_auto_optimization():
    """æµ‹è¯•è‡ªåŠ¨ä¼˜åŒ–"""
    print("\n" + "="*60)
    print("ğŸ§ª æµ‹è¯•5: è‡ªåŠ¨ä¼˜åŒ–")
    print("="*60)
    
    try:
        # è¿è¡Œè‡ªåŠ¨ä¼˜åŒ–
        optimization_result = await optimize_crawler_system()
        
        print("ğŸ”§ è‡ªåŠ¨ä¼˜åŒ–ç»“æœ:")
        print(f"  - æ€§èƒ½è¯„åˆ†: {optimization_result.get('performance_score', 0):.1f}")
        
        actions = optimization_result.get('optimization_actions', [])
        if actions:
            print("  - ä¼˜åŒ–åŠ¨ä½œ:")
            for action in actions:
                print(f"    â€¢ {action}")
        else:
            print("  - æ— éœ€ä¼˜åŒ–ï¼Œç³»ç»Ÿè¿è¡Œè‰¯å¥½")
        
        # è·å–ä¼˜åŒ–å»ºè®®
        manager = await get_enhanced_crawler_manager()
        recommendations = manager.get_recommendations()
        
        print("ğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        for rec in recommendations:
            print(f"  â€¢ {rec}")
        
        return True
        
    except Exception as e:
        print(f"âŒ è‡ªåŠ¨ä¼˜åŒ–æµ‹è¯•å¤±è´¥: {e}")
        return False


async def test_error_handling():
    """æµ‹è¯•é”™è¯¯å¤„ç†"""
    print("\n" + "="*60)
    print("ğŸ§ª æµ‹è¯•6: é”™è¯¯å¤„ç†")
    print("="*60)
    
    try:
        # æµ‹è¯•æ— æ•ˆåŸå¸‚
        print("ğŸ” æµ‹è¯•æ— æ•ˆåŸå¸‚...")
        results = await enhanced_search_jobs("æµ‹è¯•å²—ä½", "invalid_city", 2)
        print(f"  - æ— æ•ˆåŸå¸‚æœç´¢ç»“æœ: {len(results)} ä¸ªå²—ä½")
        
        # æµ‹è¯•ç©ºå…³é”®è¯
        print("ğŸ” æµ‹è¯•ç©ºå…³é”®è¯...")
        results = await enhanced_search_jobs("", "shanghai", 2)
        print(f"  - ç©ºå…³é”®è¯æœç´¢ç»“æœ: {len(results)} ä¸ªå²—ä½")
        
        print("âœ… é”™è¯¯å¤„ç†æµ‹è¯•å®Œæˆï¼Œç³»ç»Ÿå…·å¤‡è‰¯å¥½çš„å®¹é”™èƒ½åŠ›")
        return True
        
    except Exception as e:
        print(f"âŒ é”™è¯¯å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        return False


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•å¢å¼ºçˆ¬è™«ç³»ç»Ÿ")
    print("æµ‹è¯•èŒƒå›´ï¼šåŸºç¡€æœç´¢ã€å¹¶å‘æœç´¢ã€ç¼“å­˜æœºåˆ¶ã€æ€§èƒ½ç›‘æ§ã€è‡ªåŠ¨ä¼˜åŒ–ã€é”™è¯¯å¤„ç†")
    
    test_results = []
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    tests = [
        ("åŸºç¡€æœç´¢åŠŸèƒ½", test_basic_search),
        ("å¹¶å‘æœç´¢åŠŸèƒ½", test_concurrent_search),
        ("ç¼“å­˜æœºåˆ¶", test_caching_mechanism),
        ("æ€§èƒ½ç›‘æ§", test_performance_monitoring),
        ("è‡ªåŠ¨ä¼˜åŒ–", test_auto_optimization),
        ("é”™è¯¯å¤„ç†", test_error_handling)
    ]
    
    for test_name, test_func in tests:
        try:
            result = await test_func()
            test_results.append((test_name, result))
        except Exception as e:
            print(f"âŒ æµ‹è¯• '{test_name}' æ‰§è¡Œå¤±è´¥: {e}")
            test_results.append((test_name, False))
    
    # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    print("\n" + "="*60)
    print("ğŸ“Š æµ‹è¯•æŠ¥å‘Š")
    print("="*60)
    
    passed = sum(1 for _, result in test_results if result)
    total = len(test_results)
    
    print(f"æ€»æµ‹è¯•æ•°: {total}")
    print(f"é€šè¿‡æµ‹è¯•: {passed}")
    print(f"å¤±è´¥æµ‹è¯•: {total - passed}")
    print(f"é€šè¿‡ç‡: {passed/total:.1%}")
    
    print("\nè¯¦ç»†ç»“æœ:")
    for test_name, result in test_results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"  - {test_name}: {status}")
    
    # å…³é—­ç®¡ç†å™¨
    try:
        manager = await get_enhanced_crawler_manager()
        await manager.shutdown()
        print("\nğŸ›‘ å·²å…³é—­å¢å¼ºçˆ¬è™«ç®¡ç†å™¨")
    except:
        pass
    
    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å¢å¼ºçˆ¬è™«ç³»ç»Ÿè¿è¡Œæ­£å¸¸ã€‚")
    else:
        print("\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç³»ç»Ÿé…ç½®ã€‚")


if __name__ == "__main__":
    asyncio.run(main())