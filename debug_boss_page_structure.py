#!/usr/bin/env python3
"""
è°ƒè¯•Bossç›´è˜é¡µé¢ç»“æ„
åˆ†æå½“å‰é¡µé¢å…ƒç´ ï¼Œæ‰¾å‡ºæ­£ç¡®çš„é€‰æ‹©å™¨
"""

import asyncio
import logging
import sys
import os
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


async def debug_boss_page_structure():
    """è°ƒè¯•Bossç›´è˜é¡µé¢ç»“æ„"""
    try:
        from crawler.real_playwright_spider import RealPlaywrightBossSpider
        
        logger.info("ğŸ” å¼€å§‹è°ƒè¯•Bossç›´è˜é¡µé¢ç»“æ„...")
        
        # åˆ›å»ºçˆ¬è™«å®ä¾‹ï¼ˆå¯è§æ¨¡å¼ï¼Œä¾¿äºè§‚å¯Ÿï¼‰
        spider = RealPlaywrightBossSpider(headless=False)
        
        if not await spider.start():
            logger.error("âŒ æ— æ³•å¯åŠ¨Playwright")
            return
        
        # æœç´¢æµ‹è¯•å…³é”®è¯
        keyword = "æ•°æ®åˆ†æ"
        city = "shanghai"
        
        logger.info(f"ğŸ” æœç´¢: {keyword} åœ¨ {city}")
        
        # è·å–åŸå¸‚ä»£ç 
        city_code = spider.city_codes.get(city, "101210100")
        search_url = f"https://www.zhipin.com/web/geek/job?query={keyword}&city={city_code}"
        
        logger.info(f"ğŸŒ è®¿é—®: {search_url}")
        await spider.page.goto(search_url, wait_until="domcontentloaded")
        
        # ç­‰å¾…é¡µé¢åŠ è½½
        await asyncio.sleep(8)
        
        # å¤„ç†å¯èƒ½çš„ç™»å½•å¼¹çª—
        try:
            login_modal = await spider.page.query_selector('.login-dialog, .dialog-wrap')
            if login_modal:
                logger.info("ğŸ” æ£€æµ‹åˆ°ç™»å½•å¼¹çª—ï¼Œç­‰å¾…å¤„ç†...")
                await asyncio.sleep(3)
        except:
            pass
        
        # æˆªå›¾ä¿å­˜
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        screenshot_path = f"debug_boss_structure_{timestamp}.png"
        await spider.page.screenshot(path=screenshot_path)
        logger.info(f"ğŸ“¸ æˆªå›¾ä¿å­˜: {screenshot_path}")
        
        # ä¿å­˜é¡µé¢HTML
        page_html = await spider.page.content()
        html_path = f"debug_boss_structure_{timestamp}.html"
        with open(html_path, "w", encoding="utf-8") as f:
            f.write(page_html)
        logger.info(f"ğŸ“„ é¡µé¢HTMLä¿å­˜: {html_path}")
        
        # åˆ†æé¡µé¢ç»“æ„
        logger.info("ğŸ” åˆ†æé¡µé¢ç»“æ„...")
        
        # å°è¯•æ‰¾åˆ°å²—ä½å®¹å™¨
        potential_job_containers = [
            '.job-card-wrapper', 
            '.job-list-item',
            '.job-card-container', 
            'li[class*="job"]',
            'div[class*="job-card"]',
            '.job-detail-box',
            'a[ka*="search_list"]'
        ]
        
        job_elements = []
        for selector in potential_job_containers:
            try:
                elements = await spider.page.query_selector_all(selector)
                if elements:
                    logger.info(f"âœ… é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")
                    job_elements = elements
                    used_selector = selector
                    break
                else:
                    logger.debug(f"   é€‰æ‹©å™¨ '{selector}' æœªæ‰¾åˆ°å…ƒç´ ")
            except Exception as e:
                logger.debug(f"   é€‰æ‹©å™¨ '{selector}' å¼‚å¸¸: {e}")
        
        if not job_elements:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°å²—ä½å…ƒç´ ï¼Œå°è¯•é€šç”¨åˆ†æ...")
            
            # åˆ†æé¡µé¢ä¸­åŒ…å«ç‰¹å®šå…³é”®è¯çš„å…ƒç´ 
            all_divs = await spider.page.query_selector_all('div')
            logger.info(f"ğŸ“Š é¡µé¢å…±æœ‰ {len(all_divs)} ä¸ªdivå…ƒç´ ")
            
            keyword_elements = []
            for i, div in enumerate(all_divs[:100]):  # åªåˆ†æå‰100ä¸ª
                try:
                    text = await div.inner_text()
                    if text and any(kw in text for kw in ["K", "è–ª", "ç»éªŒ", "å­¦å†"]) and len(text) < 200:
                        keyword_elements.append((i, text[:100]))
                except:
                    continue
            
            logger.info(f"ğŸ¯ æ‰¾åˆ° {len(keyword_elements)} ä¸ªå¯èƒ½çš„å²—ä½ç›¸å…³å…ƒç´ :")
            for i, (index, text) in enumerate(keyword_elements[:10]):
                logger.info(f"   å…ƒç´  {index}: {text}...")
        
        else:
            # åˆ†ææ‰¾åˆ°çš„å²—ä½å…ƒç´ ç»“æ„
            logger.info(f"ğŸ“‹ åˆ†æå‰3ä¸ªå²—ä½å…ƒç´ çš„ç»“æ„ (ä½¿ç”¨é€‰æ‹©å™¨: {used_selector})...")
            
            analysis_results = []
            
            for i, job_elem in enumerate(job_elements[:3]):
                logger.info(f"\n--- å²—ä½å…ƒç´  {i+1} ---")
                
                try:
                    # è·å–å…ƒç´ çš„HTMLç»“æ„
                    elem_html = await job_elem.inner_html()
                    elem_text = await job_elem.inner_text()
                    
                    logger.info(f"ğŸ“ å…ƒç´ æ–‡æœ¬: {elem_text[:200]}...")
                    
                    # åˆ†æå†…éƒ¨ç»“æ„
                    analysis = {
                        "index": i+1,
                        "full_text": elem_text,
                        "html_snippet": elem_html[:500] + "..." if len(elem_html) > 500 else elem_html
                    }
                    
                    # å°è¯•ä¸åŒé€‰æ‹©å™¨æå–ä¿¡æ¯
                    title_selectors = ['.job-name', '.job-title', 'h3', 'a', 'span[class*="name"]']
                    company_selectors = ['.company-name', '.company', 'span[class*="company"]']
                    salary_selectors = ['.salary', '.red', 'span[class*="salary"]']
                    location_selectors = ['.job-area', '.area', 'span[class*="area"]']
                    
                    for desc, selectors in [
                        ("æ ‡é¢˜", title_selectors),
                        ("å…¬å¸", company_selectors), 
                        ("è–ªèµ„", salary_selectors),
                        ("åœ°ç‚¹", location_selectors)
                    ]:
                        found_texts = []
                        for sel in selectors:
                            try:
                                elems = await job_elem.query_selector_all(sel)
                                for elem in elems:
                                    text = await elem.inner_text()
                                    if text.strip():
                                        found_texts.append(f"{sel}: '{text.strip()}'")
                            except:
                                continue
                        
                        if found_texts:
                            logger.info(f"   {desc}: {found_texts}")
                            analysis[desc.lower()] = found_texts
                        else:
                            logger.warning(f"   {desc}: æœªæ‰¾åˆ°")
                            analysis[desc.lower()] = []
                    
                    analysis_results.append(analysis)
                    
                except Exception as e:
                    logger.error(f"   âŒ åˆ†æå²—ä½å…ƒç´  {i+1} å¤±è´¥: {e}")
            
            # ä¿å­˜åˆ†æç»“æœ
            analysis_path = f"debug_analysis_{timestamp}.json"
            with open(analysis_path, "w", encoding="utf-8") as f:
                json.dump(analysis_results, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ“Š åˆ†æç»“æœä¿å­˜: {analysis_path}")
        
        # å…³é—­æµè§ˆå™¨
        await spider.close()
        
        logger.info("âœ… é¡µé¢ç»“æ„è°ƒè¯•å®Œæˆ")
        logger.info(f"ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        logger.info(f"   - æˆªå›¾: {screenshot_path}")
        logger.info(f"   - HTML: {html_path}")
        if 'analysis_path' in locals():
            logger.info(f"   - åˆ†æ: {analysis_path}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ è°ƒè¯•è¿‡ç¨‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” Bossç›´è˜é¡µé¢ç»“æ„è°ƒè¯•å·¥å…·")
    print("=" * 40)
    
    success = asyncio.run(debug_boss_page_structure())
    
    if success:
        print("\nâœ… è°ƒè¯•å®Œæˆï¼Œè¯·æŸ¥çœ‹ç”Ÿæˆçš„æ–‡ä»¶æ¥åˆ†æé¡µé¢ç»“æ„")
        print("ğŸ’¡ åŸºäºåˆ†æç»“æœï¼Œæˆ‘ä»¬å¯ä»¥æ›´æ–°é€‰æ‹©å™¨ä»¥ä¿®å¤æŠ“å–é—®é¢˜")
    else:
        print("\nâŒ è°ƒè¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")


if __name__ == "__main__":
    main()